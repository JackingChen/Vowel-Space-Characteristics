{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dict' object has no attribute '__frozen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4823/3977126770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesOfInterest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesOfInterest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mcolumn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeaturesOfInterest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesOfInterest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mcolumn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeaturesOfInterest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/addict/addict.py\u001b[0m in \u001b[0;36m__missing__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__frozen'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__parent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dict' object has no attribute '__frozen'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jun 30 15:56:45 2021\n",
    "\n",
    "@author: jackchen\n",
    "\n",
    "\n",
    "    This script is only for TBMEA1 \n",
    "\n",
    "\n",
    "    2022/04/19: should add feature combination code with a form of class\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import spearmanr,pearsonr \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "from addict import Dict\n",
    "# import functions\n",
    "import argparse\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn.svm\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "\n",
    "from articulation.HYPERPARAM import phonewoprosody, Label\n",
    "import articulation.HYPERPARAM.FeatureSelect as FeatSel\n",
    "import articulation.HYPERPARAM.PaperNameMapping as PprNmeMp\n",
    "\n",
    "import articulation.articulation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "def Assert_labelfeature(feat_name,lab_name):\n",
    "    # =============================================================================\n",
    "    #     To check if the label match with feature\n",
    "    # =============================================================================\n",
    "    for i,n in enumerate(feat_name):\n",
    "        assert feat_name[i] == lab_name[i]\n",
    "\n",
    "def FilterFile_withinManualName(files,Manual_choosen_feature):\n",
    "    files_manualChoosen=[f  for f in files if os.path.basename(f).split(\".\")[0]  in Manual_choosen_feature]\n",
    "    return files_manualChoosen\n",
    "\n",
    "def Merge_dfs(df_1, df_2):\n",
    "    return pd.merge(df_1,df_2,left_index=True, right_index=True)\n",
    "def CCC_numpy(y_true, y_pred):\n",
    "    '''Reference numpy implementation of Lin's Concordance correlation coefficient'''\n",
    "    \n",
    "    # covariance between y_true and y_pred\n",
    "    s_xy = np.cov([y_true, y_pred])[0,1]\n",
    "    # means\n",
    "    x_m = np.mean(y_true)\n",
    "    y_m = np.mean(y_pred)\n",
    "    # variances\n",
    "    s_x_sq = np.var(y_true)\n",
    "    s_y_sq = np.var(y_pred)\n",
    "    \n",
    "    # condordance correlation coefficient\n",
    "    ccc = (2.0*s_xy) / (s_x_sq + s_y_sq + (x_m-y_m)**2)\n",
    "    \n",
    "    return ccc\n",
    "def get_args():\n",
    "    # we add compulsary arguments as named arguments for readability\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Feature_mode', default='Customized_feature',\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--preprocess', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--start_point', default=-1,\n",
    "                        help='In case that the program stop at certain point, we can resume the progress by setting this variable')\n",
    "    parser.add_argument('--experiment', default='gop_exp_ADOShappyDAAIKidallDeceiptformosaCSRC',\n",
    "                        help='If the mode is set to Session_phone_phf, you may need to determine the experiment used to generate the gop feature')\n",
    "    parser.add_argument('--pseudo', default=False,\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--suffix', default=\"\",\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--FS_method_str', default=None,\n",
    "                        help='Feature selection')\n",
    "    parser.add_argument('--UseManualCtxFeat', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--Plot', default=False,\n",
    "                        help='')\n",
    "    parser.add_argument('--selectModelScoring', default='neg_mean_squared_error',\n",
    "                        help='')\n",
    "    parser.add_argument('--Mergefeatures', default=False,\n",
    "                        help='')\n",
    "    parser.add_argument('--knn_weights', default='uniform',\n",
    "                            help='path of the base directory')\n",
    "    parser.add_argument('--knn_neighbors', default=2,  type=int,\n",
    "                            help='path of the base directory')\n",
    "    parser.add_argument('--Reorder_type', default='DKIndividual',\n",
    "                            help='[DKIndividual, DKcriteria]')\n",
    "    parser.add_argument('--FeatureComb_mode', default='baselineFeats',\n",
    "                            help='[Add_UttLvl_feature, feat_comb3, feat_comb5, feat_comb6,feat_comb7, baselineFeats,Comb_dynPhonation,Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation, Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation_Add_UttLvl_feature]')\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "start_point=args.start_point\n",
    "experiment=args.experiment\n",
    "knn_weights=args.knn_weights\n",
    "knn_neighbors=args.knn_neighbors\n",
    "Reorder_type=args.Reorder_type\n",
    "# Add_UttLvl_feature=args.Add_UttLvl_feature\n",
    "# =============================================================================\n",
    "columns=[\n",
    "'intensity_mean_mean(A:,i:,u:)', 'meanF0_mean(A:,i:,u:)',\n",
    "       'stdevF0_mean(A:,i:,u:)', 'hnr_mean(A:,i:,u:)',\n",
    "       'localJitter_mean(A:,i:,u:)', 'localabsoluteJitter_mean(A:,i:,u:)',\n",
    "       'rapJitter_mean(A:,i:,u:)', 'ddpJitter_mean(A:,i:,u:)',\n",
    "       'localShimmer_mean(A:,i:,u:)', 'localdbShimmer_mean(A:,i:,u:)',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# 用comb來組合multifeature fusion\n",
    "# 用[ [col] for col in columns]  來跑單feautre實驗\n",
    "# featuresOfInterest=[ [col] for col in columns] \n",
    "# featuresOfInterest=[ [col] + ['u_num+i_num+a_num'] for col in columns]\n",
    "# featuresOfInterest=[ [col] for col in columns]\n",
    "# featuresOfInterest=[ Comb[k] for k in Comb.keys()]\n",
    "Top_ModuleColumn_mapping_dict={}\n",
    "Top_ModuleColumn_mapping_dict['Add_UttLvl_feature']=FeatSel.Columns_comb2.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb']=FeatSel.Columns_comb.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb3']=FeatSel.Columns_comb3.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb5']=FeatSel.Columns_comb5.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb6']=FeatSel.Columns_comb6.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb7']=FeatSel.Columns_comb7.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb8']=FeatSel.Columns_comb8.copy()\n",
    "Top_ModuleColumn_mapping_dict['Comb_dynPhonation']=FeatSel.Comb_dynPhonation.copy()\n",
    "Top_ModuleColumn_mapping_dict['Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']=FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation.copy()\n",
    "Top_ModuleColumn_mapping_dict['Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation_Add_UttLvl_feature']=FeatSel.Comb_Utt_feature_staticLOCDEP_dynamicLOCDEP_dynamicphonation.copy()\n",
    "Top_ModuleColumn_mapping_dict['baselineFeats']=FeatSel.Baseline_comb.copy()\n",
    "\n",
    "featuresOfInterest=Top_ModuleColumn_mapping_dict[args.FeatureComb_mode]\n",
    "\n",
    "\n",
    "label_choose=['ADOS_C']\n",
    "\n",
    "\n",
    "pearson_scorer = make_scorer(pearsonr, greater_is_better=False)\n",
    "\n",
    "df_formant_statistics_CtxPhone_collect_dict=Dict()\n",
    "# =============================================================================\n",
    "\n",
    "class ADOSdataset():\n",
    "    def __init__(self,):\n",
    "        self.featurepath='Features'            \n",
    "        self.N=2\n",
    "        self.LabelType=Dict()\n",
    "        self.LabelType['ADOS_S']='regression'\n",
    "        self.LabelType['ADOS_C']='regression'\n",
    "        self.LabelType['ADOS_D']='regression'\n",
    "        self.LabelType['ADOS_cate']='classification'\n",
    "        self.LabelType['ASDTD']='classification'\n",
    "        self.Fractionfeatures_str='Features/artuculation_AUI/Vowels/Fraction/*.pkl'    \n",
    "        self.FeatureCombs=Dict()\n",
    "\n",
    "        # self._FeatureBuild()\n",
    "    def Get_FormantAUI_feat(self,label_choose,pickle_path,featuresOfInterest=['MSB_f1','MSB_f2','MSB_mix'],filterbyNum=True,**kwargs):\n",
    "        self.featuresOfInterest=featuresOfInterest\n",
    "        arti=articulation.articulation.Articulation()\n",
    "        if not kwargs and len(pickle_path)>0:\n",
    "            df_tmp=pickle.load(open(pickle_path,\"rb\")).sort_index()\n",
    "            # df_tmp=pickle.load(open(pickle_path,\"rb\"))\n",
    "        elif len(kwargs)>0: # usage Get_FormantAUI_feat(...,key1=values1):\n",
    "            for k, v in kwargs.items(): #there will be only one element\n",
    "                df_tmp=kwargs[k].sort_index()\n",
    "\n",
    "        if filterbyNum:\n",
    "            df_tmp=arti.BasicFilter_byNum(df_tmp,N=self.N)\n",
    "        \n",
    "        # if label_choose not in df_tmp.columns:\n",
    "        #     # print(\"len(df_tmp): \", len(df_tmp))\n",
    "        #     # print(\"Feature name = \", os.path.basename(pickle_path))\n",
    "        #     for people in df_tmp.index:\n",
    "        #         lab=Label.label_raw[label_choose][Label.label_raw['name']==people]\n",
    "        #         df_tmp.loc[people,'ADOS']=lab.values\n",
    "        #     df_y=df_tmp['ADOS'] #Still keep the form of dataframe\n",
    "        # else:\n",
    "        #     df_y=df_tmp[label_choose] #Still keep the form of dataframe\n",
    "        \n",
    "        # Always update the label from Label\n",
    "        for people in df_tmp.index:\n",
    "            lab=Label.label_raw[label_choose][Label.label_raw['name']==people]\n",
    "            df_tmp.loc[people,'ADOS']=lab.values\n",
    "        df_y=df_tmp['ADOS'] #Still keep the form of dataframe\n",
    "        \n",
    "        \n",
    "        feature_array=df_tmp[featuresOfInterest]\n",
    "        \n",
    "            \n",
    "        LabType=self.LabelType[label_choose]\n",
    "        return feature_array, df_y, LabType\n",
    "    def _FeatureBuild(self):\n",
    "        Features=Dict()\n",
    "        Features_comb=Dict()\n",
    "        files = glob.glob(self.Fractionfeatures_str)\n",
    "        for file in files:\n",
    "            feat_name=os.path.basename(file).replace(\".pkl\",\"\")\n",
    "            df_tmp=pickle.load(open(file,\"rb\")).sort_index()\n",
    "            Features[feat_name]=df_tmp\n",
    "        for keys in self.FeatureCombs.keys():\n",
    "            combF=[Features[k] for k in self.FeatureCombs[keys]]\n",
    "            Features_comb[keys]=pd.concat(combF)\n",
    "        \n",
    "        self.Features_comb=Features_comb\n",
    "\n",
    "\n",
    "ados_ds=ADOSdataset()\n",
    "ErrorFeat_bookeep=Dict()\n",
    "Session_level_all=Dict()\n",
    "\n",
    "\n",
    "if 'Add_UttLvl_feature' in args.FeatureComb_mode :\n",
    "    Merge_feature_path='RegressionMerged_dfs/ADDed_UttFeat/{knn_weights}_{knn_neighbors}_{Reorder_type}/ASD_DOCKID/'.format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type)\n",
    "else:\n",
    "    Merge_feature_path='RegressionMerged_dfs/{knn_weights}_{knn_neighbors}_{Reorder_type}/ASD_DOCKID/'.format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type)\n",
    "\n",
    "ChooseData_manual=['static_feautre_LOC','dynamic_feature_LOC','dynamic_feature_phonation']\n",
    "# ChooseData_manual=['static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation']\n",
    "# ChooseData_manual=['Utt_features+static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation']\n",
    "# ChooseData_manual=None\n",
    "\n",
    "# for feature_paths in [Merge_feature_path]:\n",
    "if ChooseData_manual==None:\n",
    "    files = glob.glob(ados_ds.featurepath +'/'+ Merge_feature_path+'/*.pkl')\n",
    "else:\n",
    "    files=[]\n",
    "    for d in ChooseData_manual:\n",
    "        files.append(ados_ds.featurepath +'/'+ Merge_feature_path+'/{}.pkl'.format(d))\n",
    "\n",
    "# load features from file\n",
    "for file in files: #iterate over features\n",
    "    feat_=os.path.basename(file).split(\".\")[0]  \n",
    "    \n",
    "    if type(featuresOfInterest)==dict or type(featuresOfInterest)==Dict:\n",
    "        column_dict=featuresOfInterest[feat_]\n",
    "    elif type(featuresOfInterest)==list:\n",
    "        column_dict=featuresOfInterest\n",
    "    else:\n",
    "        raise KeyError()\n",
    "    \n",
    "    \n",
    "    for key,feat_col in tqdm(column_dict.items()):\n",
    "        # if len(feat_col)==1:\n",
    "        #     feat_col_ = list([feat_col]) # ex: ['MSB_f1']\n",
    "        # else:\n",
    "        feat_col_ = list(feat_col) # ex: ['MSB_f1']\n",
    "        for lab_ in label_choose:\n",
    "            X,y, featType=ados_ds.Get_FormantAUI_feat(label_choose=lab_,pickle_path=file,featuresOfInterest=feat_col_,filterbyNum=False)\n",
    "            Item_name=\"{feat}::{lab}\".format(feat='-'.join([feat_]+[key]),lab=lab_)\n",
    "            Session_level_all[Item_name].X, \\\n",
    "                Session_level_all[Item_name].y, \\\n",
    "                    Session_level_all[Item_name].feattype = X,y, featType\n",
    "        \n",
    "        assert y.isna().any() !=True\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Feature merging function\n",
    "    \n",
    "    Ths slice of code provide user to manually make functions to combine df_XXX_infos\n",
    "\n",
    "'''\n",
    "# =============================================================================\n",
    "if args.Mergefeatures:\n",
    "    Merg_filepath={}\n",
    "    Merg_filepath['static_feautre_LOC']='Features/artuculation_AUI/Vowels/Formants/Formant_AUI_tVSAFCRFvals_KID_FromASD_DOCKID.pkl'\n",
    "    Merg_filepath['static_feautre_phonation']='Features/artuculation_AUI/Vowels/Phonation/Phonation_meanvars_KID_FromASD_DOCKID.pkl'\n",
    "    Merg_filepath['dynamic_feature_LOC']='Features/artuculation_AUI/Interaction/Formants/Syncrony_measure_of_variance_DKIndividual_ASD_DOCKID.pkl'\n",
    "    Merg_filepath['dynamic_feature_phonation']='Features/artuculation_AUI/Interaction/Phonation/Syncrony_measure_of_variance_phonation_ASD_DOCKID.pkl'\n",
    "    \n",
    "    merge_out_path='Features/RegressionMerged_dfs/'\n",
    "    if not os.path.exists(merge_out_path):\n",
    "        os.makedirs(merge_out_path)\n",
    "    \n",
    "    df_infos_dict=Dict()\n",
    "    for keys, paths in Merg_filepath.items():\n",
    "        df_infos_dict[keys]=pickle.load(open(paths,\"rb\")).sort_index()\n",
    "    \n",
    "    Merged_df_dict=Dict()\n",
    "    comb1 = list(combinations(list(Merg_filepath.keys()), 1))\n",
    "    comb2 = list(combinations(list(Merg_filepath.keys()), 2))\n",
    "    for c in comb1:\n",
    "        e1=c[0]\n",
    "        Merged_df_dict[e1]=df_infos_dict[e1]\n",
    "        OutPklpath=merge_out_path+ e1 + \".pkl\"\n",
    "        pickle.dump(Merged_df_dict[e1],open(OutPklpath,\"wb\"))\n",
    "        \n",
    "        \n",
    "    for c in comb2:\n",
    "        e1, e2=c\n",
    "        Merged_df_dict['+'.join(c)]=Merge_dfs(df_infos_dict[e1],df_infos_dict[e2])\n",
    "        \n",
    "        OutPklpath=merge_out_path+'+'.join(c)+\".pkl\"\n",
    "        pickle.dump(Merged_df_dict['+'.join(c)],open(OutPklpath,\"wb\"))\n",
    "        \n",
    "    \n",
    "# =============================================================================\n",
    "# Model parameters\n",
    "# =============================================================================\n",
    "\n",
    "epsilon=np.array([0.001,0.01,0.1,0.5,1,5,10.0,25,50,75,100])\n",
    "\n",
    "n_estimator=[2, 4, 8, 16, 32, 64]\n",
    "\n",
    "\n",
    "Classifier={}\n",
    "loo=LeaveOneOut()\n",
    "\n",
    "\n",
    "# CV_settings=loo\n",
    "CV_settings=10\n",
    "# CV_settings=2\n",
    "# skf = StratifiedKFold(n_splits=CV_settings)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "    Regressor\n",
    "\n",
    "'''\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "Classifier['SVR']={'model':sklearn.svm.SVR(),\\\n",
    "                  'parameters':{\n",
    "                    'model__epsilon': epsilon,\\\n",
    "                    # 'model__C':C_variable,\\\n",
    "                    'model__kernel': ['rbf'],\\\n",
    "                    # 'gamma': ['auto'],\\\n",
    "                                }}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    BookKeep area\n",
    "\n",
    "'''\n",
    "Best_predict_optimize={}\n",
    "\n",
    "df_best_result_r2=pd.DataFrame([])\n",
    "df_best_result_pear=pd.DataFrame([])\n",
    "df_best_result_spear=pd.DataFrame([])\n",
    "df_best_cross_score=pd.DataFrame([])\n",
    "df_best_result_UAR=pd.DataFrame([])\n",
    "df_best_result_AUC=pd.DataFrame([])\n",
    "df_best_result_f1=pd.DataFrame([])\n",
    "df_best_result_allThreeClassifiers=pd.DataFrame([])\n",
    "# =============================================================================\n",
    "Result_path=\"RESULTS/Fusion_result/{}/\".format(args.FeatureComb_mode)\n",
    "\n",
    "\n",
    "if not os.path.exists(Result_path):\n",
    "    os.makedirs(Result_path)\n",
    "final_result_file=\"_ADOS_{}.xlsx\".format(args.suffix)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "count=0\n",
    "OutFeature_dict=Dict()\n",
    "Best_param_dict=Dict()\n",
    "\n",
    "for clf_keys, clf in Classifier.items(): #Iterate among different classifiers \n",
    "    writer_clf = pd.ExcelWriter(Result_path+\"/\"+clf_keys+\"_\"+args.Feature_mode+\"_\"+final_result_file, engine = 'xlsxwriter')\n",
    "    for feature_lab_str, features in Session_level_all.items():\n",
    "        feature_keys, label_keys= feature_lab_str.split(\"::\")\n",
    "        feature_rawname=feature_keys[feature_keys.find('-')+1:]\n",
    "        feature_filename=feature_keys[:feature_keys.find('-')]\n",
    "        if feature_rawname in PprNmeMp.Paper_name_map.keys():\n",
    "            featurename_paper=PprNmeMp.Paper_name_map[feature_rawname]\n",
    "            feature_keys=feature_keys.replace(feature_rawname,featurename_paper)\n",
    "            \n",
    "        Labels = Session_level_all.X[feature_keys]\n",
    "        print(\"=====================Cross validation start==================\")\n",
    "        pipe = Pipeline(steps=[('scalar',StandardScaler()),(\"model\", clf['model'])])\n",
    "        p_grid=clf['parameters']\n",
    "        Gclf = GridSearchCV(estimator=pipe, param_grid=p_grid, scoring=args.selectModelScoring, cv=CV_settings, refit=True, n_jobs=-1)\n",
    "        # Score=cross_val_score(Gclf, features.X, features.y, cv=CV_settings, scoring=pearson_scorer) \n",
    "        CVpredict=cross_val_predict(Gclf, features.X, features.y, cv=CV_settings)           \n",
    "        Gclf.fit(features.X,features.y)\n",
    "        # if clf_keys == \"EN\":\n",
    "        #     print('The coefficient of best estimator is: ',Gclf.best_estimator_.coef_)\n",
    "        \n",
    "        print(\"The best score with scoring parameter: 'r2' is\", Gclf.best_score_)\n",
    "        print(\"The best parameters are :\", Gclf.best_params_)\n",
    "        best_parameters=Gclf.best_params_\n",
    "        best_score=Gclf.best_score_\n",
    "        best_parameters.update({'best_score':best_score})\n",
    "        Best_param_dict[feature_lab_str]=best_parameters\n",
    "        cv_results_info=Gclf.cv_results_\n",
    "\n",
    "        \n",
    "        \n",
    "        if features.feattype == 'regression':\n",
    "            r2=r2_score(features.y,CVpredict )\n",
    "            n,p=features.X.shape\n",
    "            r2_adj=1-(1-r2)*(n-1)/(n-p-1)\n",
    "            MSE=sklearn.metrics.mean_squared_error(features.y.values.ravel(),CVpredict)\n",
    "            pearson_result, pearson_p=pearsonr(features.y,CVpredict )\n",
    "            spear_result, spearman_p=spearmanr(features.y,CVpredict )\n",
    "            CCC = CCC_numpy(features.y, CVpredict)\n",
    "            print('Feature {0}, label {1} ,spear_result {2}'.format(feature_keys, label_keys,spear_result))\n",
    "        elif features.feattype == 'classification':\n",
    "            n,p=features.X.shape\n",
    "            UAR=recall_score(features.y, CVpredict, average='macro')\n",
    "            AUC=roc_auc_score(features.y, CVpredict)\n",
    "            f1Score=f1_score(features.y, CVpredict, average='macro')\n",
    "            \n",
    "            print('Feature {0}, label {1} ,UAR {2}'.format(feature_keys, label_keys,UAR))\n",
    "        \n",
    "        if args.Plot and p <2:\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 10), sharey=True)\n",
    "            kernel_label = [clf_keys]\n",
    "            model_color = ['m']\n",
    "            # axes.plot((features.X - min(features.X) )/ max(features.X), Gclf.best_estimator_.fit(features.X,features.y).predict(features.X), color=model_color[0],\n",
    "            #               label='CV Predict')\n",
    "            axes.scatter((features.X.values - min(features.X.values) )/ max(features.X.values), CVpredict, \n",
    "                         facecolor=\"none\", edgecolor=\"k\", s=150,\n",
    "                         label='{}'.format(feature_lab_str)\n",
    "                         )\n",
    "            axes.scatter((features.X.values - min(features.X.values) )/ max(features.X.values), features.y, \n",
    "                         facecolor=\"none\", edgecolor=\"r\", s=50,\n",
    "                         label='Real Y')\n",
    "            axes.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
    "                    ncol=1, fancybox=True, shadow=True)\n",
    "            \n",
    "            Plot_path='./Plot/LinearRegress/'\n",
    "            if not os.path.exists(Plot_path):\n",
    "                os.makedirs(Plot_path)\n",
    "            plot_file=Plot_path+\"/{0}_{1}.png\".format(clf_keys,feature_lab_str)\n",
    "            plt.savefig(plot_file, dpi=200) \n",
    "        \n",
    "        # =============================================================================\n",
    "        '''\n",
    "            Inspect the best result\n",
    "        '''\n",
    "        # =============================================================================\n",
    "        # Best_predict_optimize[label_keys]=pd.DataFrame(np.vstack((CVpredict,features.y)).T,columns=['y_pred','y'])\n",
    "        # excel_path='./Statistics/prediction_result'\n",
    "        # if not os.path.exists(excel_path):\n",
    "        #     os.makedirs(excel_path)\n",
    "        # excel_file=excel_path+\"/{0}_{1}.xlsx\"\n",
    "        # writer = pd.ExcelWriter(excel_file.format(clf_keys,feature_keys.replace(\":\",\"\")), engine = 'xlsxwriter')\n",
    "        # for label_name in  Best_predict_optimize.keys():\n",
    "        #     Best_predict_optimize[label_name].to_excel(writer,sheet_name=label_name.replace(\"/\",\"_\"))\n",
    "        # writer.save()\n",
    "                                \n",
    "        # ================================================      =============================\n",
    "        if features.feattype == 'regression':\n",
    "            df_best_result_r2.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(r2_adj,3),np.round(np.nan,6))\n",
    "            df_best_result_pear.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(pearson_result,3),np.round(pearson_p,6))\n",
    "            df_best_result_spear.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(spear_result,3),np.round(spearman_p,6))\n",
    "            df_best_result_spear.loc[feature_keys,'de-zero_num']=len(features.X)\n",
    "            # df_best_cross_score.loc[feature_keys,label_keys]=Score.mean()\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1} (MSE/pear/spear/CCC)'.format(label_keys,clf_keys)]\\\n",
    "                        ='{0}/{1}/{2}/{3}'.format(np.round(MSE,3),np.round(pearson_result,3),np.round(spear_result,3),np.round(CCC,3))\n",
    "\n",
    "        elif features.feattype == 'classification':\n",
    "            df_best_result_UAR.loc[feature_keys,label_keys]='{0}'.format(UAR)\n",
    "            df_best_result_AUC.loc[feature_keys,label_keys]='{0}'.format(AUC)\n",
    "            df_best_result_f1.loc[feature_keys,label_keys]='{0}'.format(f1Score)\n",
    "            # df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1} (UAR/AUC/f1score)'.format(label_keys,clf_keys)]\\\n",
    "            #             ='{0}/{1}/{2}'.format(np.round(UAR,3),np.round(AUC,3),np.round(f1Score,3))\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1}'.format(label_keys,clf_keys)]\\\n",
    "                        ='{0}'.format(np.round(UAR,3))\n",
    "    \n",
    "    if features.feattype == 'regression':\n",
    "        df_best_result_r2.to_excel(writer_clf,sheet_name=\"R2_adj\")\n",
    "        df_best_result_pear.to_excel(writer_clf,sheet_name=\"pear\")\n",
    "        df_best_result_spear.to_excel(writer_clf,sheet_name=\"spear\")\n",
    "        df_best_result_spear.to_csv(Result_path+\"/\"+clf_keys+\"_\"+args.Feature_mode+\"_spearman.csv\")\n",
    "    elif features.feattype == 'classification':\n",
    "        df_best_result_UAR.to_excel(writer_clf,sheet_name=\"UAR\")\n",
    "        df_best_result_AUC.to_excel(writer_clf,sheet_name=\"AUC\")\n",
    "        df_best_result_f1.to_excel(writer_clf,sheet_name=\"f1\")\n",
    "\n",
    "writer_clf.save()\n",
    "print(df_best_result_allThreeClassifiers)\n",
    "df_best_result_allThreeClassifiers.to_excel(Result_path+\"/\"+\"Regression_{knn_weights}_{knn_neighbors}_{Reorder_type}.xlsx\".format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type))\n",
    "print('generated at',Result_path+\"/\"+\"Regression_{knn_weights}_{knn_neighbors}_{Reorder_type}.xlsx\".format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['static_feautre_LOC', 'dynamic_feature_phonation', 'dynamic_feature_LOC'])\n",
      "static_feautre_LOC+static_feautre_phonation\n"
     ]
    }
   ],
   "source": [
    "print(featuresOfInterest.keys())\n",
    "print(feat_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
