{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/jack/workspace/DisVoice/articulation/HYPERPARAM/Label.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  label_raw=label_raw.append(df_labels_TD)\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "/media/jack/workspace/DisVoice/utils_jack.py:450: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  Info_name_sex=Info_name_sex.append(Info_name_sex_TD)\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================Cross validation start==================\n",
      "The best score with scoring parameter: 'r2' is -2.9425957142159325\n",
      "The best parameters are : {'model__epsilon': 0.1, 'model__kernel': 'rbf'}\n",
      "Feature static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOC_columns+DEP_columns+LOCDEP_Trend_D_cols+LOCDEP_Syncrony_cols, label ADOS_C ,spear_result 0.5342495788316084\n",
      "=====================Cross validation start==================\n",
      "The best score with scoring parameter: 'r2' is -3.1003146769120655\n",
      "The best parameters are : {'model__epsilon': 0.1, 'model__kernel': 'rbf'}\n",
      "Feature static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOC_columns+DEP_columns+LOCDEP_Trend_D_cols, label ADOS_C ,spear_result 0.47409137931374873\n",
      "=====================Cross validation start==================\n",
      "The best score with scoring parameter: 'r2' is -2.9708436549791313\n",
      "The best parameters are : {'model__epsilon': 0.1, 'model__kernel': 'rbf'}\n",
      "Feature static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOC_columns+DEP_columns+LOCDEP_Syncrony_cols, label ADOS_C ,spear_result 0.5111103381520564\n",
      "=====================Cross validation start==================\n",
      "The best score with scoring parameter: 'r2' is -3.407574950871976\n",
      "The best parameters are : {'model__epsilon': 1.0, 'model__kernel': 'rbf'}\n",
      "Feature static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOC_columns+DEP_columns, label ADOS_C ,spear_result 0.38167790093212317\n",
      "=====================Cross validation start==================\n",
      "The best score with scoring parameter: 'r2' is -3.795077021567758\n",
      "The best parameters are : {'model__epsilon': 1.0, 'model__kernel': 'rbf'}\n",
      "Feature static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-Inter-Vowel Dispersion, label ADOS_C ,spear_result 0.24663044334059123\n",
      "=====================Cross validation start==================\n",
      "The best score with scoring parameter: 'r2' is -3.2666956687859168\n",
      "The best parameters are : {'model__epsilon': 1.0, 'model__kernel': 'rbf'}\n",
      "Feature static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-formant dependency, label ADOS_C ,spear_result 0.32062053290501247\n",
      "=====================Cross validation start==================\n",
      "The best score with scoring parameter: 'r2' is -3.9869162918495022\n",
      "The best parameters are : {'model__epsilon': 0.001, 'model__kernel': 'rbf'}\n",
      "Feature static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-GC[VSC]\\textsubscript{inv}, label ADOS_C ,spear_result 0.06734185010138768\n",
      "=====================Cross validation start==================\n",
      "The best score with scoring parameter: 'r2' is -3.9365324880364474\n",
      "The best parameters are : {'model__epsilon': 1.0, 'model__kernel': 'rbf'}\n",
      "Feature static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-Syncrony[VSC], label ADOS_C ,spear_result 0.018223263586276593\n",
      "=====================Cross validation start==================\n",
      "The best score with scoring parameter: 'r2' is -3.3454469940807563\n",
      "The best parameters are : {'model__epsilon': 1.0, 'model__kernel': 'rbf'}\n",
      "Feature static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-GC[P]\\textsubscript{part}, label ADOS_C ,spear_result 0.3517375026985564\n",
      "                                                   ADOS_C/SVR (MAE/pear/spear/CCC)\n",
      "static_feautre_LOC+dynamic_feature_LOC+dynamic_...         1.355/0.528/0.534/0.314\n",
      "static_feautre_LOC+dynamic_feature_LOC+dynamic_...         1.371/0.464/0.474/0.295\n",
      "static_feautre_LOC+dynamic_feature_LOC+dynamic_...         1.355/0.484/0.511/0.324\n",
      "static_feautre_LOC+dynamic_feature_LOC+dynamic_...         1.439/0.353/0.382/0.257\n",
      "static_feautre_LOC+dynamic_feature_LOC+dynamic_...          1.526/0.211/0.247/0.14\n",
      "static_feautre_LOC+dynamic_feature_LOC+dynamic_...         1.468/0.419/0.321/0.239\n",
      "static_feautre_LOC+dynamic_feature_LOC+dynamic_...         1.593/0.032/0.067/0.011\n",
      "static_feautre_LOC+dynamic_feature_LOC+dynamic_...       1.613/-0.023/0.018/-0.008\n",
      "static_feautre_LOC+dynamic_feature_LOC+dynamic_...         1.514/0.398/0.352/0.217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jun 30 15:56:45 2021\n",
    "\n",
    "@author: jackchen\n",
    "\n",
    "\n",
    "    This script is only for TBMEA1 \n",
    "\n",
    "\n",
    "    2022/04/19: should add feature combination code with a form of class\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import spearmanr,pearsonr \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "from addict import Dict\n",
    "# import functions\n",
    "import argparse\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn.svm\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "\n",
    "from articulation.HYPERPARAM import phonewoprosody, Label\n",
    "import articulation.HYPERPARAM.FeatureSelect as FeatSel\n",
    "\n",
    "import articulation.articulation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "import shap\n",
    "import articulation.HYPERPARAM.PaperNameMapping as PprNmeMp\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import shutil\n",
    "import scipy\n",
    "\n",
    "from articulation.HYPERPARAM.PlotFigureVars import *\n",
    "\n",
    "\n",
    "def Assert_labelfeature(feat_name,lab_name):\n",
    "    # =============================================================================\n",
    "    #     To check if the label match with feature\n",
    "    # =============================================================================\n",
    "    for i,n in enumerate(feat_name):\n",
    "        assert feat_name[i] == lab_name[i]\n",
    "\n",
    "def FilterFile_withinManualName(files,Manual_choosen_feature):\n",
    "    files_manualChoosen=[f  for f in files if os.path.basename(f).split(\".\")[0]  in Manual_choosen_feature]\n",
    "    return files_manualChoosen\n",
    "\n",
    "def Merge_dfs(df_1, df_2):\n",
    "    return pd.merge(df_1,df_2,left_index=True, right_index=True)\n",
    "def Swap2PaperName(feature_rawname,PprNmeMp):\n",
    "    if feature_rawname in PprNmeMp.Paper_name_map.keys():\n",
    "        featurename_paper=PprNmeMp.Paper_name_map[feature_rawname]\n",
    "        feature_keys=featurename_paper\n",
    "    else: \n",
    "        feature_keys=feature_rawname\n",
    "    return feature_keys\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "def Return_ALigned_dfXtestndfShapValues(Feature_SHAP_info_dict,logit_number=1):\n",
    "    #Inputs\n",
    "    # logit_numberit_number=1\n",
    "    # Feature_SHAP_info_dict=Proposed_changed_info_dict\n",
    "    ###################################\n",
    "    df_XTest_stacked=pd.DataFrame()\n",
    "    df_ShapValues_stacked=pd.DataFrame()\n",
    "    for people in Feature_SHAP_info_dict.keys():\n",
    "        df_XTest=Feature_SHAP_info_dict[people]['XTest']\n",
    "        df_ShapValues=Feature_SHAP_info_dict[people]['shap_values'].loc[logit_number]\n",
    "        df_ShapValues.name=df_XTest.name\n",
    "        df_XTest_stacked=pd.concat([df_XTest_stacked,df_XTest],axis=1)\n",
    "        df_ShapValues_stacked=pd.concat([df_ShapValues_stacked,df_ShapValues],axis=1)\n",
    "    return df_XTest_stacked,df_ShapValues_stacked\n",
    "def Calculate_XTestShape_correlation(Feature_SHAP_info_dict,logit_number=1):\n",
    "    df_XTest_stacked,df_ShapValues_stacked=Return_ALigned_dfXtestndfShapValues(Feature_SHAP_info_dict,logit_number=logit_number)\n",
    "    Correlation_XtestnShap={}\n",
    "    for features in df_XTest_stacked.index:\n",
    "        r,p=pearsonr(df_XTest_stacked.loc[features],df_ShapValues_stacked.loc[features])\n",
    "        Correlation_XtestnShap[features]=r\n",
    "    df_Correlation_XtestnShap=pd.DataFrame.from_dict(Correlation_XtestnShap,orient='index')\n",
    "    df_Correlation_XtestnShap.columns=['correlation w logit:{}'.format(logit_number)]\n",
    "    return df_Correlation_XtestnShap\n",
    "def CCC_numpy(y_true, y_pred):\n",
    "    '''Reference numpy implementation of Lin's Concordance correlation coefficient'''\n",
    "    \n",
    "    # covariance between y_true and y_pred\n",
    "    s_xy = np.cov([y_true, y_pred])[0,1]\n",
    "    # means\n",
    "    x_m = np.mean(y_true)\n",
    "    y_m = np.mean(y_pred)\n",
    "    # variances\n",
    "    s_x_sq = np.var(y_true)\n",
    "    s_y_sq = np.var(y_pred)\n",
    "    \n",
    "    # condordance correlation coefficient\n",
    "    ccc = (2.0*s_xy) / (s_x_sq + s_y_sq + (x_m-y_m)**2)\n",
    "    \n",
    "    return ccc\n",
    "\n",
    "def MAE_df(a,b, Sum=False):\n",
    "    assert type(a) == type(b)\n",
    "    if type(a)==pd.core.series.Series:\n",
    "        if Sum == True:\n",
    "            return np.abs(a - b).sum()\n",
    "        else:\n",
    "            return np.abs(a - b).mean()\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    # we add compulsary arguments as named arguments for readability\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Feature_mode', default='Customized_feature',\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--preprocess', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--start_point', default=-1,\n",
    "                        help='In case that the program stop at certain point, we can resume the progress by setting this variable')\n",
    "    parser.add_argument('--experiment', default='gop_exp_ADOShappyDAAIKidallDeceiptformosaCSRC',\n",
    "                        help='If the mode is set to Session_phone_phf, you may need to determine the experiment used to generate the gop feature')\n",
    "    parser.add_argument('--pseudo', default=False,\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--suffix', default=\"\",\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--logit_number', default=0,\n",
    "                        help='')\n",
    "    parser.add_argument('--Print_Analysis_grp_Manual_select', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--Plot', default=False,\n",
    "                        help='')\n",
    "    parser.add_argument('--selectModelScoring', default='neg_mean_squared_error',\n",
    "                        help='')\n",
    "    parser.add_argument('--Mergefeatures', default=False,\n",
    "                        help='')\n",
    "    parser.add_argument('--knn_weights', default='uniform',\n",
    "                            help='path of the base directory')\n",
    "    parser.add_argument('--knn_neighbors', default=2,  type=int,\n",
    "                            help='path of the base directory')\n",
    "    parser.add_argument('--Reorder_type', default='DKIndividual',\n",
    "                            help='[DKIndividual, DKcriteria]')\n",
    "    parser.add_argument('--FeatureComb_mode', default='Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation',\n",
    "                            help='[Add_UttLvl_feature, feat_comb3, feat_comb5, feat_comb6,feat_comb7, baselineFeats,Comb_dynPhonation,Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation]')\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "start_point=args.start_point\n",
    "experiment=args.experiment\n",
    "knn_weights=args.knn_weights\n",
    "knn_neighbors=args.knn_neighbors\n",
    "Reorder_type=args.Reorder_type\n",
    "logit_number=args.logit_number\n",
    "\n",
    "\n",
    "# label_choose=['ADOS_C','Multi1','Multi2','Multi3','Multi4']\n",
    "# label_choose=['ADOS_S','ADOS_C']\n",
    "# label_choose=['ADOS_D']\n",
    "label_choose=['ADOS_C']\n",
    "# label_choose=['ADOS_S']\n",
    "# label_choose=['ADOS_cate','ASDTD']\n",
    "\n",
    "pearson_scorer = make_scorer(pearsonr, greater_is_better=False)\n",
    "\n",
    "df_formant_statistics_CtxPhone_collect_dict=Dict()\n",
    "#%%\n",
    "# =============================================================================\n",
    "\n",
    "class ADOSdataset():\n",
    "    def __init__(self,):\n",
    "        self.featurepath='Features'            \n",
    "        self.N=2\n",
    "        self.LabelType=Dict()\n",
    "        self.LabelType['ADOS_S']='regression'\n",
    "        self.LabelType['ADOS_C']='regression'\n",
    "        self.LabelType['ADOS_D']='regression'\n",
    "        self.LabelType['ADOS_cate']='classification'\n",
    "        self.LabelType['ASDTD']='classification'\n",
    "        self.Fractionfeatures_str='Features/artuculation_AUI/Vowels/Fraction/*.pkl'    \n",
    "        self.FeatureCombs=Dict()\n",
    "        # self.FeatureCombs['TD_normal vs ASDSevere_agesexmatch']=['df_formant_statistic_TD_normal', 'df_formant_statistic_agesexmatch_ASDSevere']\n",
    "        # self.FeatureCombs['TD_normal vs ASDMild_agesexmatch']=['df_formant_statistic_TD_normal', 'df_formant_statistic_agesexmatch_ASDMild']\n",
    "        # self.FeatureCombs['Notautism vs ASD']=['df_formant_statistic_77_Notautism', 'df_formant_statistic_77_ASD']\n",
    "        # self.FeatureCombs['ASD vs Autism']=['df_formant_statistic_77_ASD', 'df_formant_statistic_77_Autism']\n",
    "        # self.FeatureCombs['Notautism vs Autism']=['df_formant_statistic_77_Notautism', 'df_formant_statistic_77_Autism']\n",
    "    \n",
    "        # self._FeatureBuild()\n",
    "    def Get_FormantAUI_feat(self,label_choose,pickle_path,featuresOfInterest=['MSB_f1','MSB_f2','MSB_mix'],filterbyNum=True,**kwargs):\n",
    "        self.featuresOfInterest=featuresOfInterest\n",
    "        arti=articulation.articulation.Articulation()\n",
    "        if not kwargs and len(pickle_path)>0:\n",
    "            df_tmp=pickle.load(open(pickle_path,\"rb\")).sort_index()\n",
    "            # df_tmp=pickle.load(open(pickle_path,\"rb\"))\n",
    "        elif len(kwargs)>0: # usage Get_FormantAUI_feat(...,key1=values1):\n",
    "            for k, v in kwargs.items(): #there will be only one element\n",
    "                df_tmp=kwargs[k].sort_index()\n",
    "\n",
    "        if filterbyNum:\n",
    "            df_tmp=arti.BasicFilter_byNum(df_tmp,N=self.N)\n",
    "        \n",
    "        # if label_choose not in df_tmp.columns:\n",
    "        #     # print(\"len(df_tmp): \", len(df_tmp))\n",
    "        #     # print(\"Feature name = \", os.path.basename(pickle_path))\n",
    "        #     for people in df_tmp.index:\n",
    "        #         lab=Label.label_raw[label_choose][Label.label_raw['name']==people]\n",
    "        #         df_tmp.loc[people,'ADOS']=lab.values\n",
    "        #     df_y=df_tmp['ADOS'] #Still keep the form of dataframe\n",
    "        # else:\n",
    "        #     df_y=df_tmp[label_choose] #Still keep the form of dataframe\n",
    "        \n",
    "        # Always update the label from Label\n",
    "        for people in df_tmp.index:\n",
    "            lab=Label.label_raw[label_choose][Label.label_raw['name']==people]\n",
    "            df_tmp.loc[people,'ADOS']=lab.values\n",
    "        df_y=df_tmp['ADOS'] #Still keep the form of dataframe\n",
    "        \n",
    "        \n",
    "        feature_array=df_tmp[featuresOfInterest]\n",
    "        \n",
    "            \n",
    "        LabType=self.LabelType[label_choose]\n",
    "        return feature_array, df_y, LabType\n",
    "    def _FeatureBuild(self):\n",
    "        Features=Dict()\n",
    "        Features_comb=Dict()\n",
    "        files = glob.glob(self.Fractionfeatures_str)\n",
    "        for file in files:\n",
    "            feat_name=os.path.basename(file).replace(\".pkl\",\"\")\n",
    "            df_tmp=pickle.load(open(file,\"rb\")).sort_index()\n",
    "            Features[feat_name]=df_tmp\n",
    "        for keys in self.FeatureCombs.keys():\n",
    "            combF=[Features[k] for k in self.FeatureCombs[keys]]\n",
    "            Features_comb[keys]=pd.concat(combF)\n",
    "        \n",
    "        self.Features_comb=Features_comb\n",
    "\n",
    "\n",
    "ados_ds=ADOSdataset()\n",
    "ErrorFeat_bookeep=Dict()\n",
    "Session_level_all=Dict()\n",
    "\n",
    "# All_combination_keys=[key2 for key in All_combinations.keys() for key2 in sorted(All_combinations[key], key=len, reverse=True)]\n",
    "Top_ModuleColumn_mapping_dict={}\n",
    "Top_ModuleColumn_mapping_dict['Add_UttLvl_feature']=FeatSel.Columns_comb2.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb']=FeatSel.Columns_comb.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb3']=FeatSel.Columns_comb3.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb5']=FeatSel.Columns_comb5.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb6']=FeatSel.Columns_comb6.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb7']=FeatSel.Columns_comb7.copy()\n",
    "Top_ModuleColumn_mapping_dict['feat_comb8']=FeatSel.Columns_comb8.copy()\n",
    "Top_ModuleColumn_mapping_dict['Comb_dynPhonation']=FeatSel.Comb_dynPhonation.copy()\n",
    "Top_ModuleColumn_mapping_dict['Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']=FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation.copy()\n",
    "Top_ModuleColumn_mapping_dict['baselineFeats']=FeatSel.Baseline_comb.copy()\n",
    "\n",
    "featuresOfInterest=Top_ModuleColumn_mapping_dict[args.FeatureComb_mode]\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 如果要做全盤的實驗的話用這一區\n",
    "# FeatureLabelMatch_manual=[]\n",
    "# All_combinations=featuresOfInterest\n",
    "# # All_combinations4=FeatSel.Columns_comb4.copy()\n",
    "# for key_layer1 in All_combinations.keys():\n",
    "#     for key_layer2 in All_combinations[key_layer1].keys():\n",
    "        \n",
    "#         # if 'Utt_prosodyF0_VoiceQuality_energy' in key_layer2:\n",
    "#         #     FeatureLabelMatch_manual.append('{0}-{1}'.format(key_layer1,key_layer2))\n",
    "#         FeatureLabelMatch_manual.append('{0}-{1}'.format(key_layer1,key_layer2))\n",
    "\n",
    "# XXX 2. 如果要手動設定實驗的話用這一區\n",
    "FeatureLabelMatch_manual=[\n",
    "    # Rule: {layer1}-{layer2}\n",
    "    'static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOC_columns+DEP_columns+LOCDEP_Trend_D_cols+LOCDEP_Syncrony_cols',\n",
    "    'static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOC_columns+DEP_columns+LOCDEP_Trend_D_cols',\n",
    "    'static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOC_columns+DEP_columns+LOCDEP_Syncrony_cols',\n",
    "    'static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOC_columns+DEP_columns',\n",
    "    'static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOC_columns',\n",
    "    'static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-DEP_columns',\n",
    "    'static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOCDEP_Trend_D_cols',\n",
    "    'static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-LOCDEP_Syncrony_cols',\n",
    "    'static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation-Phonation_Trend_K_cols',\n",
    "    ]\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "if args.FeatureComb_mode == 'Add_UttLvl_feature':\n",
    "    Merge_feature_path='RegressionMerged_dfs/ADDed_UttFeat/{knn_weights}_{knn_neighbors}_{Reorder_type}/ASD_DOCKID/'.format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type)\n",
    "else:\n",
    "    Merge_feature_path='RegressionMerged_dfs/{knn_weights}_{knn_neighbors}_{Reorder_type}/ASD_DOCKID/'.format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type)\n",
    "\n",
    "for exp_str in FeatureLabelMatch_manual:\n",
    "    Layer1Feat, Layer2Feat=exp_str.split(\"-\")\n",
    "    \n",
    "    # load features from file\n",
    "    data=ados_ds.featurepath +'/'+ Merge_feature_path+'{}.pkl'.format(Layer1Feat)\n",
    "    \n",
    "\n",
    "    feat_col_ = featuresOfInterest[Layer1Feat][Layer2Feat] # ex: ['MSB_f1']\n",
    "    for lab_ in label_choose:\n",
    "        X,y, featType=ados_ds.Get_FormantAUI_feat(label_choose=lab_,pickle_path=data,featuresOfInterest=feat_col_,filterbyNum=False)\n",
    "        Item_name=\"{feat}::{lab}\".format(feat='-'.join([Layer1Feat]+[Layer2Feat]),lab=lab_)\n",
    "        Session_level_all[Item_name].X, \\\n",
    "            Session_level_all[Item_name].y, \\\n",
    "                Session_level_all[Item_name].feattype = X,y, featType\n",
    "    assert len(X.columns) >0\n",
    "    assert y.isna().any() !=True\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Feature merging function\n",
    "    \n",
    "    Ths slice of code provide user to manually make functions to combine df_XXX_infos\n",
    "\n",
    "'''\n",
    "# =============================================================================\n",
    "if args.Mergefeatures:\n",
    "    Merg_filepath={}\n",
    "    Merg_filepath['static_feautre_LOC']='Features/artuculation_AUI/Vowels/Formants/Formant_AUI_tVSAFCRFvals_KID_FromASD_DOCKID.pkl'\n",
    "    Merg_filepath['static_feautre_phonation']='Features/artuculation_AUI/Vowels/Phonation/Phonation_meanvars_KID_FromASD_DOCKID.pkl'\n",
    "    Merg_filepath['dynamic_feature_LOC']='Features/artuculation_AUI/Interaction/Formants/Syncrony_measure_of_variance_DKIndividual_ASD_DOCKID.pkl'\n",
    "    Merg_filepath['dynamic_feature_phonation']='Features/artuculation_AUI/Interaction/Phonation/Syncrony_measure_of_variance_phonation_ASD_DOCKID.pkl'\n",
    "    \n",
    "    merge_out_path='Features/RegressionMerged_dfs/'\n",
    "    if not os.path.exists(merge_out_path):\n",
    "        os.makedirs(merge_out_path)\n",
    "    \n",
    "    df_infos_dict=Dict()\n",
    "    for keys, paths in Merg_filepath.items():\n",
    "        df_infos_dict[keys]=pickle.load(open(paths,\"rb\")).sort_index()\n",
    "    \n",
    "    Merged_df_dict=Dict()\n",
    "    comb1 = list(combinations(list(Merg_filepath.keys()), 1))\n",
    "    comb2 = list(combinations(list(Merg_filepath.keys()), 2))\n",
    "    for c in comb1:\n",
    "        e1=c[0]\n",
    "        Merged_df_dict[e1]=df_infos_dict[e1]\n",
    "        OutPklpath=merge_out_path+ e1 + \".pkl\"\n",
    "        pickle.dump(Merged_df_dict[e1],open(OutPklpath,\"wb\"))\n",
    "        \n",
    "        \n",
    "    for c in comb2:\n",
    "        e1, e2=c\n",
    "        Merged_df_dict['+'.join(c)]=Merge_dfs(df_infos_dict[e1],df_infos_dict[e2])\n",
    "        \n",
    "        OutPklpath=merge_out_path+'+'.join(c)+\".pkl\"\n",
    "        pickle.dump(Merged_df_dict['+'.join(c)],open(OutPklpath,\"wb\"))\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Model parameters\n",
    "# =============================================================================\n",
    "\n",
    "epsilon=np.array([0.001,0.01,0.1,1,5,10.0,25,50,75,100])\n",
    "\n",
    "n_estimator=[2, 4, 8, 16, 32, 64]\n",
    "\n",
    "\n",
    "Classifier={}\n",
    "loo=LeaveOneOut()\n",
    "\n",
    "\n",
    "# CV_settings=loo\n",
    "CV_settings=10\n",
    "\n",
    "Classifier['SVR']={'model':sklearn.svm.SVR(),\\\n",
    "                  'parameters':{\n",
    "                    'model__epsilon': epsilon,\\\n",
    "                    # 'model__C':C_variable,\\\n",
    "                    'model__kernel': ['rbf'],\\\n",
    "                    # 'gamma': ['auto'],\\\n",
    "                                }}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    BookKeep area\n",
    "\n",
    "'''\n",
    "Best_predict_optimize={}\n",
    "\n",
    "df_best_result_r2=pd.DataFrame([])\n",
    "df_best_result_pear=pd.DataFrame([])\n",
    "df_best_result_spear=pd.DataFrame([])\n",
    "df_best_cross_score=pd.DataFrame([])\n",
    "df_best_result_UAR=pd.DataFrame([])\n",
    "df_best_result_AUC=pd.DataFrame([])\n",
    "df_best_result_f1=pd.DataFrame([])\n",
    "df_best_result_allThreeClassifiers=pd.DataFrame([])\n",
    "# =============================================================================\n",
    "Result_path=\"RESULTS/\"\n",
    "\n",
    "if not os.path.exists(Result_path):\n",
    "    os.makedirs(Result_path)\n",
    "final_result_file=\"_ADOS_{}.xlsx\".format(args.suffix)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "count=0\n",
    "OutFeature_dict=Dict()\n",
    "Best_param_dict=Dict()\n",
    "\n",
    "# ''' 要手動執行一次從Incorrect2Correct_indexes和Correct2Incorrect_indexes決定哪些indexes 需要算shap value 再在這邊指定哪些fold需要停下來算SHAP value '''\n",
    "SHAP_inspect_idxs_manual=[]\n",
    "# SHAP_inspect_idxs_manual=None # None means calculate SHAP value of all people\n",
    "# SHAP_inspect_idxs_manual=[1,3,5] # empty list means we do not execute shap function\n",
    "for clf_keys, clf in Classifier.items(): #Iterate among different classifiers \n",
    "    writer_clf = pd.ExcelWriter(Result_path+\"/\"+clf_keys+\"_\"+args.Feature_mode+\"_\"+final_result_file, engine = 'xlsxwriter')\n",
    "    for feature_lab_str, features in Session_level_all.items():\n",
    "        feature_keys, label_keys= feature_lab_str.split(\"::\")\n",
    "        feature_rawname=feature_keys[feature_keys.find('-')+1:]\n",
    "        feature_filename=feature_keys[:feature_keys.find('-')]\n",
    "        if feature_rawname in PprNmeMp.Paper_name_map.keys():\n",
    "            featurename_paper=PprNmeMp.Paper_name_map[feature_rawname]\n",
    "            feature_keys=feature_keys.replace(feature_rawname,featurename_paper)\n",
    "        \n",
    "        if SHAP_inspect_idxs_manual != None:\n",
    "            SHAP_inspect_idxs=SHAP_inspect_idxs_manual\n",
    "        else:\n",
    "            SHAP_inspect_idxs=range(len(features.y))\n",
    "        \n",
    "        Labels = Session_level_all.X[feature_keys]\n",
    "        print(\"=====================Cross validation start==================\")\n",
    "        pipe = Pipeline(steps=[('scalar',StandardScaler()),(\"model\", clf['model'])])\n",
    "        p_grid=clf['parameters']\n",
    "        Gclf = GridSearchCV(estimator=pipe, param_grid=p_grid, scoring=args.selectModelScoring, cv=CV_settings, refit=True, n_jobs=-1)\n",
    "        Gclf_manual = GridSearchCV(estimator=pipe, param_grid=p_grid, scoring=args.selectModelScoring, cv=CV_settings, refit=True, n_jobs=-1)\n",
    "        # Score=cross_val_score(Gclf, features.X, features.y, cv=CV_settings, scoring=pearson_scorer) \n",
    "        CVpredict=cross_val_predict(Gclf, features.X, features.y, cv=CV_settings)  \n",
    "        \n",
    "        # The cv is as the one in cross_val_predict function\n",
    "        cv = sklearn.model_selection.check_cv(CV_settings,features.y,classifier=sklearn.base.is_classifier(Gclf))\n",
    "        splits = list(cv.split(features.X, features.y, groups=None))\n",
    "        test_indices = np.concatenate([test for _, test in splits])\n",
    "\n",
    "        \n",
    "        CVpredict_manual=np.zeros(len(features.y))\n",
    "        for i, (train_index, test_index) in enumerate(splits):\n",
    "            X_train, X_test = features.X.iloc[train_index], features.X.iloc[test_index]\n",
    "            y_train, y_test = features.y.iloc[train_index], features.y.iloc[test_index]\n",
    "            Gclf_manual.fit(X_train,y_train)\n",
    "            result_bestmodel=Gclf_manual.predict(X_test)\n",
    "            \n",
    "            CVpredict_manual[test_index]=result_bestmodel\n",
    "            \n",
    "            # result_bestmodel_fitted_again=best_model_fittedagain.predict(X_test_encoded)\n",
    "            CVpred_fromFunction=CVpredict[test_index]\n",
    "            \n",
    "\n",
    "            # If the indexes we want to examine are in that fold, store the whole fold\n",
    "            # 先把整個fold記錄下來然後在analysis area再拆解\n",
    "            SHAP_exam_lst=[i for i in test_index if i in SHAP_inspect_idxs]\n",
    "            if len(SHAP_exam_lst) != 0:\n",
    "                explainer = shap.KernelExplainer(Gclf_manual.predict, X_train)\n",
    "                # explainer(X_test)\n",
    "                shap_values = explainer.shap_values(X_test)\n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].explainer_expected_value=explainer.expected_value\n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].shap_values=shap_values # shap_values= [logit, index, feature]\n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].XTest=X_test\n",
    "                # Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index)].testIndex=test_index\n",
    "            # shap.force_plot(explainer.expected_value[logit_number], shap_values[logit_number][inspect_sample,:], X_test.iloc[inspect_sample,:], matplotlib=True,show=False)\n",
    "            \n",
    "            \n",
    "            # assert (result_bestmodel==result_bestmodel_fitted_again).all()\n",
    "            assert (result_bestmodel==CVpred_fromFunction).all()\n",
    "        \n",
    "            \n",
    "        assert (CVpredict_manual==CVpredict).all()\n",
    "        Session_level_all[feature_lab_str]['y_pred']=CVpredict_manual\n",
    "        Session_level_all[feature_lab_str]['y_true']=features.y\n",
    "\n",
    "\n",
    "        \n",
    "        Gclf.fit(features.X,features.y)\n",
    "        # if clf_keys == \"EN\":\n",
    "        #     print('The coefficient of best estimator is: ',Gclf.best_estimator_.coef_)\n",
    "        \n",
    "        print(\"The best score with scoring parameter: 'r2' is\", Gclf.best_score_)\n",
    "        print(\"The best parameters are :\", Gclf.best_params_)\n",
    "        best_parameters=Gclf.best_params_\n",
    "        best_score=Gclf.best_score_\n",
    "        best_parameters.update({'best_score':best_score})\n",
    "        Best_param_dict[feature_lab_str]=best_parameters\n",
    "        cv_results_info=Gclf.cv_results_\n",
    "\n",
    "        \n",
    "        \n",
    "        if features.feattype == 'regression':\n",
    "            r2=r2_score(features.y,CVpredict )\n",
    "            n,p=features.X.shape\n",
    "            r2_adj=1-(1-r2)*(n-1)/(n-p-1)\n",
    "            # MSE=sklearn.metrics.mean_squared_error(features.y.values.ravel(),CVpredict)\n",
    "            MAE=sklearn.metrics.mean_absolute_error(features.y.values.ravel(),CVpredict)\n",
    "            pearson_result, pearson_p=pearsonr(features.y,CVpredict )\n",
    "            spear_result, spearman_p=spearmanr(features.y,CVpredict )\n",
    "            CCC = CCC_numpy(features.y, CVpredict)\n",
    "            print('Feature {0}, label {1} ,spear_result {2}'.format(feature_keys, label_keys,spear_result))\n",
    "        elif features.feattype == 'classification':\n",
    "            n,p=features.X.shape\n",
    "            UAR=recall_score(features.y, CVpredict, average='macro')\n",
    "            AUC=roc_auc_score(features.y, CVpredict)\n",
    "            f1Score=f1_score(features.y, CVpredict, average='macro')\n",
    "            print('Feature {0}, label {1} ,UAR {2}'.format(feature_keys, label_keys,UAR))\n",
    "        \n",
    "        if args.Plot and p <2:\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 10), sharey=True)\n",
    "            kernel_label = [clf_keys]\n",
    "            model_color = ['m']\n",
    "            # axes.plot((features.X - min(features.X) )/ max(features.X), Gclf.best_estimator_.fit(features.X,features.y).predict(features.X), color=model_color[0],\n",
    "            #               label='CV Predict')\n",
    "            axes.scatter((features.X.values - min(features.X.values) )/ max(features.X.values), CVpredict, \n",
    "                         facecolor=\"none\", edgecolor=\"k\", s=150,\n",
    "                         label='{}'.format(feature_lab_str)\n",
    "                         )\n",
    "            axes.scatter((features.X.values - min(features.X.values) )/ max(features.X.values), features.y, \n",
    "                         facecolor=\"none\", edgecolor=\"r\", s=50,\n",
    "                         label='Real Y')\n",
    "            axes.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
    "                    ncol=1, fancybox=True, shadow=True)\n",
    "            \n",
    "            Plot_path='./Plot/LinearRegress/'\n",
    "            if not os.path.exists(Plot_path):\n",
    "                os.makedirs(Plot_path)\n",
    "            plot_file=Plot_path+\"/{0}_{1}.png\".format(clf_keys,feature_lab_str)\n",
    "            plt.savefig(plot_file, dpi=200) \n",
    "        \n",
    "        # =============================================================================\n",
    "        '''\n",
    "            Inspect the best result\n",
    "        '''\n",
    "        # =============================================================================\n",
    "        # Best_predict_optimize[label_keys]=pd.DataFrame(np.vstack((CVpredict,features.y)).T,columns=['y_pred','y'])\n",
    "        # excel_path='./Statistics/prediction_result'\n",
    "        # if not os.path.exists(excel_path):\n",
    "        #     os.makedirs(excel_path)\n",
    "        # excel_file=excel_path+\"/{0}_{1}.xlsx\"\n",
    "        # writer = pd.ExcelWriter(excel_file.format(clf_keys,feature_keys.replace(\":\",\"\")), engine = 'xlsxwriter')\n",
    "        # for label_name in  Best_predict_optimize.keys():\n",
    "        #     Best_predict_optimize[label_name].to_excel(writer,sheet_name=label_name.replace(\"/\",\"_\"))\n",
    "        # writer.save()\n",
    "                                \n",
    "        # ================================================      =============================\n",
    "        if features.feattype == 'regression':\n",
    "            df_best_result_r2.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(r2_adj,3),np.round(np.nan,6))\n",
    "            df_best_result_pear.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(pearson_result,3),np.round(pearson_p,6))\n",
    "            df_best_result_spear.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(spear_result,3),np.round(spearman_p,6))\n",
    "            df_best_result_spear.loc[feature_keys,'de-zero_num']=len(features.X)\n",
    "            # df_best_cross_score.loc[feature_keys,label_keys]=Score.mean()\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1} (MAE/pear/spear/CCC)'.format(label_keys,clf_keys)]\\\n",
    "                        ='{0}/{1}/{2}/{3}'.format(np.round(MAE,3),np.round(pearson_result,3),np.round(spear_result,3),np.round(CCC,3))\n",
    "\n",
    "        elif features.feattype == 'classification':\n",
    "            df_best_result_UAR.loc[feature_keys,label_keys]='{0}'.format(UAR)\n",
    "            df_best_result_AUC.loc[feature_keys,label_keys]='{0}'.format(AUC)\n",
    "            df_best_result_f1.loc[feature_keys,label_keys]='{0}'.format(f1Score)\n",
    "            # df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1} (UAR/AUC/f1score)'.format(label_keys,clf_keys)]\\\n",
    "            #             ='{0}/{1}/{2}'.format(np.round(UAR,3),np.round(AUC,3),np.round(f1Score,3))\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1}'.format(label_keys,clf_keys)]\\\n",
    "                        ='{0}'.format(np.round(UAR,3))\n",
    "    \n",
    "    if features.feattype == 'regression':\n",
    "        df_best_result_r2.to_excel(writer_clf,sheet_name=\"R2_adj\")\n",
    "        df_best_result_pear.to_excel(writer_clf,sheet_name=\"pear\")\n",
    "        df_best_result_spear.to_excel(writer_clf,sheet_name=\"spear\")\n",
    "        df_best_result_spear.to_csv(Result_path+\"/\"+clf_keys+\"_\"+args.Feature_mode+\"_spearman.csv\")\n",
    "    elif features.feattype == 'classification':\n",
    "        df_best_result_UAR.to_excel(writer_clf,sheet_name=\"UAR\")\n",
    "        df_best_result_AUC.to_excel(writer_clf,sheet_name=\"AUC\")\n",
    "        df_best_result_f1.to_excel(writer_clf,sheet_name=\"f1\")\n",
    "\n",
    "# TASLP table.5 fusion的部份\n",
    "writer_clf.save()\n",
    "print(df_best_result_allThreeClassifiers)\n",
    "df_best_result_allThreeClassifiers.to_excel(Result_path+\"/\"+\"Regression_{knn_weights}_{knn_neighbors}_{Reorder_type}.xlsx\".format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
