{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jack/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================Cross validation start==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.06it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.12it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.09it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.15it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.89it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.96it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.98it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score with scoring parameter: 'r2' is 0.8\n",
      "The best parameters are : {'model__C': 5.0, 'model__kernel': 'rbf', 'model__probability': True, 'model__random_state': 1}\n",
      "Feature TD vs df_feature_lowMinimal_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Syncrony_cols+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols, label ASDTD ,UAR 0.8321428571428571\n",
      "=====================Cross validation start==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.88it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.85it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.89it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.86it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.63it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.72it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.81it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.76it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.78it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score with scoring parameter: 'r2' is 0.675\n",
      "The best parameters are : {'model__C': 1.0, 'model__kernel': 'rbf', 'model__probability': True, 'model__random_state': 1}\n",
      "Feature TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols, label ASDTD ,UAR 0.6785714285714286\n",
      "=====================Cross validation start==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.21it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.17it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.24it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.23it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.10it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.16it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.09it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.04it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.08it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score with scoring parameter: 'r2' is 0.825\n",
      "The best parameters are : {'model__C': 1.0, 'model__kernel': 'rbf', 'model__probability': True, 'model__random_state': 1}\n",
      "Feature TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols, label ASDTD ,UAR 0.8428571428571429\n",
      "=====================Cross validation start==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.25it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.26it/s]\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.19it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.27it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.26it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.25it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.28it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.24it/s]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.27it/s]\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score with scoring parameter: 'r2' is 0.7083333333333333\n",
      "The best parameters are : {'model__C': 5.0, 'model__kernel': 'rbf', 'model__probability': True, 'model__random_state': 1}\n",
      "Feature TD vs df_feature_moderate_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Trend_K_cols+Phonation_Proximity_cols, label ASDTD ,UAR 0.7115384615384616\n",
      "=====================Cross validation start==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.25it/s]\n",
      "100%|██████████| 6/6 [00:03<00:00,  2.00it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.01it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.05it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.12it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.01it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.03it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.16it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.04it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score with scoring parameter: 'r2' is 0.8083333333333333\n",
      "The best parameters are : {'model__C': 1.0, 'model__kernel': 'rbf', 'model__probability': True, 'model__random_state': 1}\n",
      "Feature TD vs df_feature_moderate_CSS >> Phonation_Proximity_cols, label ASDTD ,UAR 0.8102564102564103\n",
      "=====================Cross validation start==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.08it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.06it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.06it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.99it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score with scoring parameter: 'r2' is 0.7875\n",
      "The best parameters are : {'model__C': 1.0, 'model__kernel': 'rbf', 'model__probability': True, 'model__random_state': 1}\n",
      "Feature TD vs df_feature_high_CSS >> DEP_columns+Phonation_Proximity_cols, label ASDTD ,UAR 0.7689393939393939\n",
      "=====================Cross validation start==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.16it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.15it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.22it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.34it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.23it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.33it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.24it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.20it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score with scoring parameter: 'r2' is 0.8375\n",
      "The best parameters are : {'model__C': 1.0, 'model__kernel': 'rbf', 'model__probability': True, 'model__random_state': 1}\n",
      "Feature TD vs df_feature_high_CSS >> Phonation_Proximity_cols, label ASDTD ,UAR 0.843939393939394\n",
      "df_allThreeClassifiers_paperName generated at  RESULTS//TASLPTABLE-Class_Norm[func15].xlsx\n",
      "proposed_expstr='TD vs df_feature_lowMinimal_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Syncrony_cols+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
      "proposed_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
      "proposed_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
      "baseline_expstr='TD vs df_feature_moderate_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
      "baseline_expstr='TD vs df_feature_moderate_CSS >> Phonation_Proximity_cols::ASDTD'\n",
      "baseline_expstr='TD vs df_feature_high_CSS >> DEP_columns+Phonation_Proximity_cols::ASDTD'\n",
      "baseline_expstr='TD vs df_feature_high_CSS >> Phonation_Proximity_cols::ASDTD'\n",
      "For Task lowMinimal  additional feature sets are {'Phonation_Trend_D_cols', 'LOC_columns', 'LOCDEP_Trend_D_cols', 'LOCDEP_Syncrony_cols', 'Phonation_Proximity_cols'}\n",
      "Incorrect2Correct_indexes:  [21]\n",
      "Correct2Incorrect_indexes:  [10]\n",
      "testing sample  0 is in the  0 position of test fold 0_1_11_13\n",
      "testing sample  1 is in the  1 position of test fold 0_1_11_13\n",
      "testing sample  2 is in the  0 position of test fold 2_3_16_17\n",
      "testing sample  3 is in the  1 position of test fold 2_3_16_17\n",
      "testing sample  4 is in the  0 position of test fold 4_5_18_19\n",
      "testing sample  5 is in the  1 position of test fold 4_5_18_19\n",
      "testing sample  6 is in the  0 position of test fold 6_7_20_21\n",
      "testing sample  7 is in the  1 position of test fold 6_7_20_21\n",
      "testing sample  8 is in the  0 position of test fold 8_22_23\n",
      "testing sample  9 is in the  0 position of test fold 9_24_25\n",
      "testing sample  10 is in the  0 position of test fold 10_26_27\n",
      "testing sample  11 is in the  2 position of test fold 0_1_11_13\n",
      "testing sample  12 is in the  0 position of test fold 12_28_29\n",
      "testing sample  13 is in the  3 position of test fold 0_1_11_13\n",
      "testing sample  14 is in the  0 position of test fold 14_30_31\n",
      "testing sample  15 is in the  0 position of test fold 15_32_33\n",
      "testing sample  16 is in the  2 position of test fold 2_3_16_17\n",
      "testing sample  17 is in the  3 position of test fold 2_3_16_17\n",
      "testing sample  18 is in the  2 position of test fold 4_5_18_19\n",
      "testing sample  19 is in the  3 position of test fold 4_5_18_19\n",
      "testing sample  20 is in the  2 position of test fold 6_7_20_21\n",
      "testing sample  21 is in the  3 position of test fold 6_7_20_21\n",
      "testing sample  22 is in the  1 position of test fold 8_22_23\n",
      "testing sample  23 is in the  2 position of test fold 8_22_23\n",
      "testing sample  24 is in the  1 position of test fold 9_24_25\n",
      "testing sample  25 is in the  2 position of test fold 9_24_25\n",
      "testing sample  26 is in the  1 position of test fold 10_26_27\n",
      "testing sample  27 is in the  2 position of test fold 10_26_27\n",
      "testing sample  28 is in the  1 position of test fold 12_28_29\n",
      "testing sample  29 is in the  2 position of test fold 12_28_29\n",
      "testing sample  30 is in the  1 position of test fold 14_30_31\n",
      "testing sample  31 is in the  2 position of test fold 14_30_31\n",
      "testing sample  32 is in the  1 position of test fold 15_32_33\n",
      "testing sample  33 is in the  2 position of test fold 15_32_33\n",
      "testing sample  0 is in the  0 position of test fold 0_1_11_13\n",
      "testing sample  1 is in the  1 position of test fold 0_1_11_13\n",
      "testing sample  2 is in the  0 position of test fold 2_3_16_17\n",
      "testing sample  3 is in the  1 position of test fold 2_3_16_17\n",
      "testing sample  4 is in the  0 position of test fold 4_5_18_19\n",
      "testing sample  5 is in the  1 position of test fold 4_5_18_19\n",
      "testing sample  6 is in the  0 position of test fold 6_7_20_21\n",
      "testing sample  7 is in the  1 position of test fold 6_7_20_21\n",
      "testing sample  8 is in the  0 position of test fold 8_22_23\n",
      "testing sample  9 is in the  0 position of test fold 9_24_25\n",
      "testing sample  10 is in the  0 position of test fold 10_26_27\n",
      "testing sample  11 is in the  2 position of test fold 0_1_11_13\n",
      "testing sample  12 is in the  0 position of test fold 12_28_29\n",
      "testing sample  13 is in the  3 position of test fold 0_1_11_13\n",
      "testing sample  14 is in the  0 position of test fold 14_30_31\n",
      "testing sample  15 is in the  0 position of test fold 15_32_33\n",
      "testing sample  16 is in the  2 position of test fold 2_3_16_17\n",
      "testing sample  17 is in the  3 position of test fold 2_3_16_17\n",
      "testing sample  18 is in the  2 position of test fold 4_5_18_19\n",
      "testing sample  19 is in the  3 position of test fold 4_5_18_19\n",
      "testing sample  20 is in the  2 position of test fold 6_7_20_21\n",
      "testing sample  21 is in the  3 position of test fold 6_7_20_21\n",
      "testing sample  22 is in the  1 position of test fold 8_22_23\n",
      "testing sample  23 is in the  2 position of test fold 8_22_23\n",
      "testing sample  24 is in the  1 position of test fold 9_24_25\n",
      "testing sample  25 is in the  2 position of test fold 9_24_25\n",
      "testing sample  26 is in the  1 position of test fold 10_26_27\n",
      "testing sample  27 is in the  2 position of test fold 10_26_27\n",
      "testing sample  28 is in the  1 position of test fold 12_28_29\n",
      "testing sample  29 is in the  2 position of test fold 12_28_29\n",
      "testing sample  30 is in the  1 position of test fold 14_30_31\n",
      "testing sample  31 is in the  2 position of test fold 14_30_31\n",
      "testing sample  32 is in the  1 position of test fold 15_32_33\n",
      "testing sample  33 is in the  2 position of test fold 15_32_33\n",
      "testing sample  0 is in the  0 position of test fold 0_1_11_13\n",
      "testing sample  1 is in the  1 position of test fold 0_1_11_13\n",
      "testing sample  2 is in the  0 position of test fold 2_3_16_17\n",
      "testing sample  3 is in the  1 position of test fold 2_3_16_17\n",
      "testing sample  4 is in the  0 position of test fold 4_5_18_19\n",
      "testing sample  5 is in the  1 position of test fold 4_5_18_19\n",
      "testing sample  6 is in the  0 position of test fold 6_7_20_21\n",
      "testing sample  7 is in the  1 position of test fold 6_7_20_21\n",
      "testing sample  8 is in the  0 position of test fold 8_22_23\n",
      "testing sample  9 is in the  0 position of test fold 9_24_25\n",
      "testing sample  10 is in the  0 position of test fold 10_26_27\n",
      "testing sample  11 is in the  2 position of test fold 0_1_11_13\n",
      "testing sample  12 is in the  0 position of test fold 12_28_29\n",
      "testing sample  13 is in the  3 position of test fold 0_1_11_13\n",
      "testing sample  14 is in the  0 position of test fold 14_30_31\n",
      "testing sample  15 is in the  0 position of test fold 15_32_33\n",
      "testing sample  16 is in the  2 position of test fold 2_3_16_17\n",
      "testing sample  17 is in the  3 position of test fold 2_3_16_17\n",
      "testing sample  18 is in the  2 position of test fold 4_5_18_19\n",
      "testing sample  19 is in the  3 position of test fold 4_5_18_19\n",
      "testing sample  20 is in the  2 position of test fold 6_7_20_21\n",
      "testing sample  21 is in the  3 position of test fold 6_7_20_21\n",
      "testing sample  22 is in the  1 position of test fold 8_22_23\n",
      "testing sample  23 is in the  2 position of test fold 8_22_23\n",
      "testing sample  24 is in the  1 position of test fold 9_24_25\n",
      "testing sample  25 is in the  2 position of test fold 9_24_25\n",
      "testing sample  26 is in the  1 position of test fold 10_26_27\n",
      "testing sample  27 is in the  2 position of test fold 10_26_27\n",
      "testing sample  28 is in the  1 position of test fold 12_28_29\n",
      "testing sample  29 is in the  2 position of test fold 12_28_29\n",
      "testing sample  30 is in the  1 position of test fold 14_30_31\n",
      "testing sample  31 is in the  2 position of test fold 14_30_31\n",
      "testing sample  32 is in the  1 position of test fold 15_32_33\n",
      "testing sample  33 is in the  2 position of test fold 15_32_33\n",
      "testing sample  0 is in the  0 position of test fold 0_1_11_13\n",
      "testing sample  1 is in the  1 position of test fold 0_1_11_13\n",
      "testing sample  2 is in the  0 position of test fold 2_3_16_17\n",
      "testing sample  3 is in the  1 position of test fold 2_3_16_17\n",
      "testing sample  4 is in the  0 position of test fold 4_5_18_19\n",
      "testing sample  5 is in the  1 position of test fold 4_5_18_19\n",
      "testing sample  6 is in the  0 position of test fold 6_7_20_21\n",
      "testing sample  7 is in the  1 position of test fold 6_7_20_21\n",
      "testing sample  8 is in the  0 position of test fold 8_22_23\n",
      "testing sample  9 is in the  0 position of test fold 9_24_25\n",
      "testing sample  10 is in the  0 position of test fold 10_26_27\n",
      "testing sample  11 is in the  2 position of test fold 0_1_11_13\n",
      "testing sample  12 is in the  0 position of test fold 12_28_29\n",
      "testing sample  13 is in the  3 position of test fold 0_1_11_13\n",
      "testing sample  14 is in the  0 position of test fold 14_30_31\n",
      "testing sample  15 is in the  0 position of test fold 15_32_33\n",
      "testing sample  16 is in the  2 position of test fold 2_3_16_17\n",
      "testing sample  17 is in the  3 position of test fold 2_3_16_17\n",
      "testing sample  18 is in the  2 position of test fold 4_5_18_19\n",
      "testing sample  19 is in the  3 position of test fold 4_5_18_19\n",
      "testing sample  20 is in the  2 position of test fold 6_7_20_21\n",
      "testing sample  21 is in the  3 position of test fold 6_7_20_21\n",
      "testing sample  22 is in the  1 position of test fold 8_22_23\n",
      "testing sample  23 is in the  2 position of test fold 8_22_23\n",
      "testing sample  24 is in the  1 position of test fold 9_24_25\n",
      "testing sample  25 is in the  2 position of test fold 9_24_25\n",
      "testing sample  26 is in the  1 position of test fold 10_26_27\n",
      "testing sample  27 is in the  2 position of test fold 10_26_27\n",
      "testing sample  28 is in the  1 position of test fold 12_28_29\n",
      "testing sample  29 is in the  2 position of test fold 12_28_29\n",
      "testing sample  30 is in the  1 position of test fold 14_30_31\n",
      "testing sample  31 is in the  2 position of test fold 14_30_31\n",
      "testing sample  32 is in the  1 position of test fold 15_32_33\n",
      "testing sample  33 is in the  2 position of test fold 15_32_33\n",
      "base value 0.167 %\n",
      "0.167 + -0.02 + -0.08 + -0.132 + -0.191 + -0.015 + 0.271 = 0.00020169908904005274\n",
      "ASD:-0.81→TD:0.00\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images/SHAP_discussion_10.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1430\u001b[0m\n\u001b[1;32m   1426\u001b[0m     fig \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mforce_plot(expected_value, df_shap_values_additionaFeat\u001b[39m.\u001b[39mvalues, Xtest_additionaFeat\u001b[39m.\u001b[39mround(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mT, figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m3\u001b[39m) ,matplotlib\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,show\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1427\u001b[0m     \u001b[39m# fig = shap.force_plot(expected_value, df_shap_values.values, Xtest.round(2).T, figsize=(8, 3) ,matplotlib=True,show=False)\u001b[39;00m\n\u001b[0;32m-> 1430\u001b[0m     plt\u001b[39m.\u001b[39;49msavefig(\u001b[39m\"\u001b[39;49m\u001b[39mimages/SHAP_discussion_\u001b[39;49m\u001b[39m{sample}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(sample\u001b[39m=\u001b[39;49mInspect_samp),dpi\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m, bbox_inches\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtight\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m   1431\u001b[0m     plt\u001b[39m.\u001b[39msavefig(SHAP_save_path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{sample}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(sample\u001b[39m=\u001b[39mInspect_samp),dpi\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m, bbox_inches\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1433\u001b[0m \u001b[39mif\u001b[39;00m UsePaperName_bool\u001b[39m==\u001b[39m\u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/matplotlib/pyplot.py:1023\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[39m.\u001b[39msavefig)\n\u001b[1;32m   1021\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msavefig\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1022\u001b[0m     fig \u001b[39m=\u001b[39m gcf()\n\u001b[0;32m-> 1023\u001b[0m     res \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39;49msavefig(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1024\u001b[0m     fig\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mdraw_idle()  \u001b[39m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/matplotlib/figure.py:3343\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3339\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes:\n\u001b[1;32m   3340\u001b[0m         stack\u001b[39m.\u001b[39menter_context(\n\u001b[1;32m   3341\u001b[0m             ax\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39m_cm_set(facecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m, edgecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m-> 3343\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[1;32m   2367\u001b[0m             filename,\n\u001b[1;32m   2368\u001b[0m             facecolor\u001b[39m=\u001b[39;49mfacecolor,\n\u001b[1;32m   2369\u001b[0m             edgecolor\u001b[39m=\u001b[39;49medgecolor,\n\u001b[1;32m   2370\u001b[0m             orientation\u001b[39m=\u001b[39;49morientation,\n\u001b[1;32m   2371\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39;49m_bbox_inches_restore,\n\u001b[1;32m   2372\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2373\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[1;32m   2233\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m skip}))\n\u001b[1;32m   2234\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n",
      "File \u001b[0;32m~/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39m, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_print_pil(filename_or_obj, \u001b[39m\"\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m\"\u001b[39;49m, pil_kwargs, metadata)\n",
      "File \u001b[0;32m~/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:458\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m FigureCanvasAgg\u001b[39m.\u001b[39mdraw(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 458\u001b[0m mpl\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimsave(\n\u001b[1;32m    459\u001b[0m     filename_or_obj, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer_rgba(), \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mfmt, origin\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mupper\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    460\u001b[0m     dpi\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdpi, metadata\u001b[39m=\u001b[39;49mmetadata, pil_kwargs\u001b[39m=\u001b[39;49mpil_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/matplotlib/image.py:1689\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mformat\u001b[39m)\n\u001b[1;32m   1688\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1689\u001b[0m image\u001b[39m.\u001b[39;49msave(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpil_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/TASLPReview/lib/python3.10/site-packages/PIL/Image.py:2429\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2427\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mr+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2428\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2429\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mw+b\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2431\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2432\u001b[0m     save_handler(\u001b[39mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images/SHAP_discussion_10.png'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jun 30 15:56:45 2021\n",
    "\n",
    "@author: jackchen\n",
    "\n",
    "\n",
    "    This is a branch of MainClassification that run specific experiments\n",
    "    \n",
    "    Include SHAP values in this script\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import spearmanr,pearsonr \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from addict import Dict\n",
    "# import functions\n",
    "import argparse\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn.svm\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from articulation.HYPERPARAM import phonewoprosody, Label\n",
    "from articulation.HYPERPARAM.PeopleSelect import SellectP_define\n",
    "import articulation.HYPERPARAM.FeatureSelect as FeatSel\n",
    "\n",
    "import articulation.articulation\n",
    "from sklearn.metrics import f1_score,recall_score,roc_auc_score,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import combinations\n",
    "import shap\n",
    "import articulation.HYPERPARAM.PaperNameMapping as PprNmeMp\n",
    "import inspect\n",
    "import seaborn as sns\n",
    "\n",
    "from articulation.HYPERPARAM.PlotFigureVars import *\n",
    "\n",
    "def Find_Experiment_actualindex(Total_FeatComb_idxs,search_string):\n",
    "    # usage:\n",
    "    # e.x. :\n",
    "    # search_string='Phonation_Trend_K_cols+Phonation_Syncrony_cols+Phonation_Trend_D_cols'\n",
    "    # Total_FeatComb_idxs=FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation['static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation'].keys()\n",
    "    # Find_Experiment_actualindex(Total_FeatComb_idxs,search_string)\n",
    "    for FC_str in Total_FeatComb_idxs:\n",
    "        if ''.join(sorted(FC_str.split(\"+\"))) == ''.join(sorted(search_string.split(\"+\"))):\n",
    "            print(FC_str)\n",
    "\n",
    "\n",
    "\n",
    "def Assert_labelfeature(feat_name,lab_name):\n",
    "    # =============================================================================\n",
    "    #     To check if the label match with feature\n",
    "    # =============================================================================\n",
    "    for i,n in enumerate(feat_name):\n",
    "        assert feat_name[i] == lab_name[i]\n",
    "\n",
    "def FilterFile_withinManualName(files,Manual_choosen_feature):\n",
    "    files_manualChoosen=[f  for f in files if os.path.basename(f).split(\".\")[0]  in Manual_choosen_feature]\n",
    "    return files_manualChoosen\n",
    "\n",
    "def Merge_dfs(df_1, df_2):\n",
    "    return pd.merge(df_1,df_2,left_index=True, right_index=True)\n",
    "\n",
    "def Add_label(df_formant_statistic,Label,label_choose='ADOS_cate_C'):\n",
    "    for people in df_formant_statistic.index:\n",
    "        bool_ind=Label.label_raw['name']==people\n",
    "        df_formant_statistic.loc[people,label_choose]=Label.label_raw.loc[bool_ind,label_choose].values\n",
    "    return df_formant_statistic\n",
    "def Swap2PaperName(feature_rawname,PprNmeMp,method='origin'):\n",
    "    if method=='origin':\n",
    "        PaperNameMapping=PprNmeMp.Paper_name_map\n",
    "    elif method=='inverse':\n",
    "        PaperNameMapping=PprNmeMp.Inverse_Paper_name_map\n",
    "    elif method=='idx':\n",
    "        PaperNameMapping=PprNmeMp.Feature2idx_map\n",
    "    \n",
    "    if feature_rawname in PaperNameMapping.keys():\n",
    "        featurename_paper=PaperNameMapping[feature_rawname]\n",
    "        feature_keys=featurename_paper\n",
    "    else: \n",
    "        feature_keys=feature_rawname\n",
    "    return feature_keys\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def Return_ALigned_dfXtestndfShapValues(Feature_SHAP_info_dict,logit_number=1):\n",
    "    #Inputs\n",
    "    # logit_numberit_number=1\n",
    "    # Feature_SHAP_info_dict=Proposed_changed_info_dict\n",
    "    ###################################\n",
    "    df_XTest_stacked=pd.DataFrame()\n",
    "    df_ShapValues_stacked=pd.DataFrame()\n",
    "    for people in Feature_SHAP_info_dict.keys():\n",
    "        df_XTest=Feature_SHAP_info_dict[people]['XTest']\n",
    "        df_ShapValues=Feature_SHAP_info_dict[people]['shap_values'].loc[logit_number]\n",
    "        df_ShapValues.name=df_XTest.name\n",
    "        df_XTest_stacked=pd.concat([df_XTest_stacked,df_XTest],axis=1)\n",
    "        df_ShapValues_stacked=pd.concat([df_ShapValues_stacked,df_ShapValues],axis=1)\n",
    "    return df_XTest_stacked,df_ShapValues_stacked\n",
    "\n",
    "def Calculate_XTestShape_correlation(Feature_SHAP_info_dict,logit_number=1):\n",
    "    df_XTest_stacked,df_ShapValues_stacked=Return_ALigned_dfXtestndfShapValues(Feature_SHAP_info_dict,logit_number=logit_number)\n",
    "    Correlation_XtestnShap={}\n",
    "    for features in df_XTest_stacked.index:\n",
    "        r,p=pearsonr(df_XTest_stacked.loc[features],df_ShapValues_stacked.loc[features])\n",
    "        Correlation_XtestnShap[features]=r\n",
    "    df_Correlation_XtestnShap=pd.DataFrame.from_dict(Correlation_XtestnShap,orient='index')\n",
    "    df_Correlation_XtestnShap.columns=['correlation w logit:{}'.format(logit_number)]\n",
    "    return df_Correlation_XtestnShap\n",
    "\n",
    "def Prepare_data_for_summaryPlot(SHAPval_info_dict, feature_columns=None, PprNmeMp=None):\n",
    "    keys_bag=[]\n",
    "    XTest_dict={}\n",
    "    shap_values_0_bag=[]\n",
    "    shap_values_1_bag=[]\n",
    "    for people in sorted(SHAPval_info_dict.keys()):\n",
    "        keys_bag.append(people)\n",
    "        if not feature_columns == None:\n",
    "            df_=SHAPval_info_dict[people]['XTest'][feature_columns]\n",
    "            df_shape_value=SHAPval_info_dict[people]['shap_values'][feature_columns]\n",
    "        else:\n",
    "            df_=SHAPval_info_dict[people]['XTest']\n",
    "            df_shape_value=SHAPval_info_dict[people]['shap_values']\n",
    "        # if not SumCategorical_feats == None:\n",
    "        #     for k,values in SumCategorical_feats.items():\n",
    "        #         df_[k]=df_.loc[values].sum()\n",
    "        XTest_dict[people]=df_        \n",
    "        shap_values_0_bag.append(df_shape_value.loc[0].values)\n",
    "        shap_values_1_bag.append(df_shape_value.loc[1].values)\n",
    "    shap_values_0_array=np.vstack(shap_values_0_bag)\n",
    "    shap_values_1_array=np.vstack(shap_values_1_bag)\n",
    "    shap_values=[shap_values_0_array,shap_values_1_array]\n",
    "    # df_XTest=pd.DataFrame.from_dict(XTest_dict,orient='index')\n",
    "    df_XTest=pd.DataFrame.from_dict(XTest_dict).T\n",
    "    if PprNmeMp!=None:\n",
    "        df_XTest.columns=[Swap2PaperName(k,PprNmeMp) for k in df_XTest.columns]\n",
    "    return shap_values, df_XTest, keys_bag\n",
    "\n",
    "def get_args():\n",
    "    # we add compulsary arguments as named arguments for readability\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Feature_mode', default='Customized_feature',\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--preprocess', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--start_point', default=-1,\n",
    "                        help='In case that the program stop at certain point, we can resume the progress by setting this variable')\n",
    "    parser.add_argument('--experiment', default='gop_exp_ADOShappyDAAIKidallDeceiptformosaCSRC',\n",
    "                        help='If the mode is set to Session_phone_phf, you may need to determine the experiment used to generate the gop feature')\n",
    "    parser.add_argument('--pseudo', default=False,\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--suffix', default=\"\",\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--FS_method_str', default=None,\n",
    "                        help='Feature selection')\n",
    "    parser.add_argument('--Print_Analysis_grp_Manual_select', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--Plot', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--Reverse_exp', default=False, dest=\"Reverse_exp\",\n",
    "                        help='')\n",
    "    parser.add_argument('--selectModelScoring', default='recall_macro',\n",
    "                        help='[recall_macro,accuracy]')\n",
    "    parser.add_argument('--Mergefeatures', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--logit_number', default=0,\n",
    "                        help='現在都改用decision function了，所以指會有一個loigit')\n",
    "    parser.add_argument('--decision_boundary', default=0,\n",
    "                        help='現在都改用decision function了，decision_boundary = 0')\n",
    "    parser.add_argument('--knn_weights', default='uniform',\n",
    "                            help='path of the base directory')\n",
    "    parser.add_argument('--knn_neighbors', default=2,  type=int,\n",
    "                            help='path of the base directory')\n",
    "    parser.add_argument('--Reorder_type', default='DKIndividual',\n",
    "                            help='[DKIndividual, DKcriteria]')\n",
    "    parser.add_argument('--Normalize_way', default='func15',\n",
    "                            help='func1 func2 func3 func4 func7 proposed')\n",
    "    parser.add_argument('--FeatureComb_mode', default='Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation',\n",
    "                            help='[Add_UttLvl_feature, feat_comb3, feat_comb5, feat_comb6,feat_comb7, baselineFeats,Comb_dynPhonation,Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation]')\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "args = get_args()\n",
    "start_point=args.start_point\n",
    "experiment=args.experiment\n",
    "knn_weights=args.knn_weights\n",
    "knn_neighbors=args.knn_neighbors\n",
    "Reorder_type=args.Reorder_type\n",
    "logit_number=args.logit_number\n",
    "Reverse_exp=args.Reverse_exp\n",
    "decision_boundary=args.decision_boundary\n",
    "\n",
    "\n",
    "Session_level_all=Dict()\n",
    "# Discriminative analysis Main\n",
    "columns=[\n",
    "    'Divergence[within_covariance_norm(A:,i:,u:)]',\n",
    "    'Divergence[within_variance_norm(A:,i:,u:)]',    \n",
    "    'Divergence[between_covariance_norm(A:,i:,u:)]',    \n",
    "    'Divergence[between_variance_norm(A:,i:,u:)]',    \n",
    "    'Divergence[total_covariance_norm(A:,i:,u:)]', \n",
    "    'Divergence[total_variance_norm(A:,i:,u:)]',\n",
    "    'Divergence[sam_wilks_lin_norm(A:,i:,u:)]',    \n",
    "    'Divergence[pillai_lin_norm(A:,i:,u:)]',\n",
    "    'Divergence[roys_root_lin_norm(A:,i:,u:)]',\n",
    "    'Divergence[hotelling_lin_norm(A:,i:,u:)]',\n",
    "    'Divergence[Between_Within_Det_ratio_norm(A:,i:,u:)]',\n",
    "    'Divergence[Between_Within_Tr_ratio_norm(A:,i:,u:)]',\n",
    "    'Divergence[within_covariance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[within_variance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[between_covariance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[between_variance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[total_covariance_norm(A:,i:,u:)]_var_p1', \n",
    "    'Divergence[total_variance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[sam_wilks_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[pillai_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[roys_root_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[hotelling_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[Between_Within_Det_ratio_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[Between_Within_Tr_ratio_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[within_covariance_norm(A:,i:,u:)]_var_p2',    \n",
    "    'Divergence[within_variance_norm(A:,i:,u:)]_var_p2',    \n",
    "    'Divergence[between_covariance_norm(A:,i:,u:)]_var_p2',    \n",
    "    'Divergence[between_variance_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[total_covariance_norm(A:,i:,u:)]_var_p2', \n",
    "    'Divergence[total_variance_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[sam_wilks_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[pillai_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[roys_root_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[hotelling_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[Between_Within_Det_ratio_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[Between_Within_Tr_ratio_norm(A:,i:,u:)]_var_p2',\n",
    "    ]\n",
    "\n",
    "# columns=[\n",
    "#     'VSA2',\n",
    "#     'FCR2',\n",
    "#     'within_covariance_norm(A:,i:,u:)',\n",
    "#     'between_covariance_norm(A:,i:,u:)',\n",
    "#     'within_variance_norm(A:,i:,u:)',\n",
    "#     'between_variance_norm(A:,i:,u:)',\n",
    "#     'total_covariance_norm(A:,i:,u:)',\n",
    "#     'total_variance_norm(A:,i:,u:)',\n",
    "#     'sam_wilks_lin_norm(A:,i:,u:)',\n",
    "#     'pillai_lin_norm(A:,i:,u:)',\n",
    "#     'Between_Within_Det_ratio_norm(A:,i:,u:)',\n",
    "#     'Between_Within_Tr_ratio_norm(A:,i:,u:)',\n",
    "# ]\n",
    "# columns=[\n",
    "# 'Norm(WC)_sam_wilks_DKRaito', \n",
    "# 'Norm(WC)_pillai_DKRaito',\n",
    "# 'Norm(WC)_hotelling_DKRaito', 'Norm(WC)_roys_root_DKRaito',\n",
    "# 'Norm(WC)_Det_DKRaito', 'Norm(WC)_Tr_DKRaito',\n",
    "# 'Norm(BC)_sam_wilks_DKRaito', 'Norm(BC)_pillai_DKRaito',\n",
    "# 'Norm(BC)_hotelling_DKRaito', 'Norm(BC)_roys_root_DKRaito',\n",
    "# 'Norm(BC)_Det_DKRaito', 'Norm(BC)_Tr_DKRaito',\n",
    "# 'Norm(TotalVar)_sam_wilks_DKRaito', 'Norm(TotalVar)_pillai_DKRaito',\n",
    "# 'Norm(TotalVar)_hotelling_DKRaito', 'Norm(TotalVar)_roys_root_DKRaito',\n",
    "# 'Norm(TotalVar)_Det_DKRaito', 'Norm(TotalVar)_Tr_DKRaito',\n",
    "# ]\n",
    "# Discriminative analysis: Side exp\n",
    "# columns=[\n",
    "# 'VSA1',\n",
    "# 'FCR',\n",
    "# 'within_covariance(A:,i:,u:)',\n",
    "# 'between_covariance(A:,i:,u:)',\n",
    "# 'within_variance(A:,i:,u:)',\n",
    "# 'between_variance(A:,i:,u:)',\n",
    "# 'sam_wilks_lin(A:,i:,u:)',\n",
    "# 'pillai_lin(A:,i:,u:)',\n",
    "# ]\n",
    "\n",
    "\n",
    "featuresOfInterest_manual=[ [col] for col in columns]\n",
    "# featuresOfInterest_manual=[ [col] + ['u_num+i_num+a_num'] for col in columns]\n",
    "\n",
    "\n",
    "# label_choose=['ADOS_C','Multi1','Multi2','Multi3','Multi4']\n",
    "label_choose=['ADOS_C']\n",
    "# label_choose=['ADOS_cate','ASDTD']\n",
    "\n",
    "\n",
    "def MERGEFEATURES():\n",
    "    # =============================================================================\n",
    "    '''\n",
    "\n",
    "        Feature merging function\n",
    "        \n",
    "        Ths slice of code provide user to manually make functions to combine df_XXX_infos\n",
    "\n",
    "    '''\n",
    "    # =============================================================================\n",
    "    # dataset_role='ASD_DOCKID'\n",
    "    for dataset_role in ['ASD_DOCKID','TD_DOCKID']:\n",
    "        Merg_filepath={}\n",
    "        Merg_filepath['static_feautre_LOC']='Features/artuculation_AUI/Vowels/Formants/{Normalize_way}/Formant_AUI_tVSAFCRFvals_KID_From{dataset_role}.pkl'.format(dataset_role=dataset_role,Normalize_way=args.Normalize_way)\n",
    "        # Merg_filepath['static_feautre_phonation']='Features/artuculation_AUI/Vowels/Phonation/Phonation_meanvars_KID_From{dataset_role}.pkl'.format(dataset_role=dataset_role)\n",
    "        Merg_filepath['dynamic_feature_LOC']='Features/artuculation_AUI/Interaction/Formants/{Normalize_way}/Syncrony_measure_of_variance_DKIndividual_{dataset_role}.pkl'.format(dataset_role=dataset_role,Normalize_way=args.Normalize_way)\n",
    "        Merg_filepath['dynamic_feature_phonation']='Features/artuculation_AUI/Interaction/Phonation/Syncrony_measure_of_variance_phonation_{dataset_role}.pkl'.format(dataset_role=dataset_role)\n",
    "        \n",
    "        merge_out_path='Features/ClassificationMerged_dfs/{Normalize_way}/{dataset_role}/'.format(\n",
    "            knn_weights=knn_weights,\n",
    "            knn_neighbors=knn_neighbors,\n",
    "            Reorder_type=Reorder_type,\n",
    "            dataset_role=dataset_role,\n",
    "            Normalize_way=args.Normalize_way\n",
    "            )\n",
    "        if not os.path.exists(merge_out_path):\n",
    "            os.makedirs(merge_out_path)\n",
    "        \n",
    "        df_infos_dict=Dict()\n",
    "        for keys, paths in Merg_filepath.items():\n",
    "            df_infos_dict[keys]=pickle.load(open(paths,\"rb\")).sort_index()\n",
    "        \n",
    "        Merged_df_dict=Dict()\n",
    "        comb1 = list(combinations(list(Merg_filepath.keys()), 1))\n",
    "        comb2 = list(combinations(list(Merg_filepath.keys()), 2))\n",
    "        for c in comb1:\n",
    "            e1=c[0]\n",
    "            Merged_df_dict[e1]=df_infos_dict[e1]\n",
    "            OutPklpath=merge_out_path+ e1 + \".pkl\"\n",
    "            pickle.dump(Merged_df_dict[e1],open(OutPklpath,\"wb\"))\n",
    "            \n",
    "            \n",
    "        for c in comb2:\n",
    "            e1, e2=c\n",
    "            Merged_df_dict['+'.join(c)]=Merge_dfs(df_infos_dict[e1],df_infos_dict[e2])\n",
    "            \n",
    "            OutPklpath=merge_out_path+'+'.join(c)+\".pkl\"\n",
    "            pickle.dump(Merged_df_dict['+'.join(c)],open(OutPklpath,\"wb\"))\n",
    "        # Condition for : Columns_comb3 = All possible LOC feature combination + phonation_proximity_col\n",
    "        c = ('static_feautre_LOC', 'dynamic_feature_LOC', 'dynamic_feature_phonation')\n",
    "        e1, e2, e3=c\n",
    "        \n",
    "        Merged_df_dict['+'.join(c)]=Merge_dfs(df_infos_dict[e1],df_infos_dict[e2])\n",
    "        Merged_df_dict['+'.join(c)]=Merge_dfs(Merged_df_dict['+'.join(c)],df_infos_dict[e3])\n",
    "        # Merged_df_dict['+'.join(c)]=Merge_dfs(Merged_df_dict['+'.join(c)],Utt_featuresCombinded_dict[role])\n",
    "        OutPklpath=merge_out_path+'+'.join(c)+\".pkl\"\n",
    "        pickle.dump(Merged_df_dict['+'.join(c)],open(OutPklpath,\"wb\"))\n",
    "if args.Mergefeatures:\n",
    "    MERGEFEATURES()\n",
    "\n",
    "df_formant_statistics_CtxPhone_collect_dict=Dict()\n",
    "# =============================================================================\n",
    "\n",
    "class ADOSdataset():\n",
    "    def __init__(self,knn_weights,knn_neighbors,Reorder_type,FeatureComb_mode):\n",
    "        self.featurepath='Features'            \n",
    "        self.N=2\n",
    "        self.LabelType=Dict()\n",
    "        self.LabelType['ADOS_C']='regression'\n",
    "        self.LabelType['ADOS_cate_C']='classification'\n",
    "        self.LabelType['ASDTD']='classification'\n",
    "        self.Fractionfeatures_str='Features/artuculation_AUI/Vowels/Fraction/*.pkl'    \n",
    "        self.FeatureComb_mode=FeatureComb_mode\n",
    "        if self.FeatureComb_mode == 'Add_UttLvl_feature':\n",
    "            self.File_root_path='Features/ClassificationMerged_dfs/{Normalize_way}/ADDed_UttFeat/{knn_weights}_{knn_neighbors}_{Reorder_type}/'.format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type,Normalize_way=args.Normalize_way)\n",
    "            self.Merge_feature_path=self.File_root_path+'{dataset_role}/*.pkl'.format(dataset_role='ASD_DOCKID')\n",
    "        else:\n",
    "            self.File_root_path='Features/ClassificationMerged_dfs/{Normalize_way}/'.format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type,Normalize_way=args.Normalize_way)\n",
    "            self.Merge_feature_path=self.File_root_path+'{dataset_role}/*.pkl'.format(dataset_role='ASD_DOCKID')\n",
    "        \n",
    "        self.Top_ModuleColumn_mapping_dict={}\n",
    "        self.Top_ModuleColumn_mapping_dict['Add_UttLvl_feature']=FeatSel.Columns_comb2.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb']=FeatSel.Columns_comb.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb3']=FeatSel.Columns_comb3.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb5']=FeatSel.Columns_comb5.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb6']=FeatSel.Columns_comb6.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb7']=FeatSel.Columns_comb7.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['Comb_dynPhonation']=FeatSel.Comb_dynPhonation.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']=FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation.copy()\n",
    "        \n",
    "        self.Top_ModuleColumn_mapping_dict['baselineFeats']=FeatSel.Baseline_comb.copy()\n",
    "        \n",
    "        self.FeatureCombs_manual=Dict()\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatLOC_kid']=['df_formant_statistic_TD_normal_kid', 'df_formant_statistic_agesexmatch_ASDSevereGrp_kid']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatLOC_kid']=['df_formant_statistic_TD_normal_kid', 'df_formant_statistic_agesexmatch_ASDMildGrp_kid']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatLOC_doc']=['df_formant_statistic_TD_normal_doc', 'df_formant_statistic_agesexmatch_ASDSevereGrp_doc']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatLOC_doc']=['df_formant_statistic_TD_normal_doc', 'df_formant_statistic_agesexmatch_ASDMildGrp_doc']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatLOC_DKRatio']=['df_formant_statistic_TD_normalGrp_DKRatio', 'df_formant_statistic_agesexmatch_ASDSevereGrp_DKRatio']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatLOC_DKRatio']=['df_formant_statistic_TD_normalGrp_DKRatio', 'df_formant_statistic_agesexmatch_ASDMildGrp_DKRatio']\n",
    "        \n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatCoor']=['df_syncrony_statistic_TD_normalGrp', 'df_syncrony_statistic_agesexmatch_ASDSevereGrp']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatCoor']=['df_syncrony_statistic_TD_normalGrp', 'df_syncrony_statistic_agesexmatch_ASDMildGrp']\n",
    "        # self.FeatureCombs_manual['Notautism vs ASD']=['df_formant_statistic_77_Notautism', 'df_formant_statistic_77_ASD']\n",
    "        # self.FeatureCombs_manual['ASD vs Autism']=['df_formant_statistic_77_ASD', 'df_formant_statistic_77_Autism']\n",
    "        # self.FeatureCombs_manual['Notautism vs Autism']=['df_formant_statistic_77_Notautism', 'df_formant_statistic_77_Autism']\n",
    "    \n",
    "        # self._FeatureBuild_single()\n",
    "        self._FeatureBuild_Module()\n",
    "    def Get_FormantAUI_feat(self,label_choose,pickle_path,featuresOfInterest=['MSB_f1','MSB_f2','MSB_mix'],filterbyNum=True,**kwargs):\n",
    "        arti=articulation.articulation.Articulation()\n",
    "        #如果path有放的話字串的話，就使用path的字串，不然就使用「feat_」等於的東西，在function裡面會以kwargs的形式出現\n",
    "        if not kwargs and len(pickle_path)>0:\n",
    "            df_tmp=pickle.load(open(pickle_path,\"rb\")).sort_index()\n",
    "        elif len(kwargs)>0: # usage Get_FormantAUI_feat(...,key1=values1):\n",
    "            for k, v in kwargs.items(): #there will be only one element\n",
    "                df_tmp=kwargs[k].sort_index()\n",
    "\n",
    "        if filterbyNum:\n",
    "            df_tmp=arti.BasicFilter_byNum(df_tmp,N=self.N)\n",
    "\n",
    "        if label_choose not in df_tmp.columns:\n",
    "            for people in df_tmp.index:\n",
    "                lab=Label.label_raw[label_choose][Label.label_raw['name']==people]\n",
    "                df_tmp.loc[people,'ADOS']=lab.values\n",
    "            df_y=df_tmp['ADOS'] #Still keep the form of dataframe\n",
    "        else:\n",
    "            df_y=df_tmp[label_choose] #Still keep the form of dataframe\n",
    "        \n",
    "        \n",
    "        feature_array=df_tmp[featuresOfInterest]\n",
    "        \n",
    "            \n",
    "        LabType=self.LabelType[label_choose]\n",
    "        return feature_array, df_y, LabType\n",
    "    def _FeatureBuild_single(self):\n",
    "        Features=Dict()\n",
    "        Features_comb=Dict()\n",
    "        files = glob.glob(self.Fractionfeatures_str)\n",
    "        for file in files:\n",
    "            feat_name=os.path.basename(file).replace(\".pkl\",\"\")\n",
    "            df_tmp=pickle.load(open(file,\"rb\")).sort_index()\n",
    "            Features[feat_name]=df_tmp\n",
    "        for keys in self.FeatureCombs_manual.keys():\n",
    "            combF=[Features[k] for k in self.FeatureCombs_manual[keys]]\n",
    "            Features_comb[keys]=pd.concat(combF)\n",
    "        \n",
    "        self.Features_comb_single=Features_comb\n",
    "    def _FeatureBuild_Module(self):\n",
    "        Labels_add=['ASDTD']\n",
    "        ModuledFeatureCombination=self.Top_ModuleColumn_mapping_dict[self.FeatureComb_mode]\n",
    "        \n",
    "        sellect_people_define=SellectP_define()\n",
    "        #Loading features from ASD        \n",
    "        Features_comb=Dict()\n",
    "        IterateFilesFullPaths = glob.glob(self.Merge_feature_path)\n",
    "        \n",
    "\n",
    "        if self.FeatureComb_mode in ['feat_comb3','feat_comb5','feat_comb6','feat_comb7','Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']:\n",
    "            DfCombFilenames=['static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation.pkl']\n",
    "        elif self.FeatureComb_mode == 'Comb_dynPhonation':\n",
    "            DfCombFilenames=['dynamic_feature_phonation.pkl']\n",
    "        elif self.FeatureComb_mode == 'baselineFeats':\n",
    "             DfCombFilenames=['{}.pkl'.format(Dataname) for Dataname in ModuledFeatureCombination.keys()]\n",
    "        else:\n",
    "            DfCombFilenames=[os.path.basename(f) for f in IterateFilesFullPaths]\n",
    "        File_ASD_paths=[self.File_root_path+\"ASD_DOCKID/\"+f for f in DfCombFilenames]\n",
    "        File_TD_paths=[self.File_root_path+\"TD_DOCKID/\"+f for f in DfCombFilenames]\n",
    "        \n",
    "        \n",
    "        df_Top_Check_length=pd.DataFrame()\n",
    "        for file_ASD, file_TD in zip(File_ASD_paths,File_TD_paths):\n",
    "            if not os.path.exists(file_ASD) or not os.path.exists(file_TD):\n",
    "                raise FileExistsError()\n",
    "            \n",
    "            assert os.path.basename(file_ASD) == os.path.basename(file_TD)\n",
    "            filename=os.path.basename(file_ASD)\n",
    "            k_FeatTypeLayer1=filename.replace(\".pkl\",\"\")\n",
    "            df_feature_ASD=pickle.load(open(file_ASD,\"rb\")).sort_index()\n",
    "            df_feature_TD=pickle.load(open(file_TD,\"rb\")).sort_index()\n",
    "            df_feature_ASD['ASDTD']=sellect_people_define.ASDTD_label['ASD']\n",
    "            df_feature_TD['ASDTD']=sellect_people_define.ASDTD_label['TD']\n",
    "            \n",
    "            # ADD label\n",
    "            df_feature_ASD=Add_label(df_feature_ASD,Label,label_choose='ADOS_cate_CSS')\n",
    "            df_feature_ASD=Add_label(df_feature_ASD,Label,label_choose='age_year')\n",
    "            df_feature_ASD=Add_label(df_feature_ASD,Label,label_choose='sex')\n",
    "            # create different ASD cohort\n",
    "            filter_Minimal_TCSS=df_feature_ASD['ADOS_cate_CSS']==0\n",
    "            filter_low_TCSS=df_feature_ASD['ADOS_cate_CSS']==1\n",
    "            filter_moderate_TCSS=df_feature_ASD['ADOS_cate_CSS']==2\n",
    "            filter_high_TCSS=df_feature_ASD['ADOS_cate_CSS']==3\n",
    "            \n",
    "            df_feauture_ASDgrp_dict={}\n",
    "            df_feauture_ASDgrp_dict['df_feature_ASD']=df_feature_ASD\n",
    "            \n",
    "            # df_feauture_ASDgrp_dict['df_feature_Minimal_CSS']=df_feature_ASD[filter_Minimal_TCSS]\n",
    "            # df_feauture_ASDgrp_dict['df_feature_low_CSS']=df_feature_ASD[filter_low_TCSS]\n",
    "            # df_feauture_ASDgrp_dict['df_feature_moderate_CSS']=df_feature_ASD[filter_moderate_TCSS]\n",
    "            # df_feauture_ASDgrp_dict['df_feature_high_CSS']=df_feature_ASD[filter_high_TCSS]\n",
    "            df_feauture_ASDgrp_dict['df_feature_lowMinimal_CSS']=df_feature_ASD[filter_low_TCSS | filter_Minimal_TCSS]\n",
    "            df_feauture_ASDgrp_dict['df_feature_moderate_CSS']=df_feature_ASD[filter_moderate_TCSS]\n",
    "            df_feauture_ASDgrp_dict['df_feature_high_CSS']=df_feature_ASD[filter_high_TCSS]\n",
    "\n",
    "            #Check the length of each paired comparison, should be stored on the top of for loop\n",
    "            Tmp_Numcmp_dict={}\n",
    "            for key in df_feauture_ASDgrp_dict.keys():\n",
    "                Numcmp_str='ASD({0}) vs TD({1})'.format(len(df_feauture_ASDgrp_dict[key]),len(df_feature_TD))\n",
    "                Tmp_Numcmp_dict[key]=Numcmp_str\n",
    "                \n",
    "            \n",
    "            df_Tmp_Numcmp_list=pd.DataFrame.from_dict(Tmp_Numcmp_dict,orient='index')\n",
    "            df_Tmp_Numcmp_list.columns=[k_FeatTypeLayer1]\n",
    "\n",
    "            if len(df_Top_Check_length)==0:\n",
    "                df_Top_Check_length=df_Tmp_Numcmp_list\n",
    "            else:\n",
    "                df_Top_Check_length=Merge_dfs(df_Top_Check_length,df_Tmp_Numcmp_list)\n",
    "            # 手動執行到這邊，從for 上面\n",
    "            for k_FeatTypeLayer2 in ModuledFeatureCombination[k_FeatTypeLayer1].keys():\n",
    "                colums_sel=ModuledFeatureCombination[k_FeatTypeLayer1][k_FeatTypeLayer2]\n",
    "                \n",
    "\n",
    "                # 1. Set ASD vs TD experiment\n",
    "                for k_ASDgrp in df_feauture_ASDgrp_dict.keys():\n",
    "                    df_ASD_subgrp=df_feauture_ASDgrp_dict[k_ASDgrp].copy()[colums_sel+Labels_add]\n",
    "                    df_TD_subgrp=df_feature_TD.copy()[colums_sel+Labels_add]\n",
    "                    \n",
    "                    experiment_str=\"{TD_name} vs {ASD_name} >> {feature_type}\".format(TD_name='TD',ASD_name=k_ASDgrp,feature_type=k_FeatTypeLayer2)\n",
    "                    Features_comb[experiment_str]=pd.concat([df_ASD_subgrp,df_TD_subgrp],axis=0)\n",
    "                # 2. Set ASDsevere vs ASDmild experiment\n",
    "                # experiment_str=\"{ASDsevere_name} vs {ASDmild_name} >> {feature_type}\".format(ASDsevere_name='df_feature_moderatehigh_CSS',ASDmild_name='df_feature_lowMinimal_CSS',feature_type=k_FeatTypeLayer2)\n",
    "                # df_ASDsevere_subgrp=df_feauture_ASDgrp_dict['df_feature_moderatehigh_CSS'].copy()\n",
    "                # df_ASDmild_subgrp=df_feauture_ASDgrp_dict['df_feature_lowMinimal_CSS'].copy()\n",
    "                # df_ASDsevere_subgrp['ASDsevereMild']=sellect_people_define.ASDsevereMild_label['ASDsevere']\n",
    "                # df_ASDmild_subgrp['ASDsevereMild']=sellect_people_define.ASDsevereMild_label['ASDmild']\n",
    "                # Features_comb[experiment_str]=pd.concat([df_ASDsevere_subgrp,df_ASDmild_subgrp],axis=0)\n",
    "        self.Features_comb_multi=Features_comb\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Feature merging function\n",
    "    \n",
    "    Ths slice of code provide user to manually make functions to combine df_XXX_infos\n",
    "\n",
    "'''\n",
    "# =============================================================================\n",
    "\n",
    "ados_ds=ADOSdataset(knn_weights,knn_neighbors,Reorder_type,FeatureComb_mode=args.FeatureComb_mode)\n",
    "ErrorFeat_bookeep=Dict()\n",
    "\n",
    "\n",
    "FeatureLabelMatch_manual=[\n",
    "\n",
    "    ['TD vs df_feature_lowMinimal_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Syncrony_cols+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "    ['TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "    ['TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols', 'ASDTD'],\n",
    "    \n",
    "\n",
    "    ['TD vs df_feature_moderate_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Trend_K_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "    ['TD vs df_feature_moderate_CSS >> Phonation_Proximity_cols', 'ASDTD'],\n",
    "    # ['TD vs df_feature_moderate_CSS >> Phonation_Trend_D_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "\n",
    "    ['TD vs df_feature_high_CSS >> DEP_columns+Phonation_Proximity_cols', 'ASDTD'],\n",
    "    # ['TD vs df_feature_high_CSS >> Phonation_Proximity_cols+Phonation_Syncrony_cols', 'ASDTD'],\n",
    "    ['TD vs df_feature_high_CSS >> Phonation_Proximity_cols', 'ASDTD'],\n",
    " ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FeatSel 掌管該出現的columns\n",
    "# ados_ds.Features_comb_multi 掌管load進來的data\n",
    "\n",
    "\n",
    "Top_ModuleColumn_mapping_dict={}\n",
    "Top_ModuleColumn_mapping_dict['Add_UttLvl_feature']={ e2_str:FeatSel.Columns_comb2[e_str][e2_str] for e_str in FeatSel.Columns_comb2.keys() for e2_str in FeatSel.Columns_comb2[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb3']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb3[e_str][e2_str] for e_str in FeatSel.Columns_comb3.keys() for e2_str in FeatSel.Columns_comb3[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb5']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb5[e_str][e2_str] for e_str in FeatSel.Columns_comb5.keys() for e2_str in FeatSel.Columns_comb5[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb6']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb6[e_str][e2_str] for e_str in FeatSel.Columns_comb6.keys() for e2_str in FeatSel.Columns_comb6[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb7']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb7[e_str][e2_str] for e_str in FeatSel.Columns_comb7.keys() for e2_str in FeatSel.Columns_comb7[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['Comb_dynPhonation']=ModuleColumn_mapping={ e2_str:FeatSel.Comb_dynPhonation[e_str][e2_str] for e_str in FeatSel.Comb_dynPhonation.keys() for e2_str in FeatSel.Comb_dynPhonation[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']=ModuleColumn_mapping={ e2_str:FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation[e_str][e2_str] for e_str in FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation.keys() for e2_str in FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb[e_str][e2_str] for e_str in FeatSel.Columns_comb.keys() for e2_str in FeatSel.Columns_comb[e_str].keys()}\n",
    "\n",
    "\n",
    "ModuleColumn_mapping=Top_ModuleColumn_mapping_dict[args.FeatureComb_mode]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Here starts to load features to Session_level_all dict\n",
    "    \n",
    "'''\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "for exp_str, lab_ in FeatureLabelMatch_manual:\n",
    "    comparison_pair=exp_str.split(\" >> \")[0]\n",
    "    ModuleColumn_str=exp_str.split(\" >> \")[-1]\n",
    "    featuresOfInterest=[ModuleColumn_mapping[ModuleColumn_str]]\n",
    "    # feat_=key\n",
    "    for feat_col in featuresOfInterest:\n",
    "        feat_col_ = list(feat_col) # ex: ['MSB_f1']\n",
    "        if len(feat_col) > 144: # 144 is the limit of the filename\n",
    "            key=feat_col_\n",
    "        else:\n",
    "            key=[feat_col_]\n",
    "        # X,y, featType=ados_ds.Get_FormantAUI_feat(\\\n",
    "        #     label_choose=lab_,pickle_path='',featuresOfInterest=feat_col_,filterbyNum=False,\\\n",
    "        #     feat_=ados_ds.Features_comb_single[feat_])\n",
    "        \n",
    "        X,y, featType=ados_ds.Get_FormantAUI_feat(\\\n",
    "            label_choose=lab_,pickle_path='',featuresOfInterest=feat_col,filterbyNum=False,\\\n",
    "            feat_=ados_ds.Features_comb_multi[exp_str])\n",
    "        \n",
    "        if X.isnull().values.any() or y.isnull().values.any():\n",
    "            print(\"Feat: \",key,'Contains nan')\n",
    "            ErrorFeat_bookeep['{0} {1} {2}'.format(exp_str,lab_,key)].X=X\n",
    "            ErrorFeat_bookeep['{0} {1} {2}'.format(exp_str,lab_,key)].y=y\n",
    "            continue\n",
    "        \n",
    "        Item_name=\"{feat}::{lab}\".format(feat=' >> '.join([comparison_pair,ModuleColumn_str]),lab=lab_)\n",
    "        Session_level_all[Item_name].X, \\\n",
    "            Session_level_all[Item_name].y, \\\n",
    "                Session_level_all[Item_name].feattype = X,y, featType\n",
    "\n",
    "# =============================================================================\n",
    "# Model parameters\n",
    "# =============================================================================\n",
    "# C_variable=np.array([0.0001, 0.01, 0.1,0.5,1.0,10.0, 50.0, 100.0, 1000.0])\n",
    "# C_variable=np.array([0.01, 0.1,0.5,1.0,10.0, 50.0, 100.0])\n",
    "# C_variable=np.array(np.arange(0.1,1.5,0.1))\n",
    "C_variable=np.array([0.001,0.01,0.1,1,5,10.0,25,50,75,100])\n",
    "# C_variable=np.array([0.001,0.01,10.0,50,100] + list(np.arange(0.1,1.5,0.2))  )\n",
    "# C_variable=np.array([0.01, 0.1,0.5,1.0, 5.0])\n",
    "n_estimator=[ 32, 50, 64, 100 ,128, 256]\n",
    "\n",
    "'''\n",
    "\n",
    "    Classifier\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "# This is the closest \n",
    "Classifier={}\n",
    "Classifier['SVC']={'model':sklearn.svm.SVC(),\\\n",
    "                  'parameters':{'model__random_state':[1],\\\n",
    "                      'model__C':C_variable,\\\n",
    "                    'model__kernel': ['rbf'],\\\n",
    "                      # 'model__gamma':['auto'],\\\n",
    "                    'model__probability':[True],\\\n",
    "                                }}\n",
    "\n",
    "   \n",
    "\n",
    "loo=LeaveOneOut()\n",
    "# CV_settings=loo\n",
    "CV_settings=10\n",
    "\n",
    "# =============================================================================\n",
    "# Outputs\n",
    "Best_predict_optimize={}\n",
    "\n",
    "df_best_result_r2=pd.DataFrame([])\n",
    "df_best_result_pear=pd.DataFrame([])\n",
    "df_best_result_spear=pd.DataFrame([])\n",
    "df_best_cross_score=pd.DataFrame([])\n",
    "df_best_result_UAR=pd.DataFrame([])\n",
    "df_best_result_AUC=pd.DataFrame([])\n",
    "df_best_result_f1=pd.DataFrame([])\n",
    "df_best_result_allThreeClassifiers=pd.DataFrame([])\n",
    "# =============================================================================\n",
    "Result_path=\"RESULTS/\"\n",
    "if not os.path.exists(Result_path):\n",
    "    os.makedirs(Result_path)\n",
    "final_result_file=\"_ADOS_{}.xlsx\".format(args.suffix)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "count=0\n",
    "OutFeature_dict=Dict()\n",
    "Best_param_dict=Dict()\n",
    "sellect_people_define=SellectP_define()\n",
    "\n",
    "# ''' 要手動執行一次從Incorrect2Correct_indexes和Correct2Incorrect_indexes決定哪些indexes 需要算shap value 再在這邊指定哪些fold需要停下來算SHAP value '''\n",
    "SHAP_inspect_idxs_manual=None # None means calculate SHAP value of all people\n",
    "# SHAP_inspect_idxs_manual=[4, 6, 15] # None means calculate SHAP value of all people\n",
    "# SHAP_inspect_idxs_manual=[] # empty list means we do not execute shap function\n",
    "# SHAP_inspect_idxs_manual=sorted(list(set([14, 21]+[]+[24, 28, 30, 31, 39, 41, 45]+[22, 23, 27, 47, 58]+[6, 13, 19, 23, 24, 25]+[28, 35, 38, 45])))\n",
    "\n",
    "for clf_keys, clf in Classifier.items(): #Iterate among different classifiers \n",
    "    writer_clf = pd.ExcelWriter(Result_path+\"/\"+clf_keys+\"_\"+args.Feature_mode+\"_\"+final_result_file, engine = 'xlsxwriter')\n",
    "    for feature_lab_str, features in Session_level_all.items():\n",
    "\n",
    "        feature_keys, label_keys= feature_lab_str.split(\"::\")\n",
    "        feature_rawname=feature_keys[feature_keys.find('-')+1:]\n",
    "        # if feature_rawname in paper_name_map.keys():\n",
    "        #     featurename_paper=paper_name_map[feature_rawname]\n",
    "        #     feature_keys=feature_keys.replace(feature_rawname,featurename_paper)\n",
    "        \n",
    "        if SHAP_inspect_idxs_manual != None:\n",
    "            SHAP_inspect_idxs=SHAP_inspect_idxs_manual\n",
    "        else:\n",
    "            SHAP_inspect_idxs=range(len(features.y))\n",
    "        \n",
    "        Labels = Session_level_all.X[feature_keys]\n",
    "        print(\"=====================Cross validation start==================\")\n",
    "        pipe = Pipeline(steps=[('scalar',StandardScaler()),(\"model\", clf['model'])])\n",
    "        p_grid=clf['parameters']\n",
    "        Gclf = GridSearchCV(estimator=pipe, param_grid=p_grid, scoring=args.selectModelScoring, cv=CV_settings, refit=True, n_jobs=-1)\n",
    "        Gclf_manual = GridSearchCV(estimator=pipe, param_grid=p_grid, scoring=args.selectModelScoring, cv=CV_settings, refit=True, n_jobs=-1)\n",
    "        \n",
    "        \n",
    "        # The cv is as the one in cross_val_predict function\n",
    "        cv = sklearn.model_selection.check_cv(CV_settings,features.y,classifier=sklearn.base.is_classifier(Gclf))\n",
    "        splits = list(cv.split(features.X, features.y, groups=None))\n",
    "        test_indices = np.concatenate([test for _, test in splits])\n",
    "\n",
    "        CVpredict_manual=np.zeros(len(features.y))\n",
    "        for i, (train_index, test_index) in enumerate(splits):\n",
    "            X_train, X_test = features.X.iloc[train_index], features.X.iloc[test_index]\n",
    "            y_train, y_test = features.y.iloc[train_index], features.y.iloc[test_index]\n",
    "            Gclf_manual.fit(X_train,y_train)\n",
    "            Gclf_manual_predict=Gclf_manual.predict(X_test)\n",
    "            ##################################################################\n",
    "            #確認predict proba會跟prediction有match到\n",
    "            #原本的SVM的predict proba (https://github.com/scikit-learn/scikit-learn/issues/13211)\n",
    "            # the normal classifiers' predictions are based on decision_function values\n",
    "            ##################################################################\n",
    "            calibrated_bestEst=CalibratedClassifierCV(\n",
    "                estimator=Gclf_manual.best_estimator_,\n",
    "                cv=\"prefit\"\n",
    "                )\n",
    "            calibrated_bestEst.fit(X_train,y_train)\n",
    "            result_bestmodel=calibrated_bestEst.predict(X_test)\n",
    "            ##################################################################\n",
    "\n",
    "            decisFunc=Gclf_manual.decision_function(X_test)\n",
    "            # predict_proba 是把SVM的output另外fit一個ligistic regression 來smooth他的output到0~1之間\n",
    "            # https://splunktool.com/why-is-the-result-of-sklearnsvmsvcpredict-inconsistent-with-sklearnsvmsvcpredictproba\n",
    "            \n",
    "            # ASSERT based on predict_proba\n",
    "            # TDPred_score_logit0=calibrated_bestEst.predict_proba(X_test)[:,0] \n",
    "            # TDPred_score_logit1=calibrated_bestEst.predict_proba(X_test)[:,1]\n",
    "            # Logit01=np.vstack([TDPred_score_logit0,TDPred_score_logit1])\n",
    "            # TDPredtion_Fromscore=np.argmax(Logit01,axis=0)\n",
    "            # assert (result_bestmodel-1 == TDPredtion_Fromscore).all()\n",
    "            \n",
    "            \n",
    "            # 測試calibrated predict有沒有等於原本的predict\n",
    "            # assert (Gclf_manual_predict == result_bestmodel).all()\n",
    "            \n",
    "            # ASSERT based on decision function\n",
    "            # decision function的unit test\n",
    "            decisFunc_pred=np.ones(len(result_bestmodel)).astype(int)\n",
    "            decisFunc_pred[decisFunc >0]=2\n",
    "            assert (Gclf_manual_predict == decisFunc_pred).all()\n",
    "            # from sklearn.calibration import calibration_curve\n",
    "            # calibration_curve(Gclf_manual.decision_function(X_test))\n",
    "            ##################################################################\n",
    "            CVpredict_manual[test_index]=Gclf_manual_predict\n",
    "            # CVpred_fromFunction=CVpredict[test_index]\n",
    "            \n",
    "            # SHAP value generating\n",
    "            # logit_number=0\n",
    "            # inspect_sample=0\n",
    "            # If the indexes we want to examine are in that fold, store the whole fold\n",
    "            # 先把整個fold記錄下來然後在analysis area再拆解\n",
    "            SHAP_exam_lst=[i for i in test_index if i in SHAP_inspect_idxs]\n",
    "            if len(SHAP_exam_lst) != 0:\n",
    "                explainer = shap.KernelExplainer(Gclf_manual.decision_function, X_train)\n",
    "                # explainer = shap.KernelExplainer(Gclf_manual.predict_proba, X_train)\n",
    "                shap_values = explainer.shap_values(X_test)\n",
    "                \n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].explainer_expected_value=explainer.expected_value\n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].shap_values=shap_values # shap_values= [logit, index, feature]\n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].XTest=X_test\n",
    "                Session_level_all[feature_lab_str]['Logit{}_predictproba'.format(logit_number)]['_'.join(test_index.astype(str))].predictproba=Gclf_manual.predict_proba(X_test)[:,logit_number]\n",
    "                Session_level_all[feature_lab_str]['Logit{}_predictproba'.format(logit_number)]['_'.join(test_index.astype(str))].decisionfunc=Gclf_manual.decision_function(X_test)\n",
    "                # Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index)].testIndex=test_index\n",
    "            # shap.force_plot(explainer.expected_value[logit_number], shap_values[logit_number][inspect_sample,:], X_test.iloc[inspect_sample,:], matplotlib=True,show=False)\n",
    "            \n",
    "            \n",
    "            # assert (result_bestmodel==result_bestmodel_fitted_again).all()\n",
    "            # assert (result_bestmodel==CVpred_fromFunction).all()\n",
    "        # XXX 原本的CVpredict有點怪，他們經過\n",
    "        # CVpredict=cross_val_predict(Gclf, features.X, features.y, cv=CV_settings)   \n",
    "        CVpredict = CVpredict_manual\n",
    "        \n",
    "        # assert (CVpredict_manual==CVpredict).all()\n",
    "        Session_level_all[feature_lab_str]['y_pred']=CVpredict_manual\n",
    "        Session_level_all[feature_lab_str]['y_true']=features.y\n",
    "        \n",
    "        Gclf.fit(features.X,features.y)\n",
    "        if clf_keys == \"EN\":\n",
    "            print('The coefficient of best estimator is: ',Gclf.best_estimator_.coef_)\n",
    "        \n",
    "        print(\"The best score with scoring parameter: 'r2' is\", Gclf.best_score_)\n",
    "        print(\"The best parameters are :\", Gclf.best_params_)\n",
    "        best_parameters=Gclf.best_params_\n",
    "        best_score=Gclf.best_score_\n",
    "        best_parameters.update({'best_score':best_score})\n",
    "        Best_param_dict[feature_lab_str]=best_parameters\n",
    "        cv_results_info=Gclf.cv_results_\n",
    "\n",
    "        num_ASD=len(np.where(features.y==sellect_people_define.ASDTD_label['ASD'])[0])\n",
    "        num_TD=len(np.where(features.y==sellect_people_define.ASDTD_label['TD'])[0])\n",
    "        \n",
    "        if features.feattype == 'regression':\n",
    "            r2=r2_score(features.y,CVpredict )\n",
    "            n,p=features.X.shape\n",
    "            r2_adj=1-(1-r2)*(n-1)/(n-p-1)\n",
    "            pearson_result, pearson_p=pearsonr(features.y,CVpredict )\n",
    "            spear_result, spearman_p=spearmanr(features.y,CVpredict )\n",
    "            print('Feature {0}, label {1} ,spear_result {2}'.format(feature_keys, label_keys,spear_result))\n",
    "        elif features.feattype == 'classification':\n",
    "            n,p=features.X.shape\n",
    "            CM=confusion_matrix(features.y, CVpredict)\n",
    "            Session_level_all[feature_lab_str]['Confusion_matrix']=pd.DataFrame(CM,\\\n",
    "                                                                    index=['y_true_{}'.format(ii) for ii in range(CM.shape[0])],\\\n",
    "                                                                    columns=['y_pred_{}'.format(ii) for ii in range(CM.shape[1])])\n",
    "            UAR=recall_score(features.y, CVpredict, average='macro')\n",
    "            AUC=roc_auc_score(features.y, CVpredict)\n",
    "            f1Score=f1_score(features.y, CVpredict, average='macro')\n",
    "            print('Feature {0}, label {1} ,UAR {2}'.format(feature_keys, label_keys,UAR))\n",
    "            \n",
    "        if args.Plot and p <2:\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 10), sharey=True)\n",
    "            kernel_label = [clf_keys]\n",
    "            model_color = ['m']\n",
    "            # axes.plot((features.X - min(features.X) )/ max(features.X), Gclf.best_estimator_.fit(features.X,features.y).predict(features.X), color=model_color[0],\n",
    "            #               label='CV Predict')\n",
    "            axes.scatter((features.X.values - min(features.X.values) )/ max(features.X.values), CVpredict, \n",
    "                         facecolor=\"none\", edgecolor=\"k\", s=150,\n",
    "                         label='{}'.format(feature_lab_str)\n",
    "                         )\n",
    "            axes.scatter((features.X.values - min(features.X.values) )/ max(features.X.values), features.y, \n",
    "                         facecolor=\"none\", edgecolor=\"r\", s=50,\n",
    "                         label='Real Y')\n",
    "            axes.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
    "                    ncol=1, fancybox=True, shadow=True)\n",
    "            \n",
    "            Plot_path='./Plot/LinearRegress/'\n",
    "            if not os.path.exists(Plot_path):\n",
    "                os.makedirs(Plot_path)\n",
    "            plot_file=Plot_path+\"/{0}_{1}.png\".format(clf_keys,feature_lab_str)\n",
    "            plt.savefig(plot_file, dpi=200) \n",
    "        \n",
    "        # =============================================================================\n",
    "        '''\n",
    "            Inspect the best result\n",
    "        '''\n",
    "        # =============================================================================\n",
    "        Best_predict_optimize[label_keys]=pd.DataFrame(np.vstack((CVpredict,features.y)).T,columns=['y_pred','y'])\n",
    "        excel_path='./Statistics/prediction_result'\n",
    "        if not os.path.exists(excel_path):\n",
    "            os.makedirs(excel_path)\n",
    "        excel_file=excel_path+\"/{0}_{1}.xlsx\"\n",
    "        writer = pd.ExcelWriter(excel_file.format(clf_keys,feature_keys.replace(\":\",\"\")), engine = 'xlsxwriter')\n",
    "        for label_name in  Best_predict_optimize.keys():\n",
    "            Best_predict_optimize[label_name].to_excel(writer,sheet_name=label_name.replace(\"/\",\"_\"))\n",
    "        writer.close()\n",
    "                                \n",
    "        # ================================================      =============================\n",
    "        if features.feattype == 'regression':\n",
    "            df_best_result_r2.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(r2_adj,3),np.round(np.nan,6))\n",
    "            df_best_result_pear.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(pearson_result,3),np.round(pearson_p,6))\n",
    "            df_best_result_spear.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(spear_result,3),np.round(spearman_p,6))\n",
    "            df_best_result_spear.loc[feature_keys,'de-zero_num']=len(features.X)\n",
    "            # df_best_cross_score.loc[feature_keys,label_keys]=Score.mean()\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1} (R2adj/pear/spear)'.format(label_keys,clf_keys)]\\\n",
    "                        ='{0}/{1}/{2}'.format(np.round(r2_adj,3),np.round(pearson_result,3),np.round(spear_result,3))\n",
    "\n",
    "        elif features.feattype == 'classification':\n",
    "            df_best_result_UAR.loc[feature_keys,label_keys]='{0}'.format(UAR)\n",
    "            df_best_result_AUC.loc[feature_keys,label_keys]='{0}'.format(AUC)\n",
    "            df_best_result_f1.loc[feature_keys,label_keys]='{0}'.format(f1Score)\n",
    "            # df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1} (UAR/AUC/f1score)'.format(label_keys,clf_keys)]\\\n",
    "            #             ='{0}/{1}/{2}'.format(np.round(UAR,3),np.round(AUC,3),np.round(f1Score,3))\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1}'.format(label_keys,clf_keys)]\\\n",
    "                        ='{0}'.format(np.round(UAR,3))\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'num_ASD']\\\n",
    "                        ='{0}'.format(num_ASD)\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'num_TD']\\\n",
    "                        ='{0}'.format(num_TD)\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'f1']\\\n",
    "                        ='{0}'.format(np.round(f1Score,3))\n",
    "        count+=1\n",
    "    if features.feattype == 'regression':\n",
    "        df_best_result_r2.to_excel(writer_clf,sheet_name=\"R2_adj\")\n",
    "        df_best_result_pear.to_excel(writer_clf,sheet_name=\"pear\")\n",
    "        df_best_result_spear.to_excel(writer_clf,sheet_name=\"spear\")\n",
    "        df_best_result_spear.to_csv(Result_path+\"/\"+clf_keys+\"_\"+args.Feature_mode+\"_spearman.csv\")\n",
    "    elif features.feattype == 'classification':\n",
    "        df_best_result_UAR.to_excel(writer_clf,sheet_name=\"UAR\")\n",
    "        df_best_result_AUC.to_excel(writer_clf,sheet_name=\"AUC\")\n",
    "        df_best_result_f1.to_excel(writer_clf,sheet_name=\"f1\")\n",
    "\n",
    "writer_clf.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Change to paper name\n",
    "df_allThreeClassifiers_paperName=df_best_result_allThreeClassifiers.copy()\n",
    "index_bag=[]\n",
    "for exp_str in df_best_result_allThreeClassifiers.index:\n",
    "    experiment_name, feature_name=exp_str.split(\" >> \")\n",
    "    paper_idx='+'.join([Swap2PaperName(n, PprNmeMp) for n in feature_name.split(\"+\")])\n",
    "    index_bag.append(paper_idx)\n",
    "df_allThreeClassifiers_paperName.index=index_bag\n",
    "\n",
    "df_allThreeClassifiers_paperName.to_excel(Result_path+\"/\"+f\"TASLPTABLE-ClassFusion_Norm[{args.Normalize_way}].xlsx\")\n",
    "print(\"df_allThreeClassifiers_paperName generated at \", Result_path+\"/\"+f\"TASLPTABLE-Class_Norm[{args.Normalize_way}].xlsx\")\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Analysis part\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def Organize_Needed_SHAP_info(Incorrect2Correct_indexes, Session_level_all, proposed_expstr):\n",
    "    Incorrect2Correct_info_dict=Dict()\n",
    "    for tst_idx in Incorrect2Correct_indexes:\n",
    "        for key, values in Session_level_all[proposed_expstr]['SHAP_info'].items():\n",
    "            test_fold_idx=[int(k) for k in key.split(\"_\")]\n",
    "            for i,ii in enumerate(test_fold_idx): #ii is the index of the sample, i is the position of this sample in this test fold\n",
    "                if tst_idx == ii:\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['XTest']=values['XTest'].iloc[i,:]\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['explainer_expected_value']=values['explainer_expected_value']\n",
    "                    \n",
    "                    # 因為之前都用predict_proba 但是自從發現predict_proba有BUG之後就改用decision function了，\n",
    "                    # 但是改成decision function之後資料結構會跟原來的不一樣，所以額外仿造了一個logit的values\n",
    "                    # shap_values_array=[array[i,:] for array in values['shap_values']]\n",
    "                    shap_values_array=[array[i,:] for array in [values['shap_values'],-values['shap_values']]]\n",
    "                    \n",
    "                    df_shap_values=pd.DataFrame(shap_values_array,columns=Incorrect2Correct_info_dict[tst_idx]['XTest'].index)\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['shap_values']=df_shap_values\n",
    "                    print(\"testing sample \", ii, \"is in the \", i, \"position of test fold\", key)\n",
    "                    assert (Incorrect2Correct_info_dict[tst_idx]['XTest'] == Session_level_all[proposed_expstr]['X'].iloc[tst_idx]).all()\n",
    "                    # print(\"It's feature value captured is\", Incorrect2Correct_info_dict[tst_idx]['XTest'])\n",
    "                    # print(\"It's original X value is\", Session_level_all[proposed_expstr]['X'].iloc[tst_idx])\n",
    "                    # print(\"See if they match\")\n",
    "    return Incorrect2Correct_info_dict\n",
    "# =============================================================================\n",
    "# Debug\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# \\Debug\n",
    "# =============================================================================\n",
    "def Get_Model_Type12Errors(model_str='baseline', tureLab_str='y_true'):\n",
    "    # Positive = ASD\n",
    "    Type1Err= ( df_Y_pred[model_str]  == sellect_people_define.ASDTD_label['ASD']) & ( df_Y_pred[tureLab_str] == sellect_people_define.ASDTD_label['TD']  )\n",
    "    Type2Err= ( df_Y_pred[model_str]  == sellect_people_define.ASDTD_label['TD'] ) & ( df_Y_pred[tureLab_str] == sellect_people_define.ASDTD_label['ASD']  )\n",
    "    return Type1Err, Type2Err\n",
    "# =============================================================================\n",
    "\n",
    "'''\n",
    "\n",
    "    Part 1: Check incorrect to correct and correct to incorrect\n",
    "\n",
    "'''\n",
    "\n",
    "if args.Print_Analysis_grp_Manual_select == True:\n",
    "    count=0\n",
    "    for exp_lst in FeatureLabelMatch_manual:\n",
    "        exp_lst_str='::'.join(exp_lst)\n",
    "        if count < len(FeatureLabelMatch_manual)/3:\n",
    "            print(\"proposed_expstr='{}'\".format(exp_lst_str))\n",
    "        else:\n",
    "            print(\"baseline_expstr='{}'\".format(exp_lst_str))\n",
    "        count+=1\n",
    "    \n",
    "\n",
    "############################################################\n",
    "# XXX 選擇要分析的地方\n",
    "# Reverse_exp=False 代表我們檢查從baseline到proposed; \n",
    "# Reverse_exp=True 代表我們檢查從proposed到baseline; \n",
    "# 所以如果是要算I->I+ 就proposed_expstr=I+, baseline_expstr=I; 如果是要算I->I- 就proposed_expstr=I+, baseline_expstr=I 然後Reverse_exp=True\n",
    "\n",
    "\n",
    "# Low Minimal\n",
    "proposed_expstr='TD vs df_feature_lowMinimal_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Syncrony_cols+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
    "baseline_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "Reverse_exp=False\n",
    "\n",
    "[4, 6, 15]\n",
    "[0, 25]\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_lowMinimal_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Syncrony_cols+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Moderate\n",
    "# proposed_expstr='TD vs df_feature_moderate_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_moderate_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=False\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_moderate_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_moderate_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=True\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "# high\n",
    "# proposed_expstr='TD vs df_feature_high_CSS >> DEP_columns+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_high_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=False\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_high_CSS >> DEP_columns+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_high_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=True\n",
    "\n",
    "\n",
    "\n",
    "experiment_title=baseline_expstr[re.search(\"df_feature_\",baseline_expstr).end():re.search(\"_CSS >> \",baseline_expstr).start()]\n",
    "\n",
    "proposed_featset_lst=proposed_expstr[re.search(\" >> \",proposed_expstr).end():re.search(\"::\",proposed_expstr).start()].split(\"+\")\n",
    "baseline_featset_lst=baseline_expstr[re.search(\" >> \",baseline_expstr).end():re.search(\"::\",baseline_expstr).start()].split(\"+\")\n",
    "Additional_featureSet=set(proposed_featset_lst) - set(baseline_featset_lst)\n",
    "print(\"For Task\", experiment_title, \" additional feature sets are\", Additional_featureSet)\n",
    "# =============================================================================\n",
    "# Error type analyses\n",
    "# =============================================================================\n",
    "# df_compare_pair=pd.DataFrame(list())\n",
    "Y_pred_lst=[\n",
    "Session_level_all[proposed_expstr]['y_pred'],\n",
    "Session_level_all[baseline_expstr]['y_pred'],\n",
    "Session_level_all[proposed_expstr]['y_true'],\n",
    "Session_level_all[proposed_expstr]['y_true'].index,\n",
    "]\n",
    "assert (Session_level_all[proposed_expstr]['y_true'] == Session_level_all[baseline_expstr]['y_true']).all()\n",
    "\n",
    "df_Y_pred=pd.DataFrame(Y_pred_lst[:-1],index=['proposed','baseline','y_true']).T\n",
    "df_Y_pred_withName=pd.DataFrame(Y_pred_lst,index=['proposed','baseline','y_true','name']).T\n",
    "df_Index2Name_mapping=df_Y_pred_withName['name']\n",
    "\n",
    "\n",
    "Incorrect=df_Y_pred['baseline'] != df_Y_pred['y_true']\n",
    "Correct=df_Y_pred['proposed'] == df_Y_pred['y_true']\n",
    "Incorrect2Correct= Correct & Incorrect\n",
    "\n",
    "\n",
    "Incorrect=df_Y_pred['baseline'] == df_Y_pred['y_true']\n",
    "Correct=df_Y_pred['proposed'] != df_Y_pred['y_true']\n",
    "Correct2Incorrect= Correct & Incorrect\n",
    "\n",
    "Incorrect2Correct_indexes=list(df_Y_pred[Incorrect2Correct].index)\n",
    "Correct2Incorrect_indexes=list(df_Y_pred[Correct2Incorrect].index)\n",
    "print('Incorrect2Correct_indexes: ', Incorrect2Correct_indexes)\n",
    "print('Correct2Incorrect_indexes: ', Correct2Incorrect_indexes)\n",
    "\n",
    "\n",
    "Ones=df_Y_pred['baseline'] ==sellect_people_define.ASDTD_label['ASD']\n",
    "Twos=df_Y_pred['proposed'] ==sellect_people_define.ASDTD_label['TD']\n",
    "Ones2Twos=  Ones & Twos\n",
    "\n",
    "Twos=df_Y_pred['baseline'] ==sellect_people_define.ASDTD_label['TD']\n",
    "Ones=df_Y_pred['proposed'] ==sellect_people_define.ASDTD_label['ASD']\n",
    "Twos2Ones=  Ones & Twos\n",
    "\n",
    "Ones2Twos_indexes=list(df_Y_pred[Ones2Twos].index)\n",
    "Twos2Ones_indexes=list(df_Y_pred[Twos2Ones].index)\n",
    "\n",
    "assert len(Ones2Twos_indexes+Twos2Ones_indexes) == len(Incorrect2Correct_indexes+Correct2Incorrect_indexes)\n",
    "\n",
    "ASDTD2Logit_map={\n",
    "    'TD': sellect_people_define.ASDTD_label['TD']-1,\n",
    "    'ASD': sellect_people_define.ASDTD_label['ASD']-1,\n",
    "    }\n",
    "\n",
    "quadrant1_indexes=intersection(Correct2Incorrect_indexes, Ones2Twos_indexes)\n",
    "quadrant2_indexes=intersection(Incorrect2Correct_indexes, Ones2Twos_indexes)\n",
    "quadrant3_indexes=intersection(Correct2Incorrect_indexes, Twos2Ones_indexes)\n",
    "quadrant4_indexes=intersection(Incorrect2Correct_indexes, Twos2Ones_indexes)\n",
    "\n",
    "\n",
    "Type1Err_dict, Type2Err_dict={}, {}\n",
    "for model_str in ['baseline', 'proposed']:\n",
    "    Type1Err_dict[model_str], Type2Err_dict[model_str] = Get_Model_Type12Errors(model_str=model_str, tureLab_str='y_true')\n",
    "\n",
    "model_str='baseline'\n",
    "Type1Err_baseline_indexes=list(df_Y_pred[Type1Err_dict[model_str]].index)\n",
    "Type2Err_baseline_indexes=list(df_Y_pred[Type2Err_dict[model_str]].index)\n",
    "model_str='proposed'\n",
    "Type1Err_proposed_indexes=list(df_Y_pred[Type1Err_dict[model_str]].index)\n",
    "Type2Err_proposed_indexes=list(df_Y_pred[Type2Err_dict[model_str]].index)\n",
    "\n",
    "\n",
    "\n",
    "All_err_indexes=list(set(Type1Err_baseline_indexes+Type2Err_baseline_indexes+Type1Err_proposed_indexes+Type2Err_proposed_indexes))\n",
    "All_indexes=list(df_Y_pred.index)\n",
    "'''\n",
    "\n",
    "    Part 2: Check the SHAP values based on indexes in part 1\n",
    "    \n",
    "    先紀錄，再執行分析和畫圖\n",
    "\n",
    "'''\n",
    "Manual_inspect_idxs=[2, 3, 29, 36, 44]\n",
    "##############################################\n",
    "\n",
    "# selected_idxs=Ones2Twos_indexes+Twos2Ones_indexes\n",
    "selected_idxs=All_indexes\n",
    "Baseline_changed_info_dict=Organize_Needed_SHAP_info(selected_idxs, Session_level_all, baseline_expstr)\n",
    "Proposed_changed_info_dict=Organize_Needed_SHAP_info(selected_idxs, Session_level_all, proposed_expstr)\n",
    "\n",
    "Baseline_totalPoeple_info_dict=Organize_Needed_SHAP_info(df_Y_pred.index, Session_level_all, baseline_expstr)\n",
    "Proposed_totalPoeple_info_dict=Organize_Needed_SHAP_info(df_Y_pred.index, Session_level_all, proposed_expstr)\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "# 個體分析： 會存到SAHP_figures/{quadrant}/的資料夾，再開Jupyter去看\n",
    "def Organize_Needed_decisionProb(Incorrect2Correct_indexes, Session_level_all, proposed_expstr):\n",
    "    Incorrect2Correct_info_dict=Dict()\n",
    "    for tst_idx in Incorrect2Correct_indexes:\n",
    "        for key, values in Session_level_all[proposed_expstr]['Logit{}_predictproba'.format(logit_number)].items():\n",
    "            test_fold_idx=[int(k) for k in key.split(\"_\")]\n",
    "            for i,ii in enumerate(test_fold_idx): #ii is the index of the sample, i is the position of this sample in this test fold\n",
    "                if tst_idx == ii:\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['predictproba']=values['predictproba'][i]\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['decisionfunc']=values['decisionfunc'][i]\n",
    "\n",
    "    return Incorrect2Correct_info_dict\n",
    "\n",
    "SHAP_save_path_root=\"SHAP_figures/{quadrant}/\"\n",
    "# step 1: prepare data\n",
    "\n",
    "# selected_idxs=Ones2Twos_indexes+Twos2Ones_indexes\n",
    "Baseline_changed_decision_info_dict=Organize_Needed_decisionProb(selected_idxs, Session_level_all, baseline_expstr)\n",
    "Proposed_changed_decision_info_dict=Organize_Needed_decisionProb(selected_idxs, Session_level_all, proposed_expstr)\n",
    "Baseline_total_decision_info_dict=Organize_Needed_decisionProb(df_Y_pred.index, Session_level_all, baseline_expstr)\n",
    "Proposed_total_decision_info_dict=Organize_Needed_decisionProb(df_Y_pred.index, Session_level_all, proposed_expstr)\n",
    "\n",
    "\n",
    "\n",
    "df_Proposed_changed_decision_info_dict=pd.DataFrame.from_dict(Proposed_changed_decision_info_dict,orient='index')\n",
    "df_Baseline_changed_decision_info_dict=pd.DataFrame.from_dict(Baseline_changed_decision_info_dict,orient='index')\n",
    "df_Proposed_total_decision_info_dict=pd.DataFrame.from_dict(Proposed_total_decision_info_dict,orient='index')\n",
    "df_Baseline_total_decision_info_dict=pd.DataFrame.from_dict(Baseline_total_decision_info_dict,orient='index')\n",
    "Sample_idxs_array=df_Baseline_changed_decision_info_dict.index.values\n",
    "\n",
    "df_Y_true=df_Y_pred.loc[df_Baseline_changed_decision_info_dict.index]['y_true']\n",
    "df_Y_true_ASD_bool=df_Y_true==sellect_people_define.ASDTD_label['ASD']\n",
    "df_Y_true_TD_bool=df_Y_true==sellect_people_define.ASDTD_label['TD']\n",
    "Incorrect_baseline=df_Y_pred['baseline'] != df_Y_pred['y_true']\n",
    "Incorrect_proposed=df_Y_pred['proposed'] != df_Y_pred['y_true']\n",
    "Correct_baseline=df_Y_pred['baseline'] == df_Y_pred['y_true']\n",
    "Correct_proposed=df_Y_pred['proposed'] == df_Y_pred['y_true']\n",
    "\n",
    "import shutil\n",
    "from collections import Counter\n",
    "shutil.rmtree(SHAP_save_path_root.format(quadrant=\"\"), ignore_errors = True)\n",
    "\n",
    "\n",
    "#%%\n",
    "Analysis_grp_bool=False\n",
    "N=5\n",
    "Xtest_dict={}\n",
    "expected_value_lst=[]\n",
    "UsePaperName_bool=True\n",
    "Quadrant_FeatureImportance_dict={}\n",
    "Quadrant_feature_AddedTopFive_dict={}\n",
    "Quadrant_feature_AddedFeatureImportance_dict={}\n",
    "Manual_inspect_idxs=[21, 1, 14, 15]\n",
    "df_Result_dict=Dict()\n",
    "# =============================================================================\n",
    "# 有predict probability的變數\n",
    "# 最後結果會放在一個nice table就不用一直手動複製了  \n",
    "# nice table 就放在df_Total_ErrAnal這個變數\n",
    "# 一路上都有放assertion做驗證所以等到assertion有誤的時候再去debug\n",
    "# XXX  放force plot的地方\n",
    "# =============================================================================\n",
    "# for Analysis_grp_str in ['Manual_inspect_idxs','quadrant1_indexes','quadrant2_indexes','quadrant3_indexes','quadrant4_indexes']:\n",
    "for Analysis_grp_str in ['quadrant1_indexes','quadrant2_indexes','quadrant3_indexes','quadrant4_indexes']:    \n",
    "# for Analysis_grp_str in ['quadrant2_indexes','quadrant4_indexes']:\n",
    "# for Analysis_grp_str in ['Manual_inspect_idxs']:\n",
    "    Analysis_grp_indexes=vars()[Analysis_grp_str]\n",
    "    df_shap_values_stacked=pd.DataFrame([])\n",
    "    for Inspect_samp in Analysis_grp_indexes:\n",
    "        shap_info_proposed=Proposed_changed_info_dict[Inspect_samp]\n",
    "        shap_info_baseline=Baseline_changed_info_dict[Inspect_samp]\n",
    "        # expected_value_proposed=shap_info_proposed['explainer_expected_value'][logit_number]\n",
    "        # expected_value_baseline=shap_info_baseline['explainer_expected_value'][logit_number]\n",
    "        expected_value_proposed=shap_info_proposed['explainer_expected_value']\n",
    "        expected_value_baseline=shap_info_baseline['explainer_expected_value']\n",
    "        df_shap_values_proposed=shap_info_proposed['shap_values'].loc[[logit_number]]\n",
    "        df_shap_values_baseline=shap_info_baseline['shap_values'].loc[[logit_number]]\n",
    "        deltaprob_baseline=df_shap_values_baseline.T.sum()\n",
    "        deltaprob_proposed=df_shap_values_proposed.T.sum()\n",
    "        BaselineFeatures=[getattr(FeatSel,k)  for k in baseline_featset_lst]\n",
    "        BaselineFeatures_flatten=[e for ee in BaselineFeatures for e in ee]\n",
    "        ProposedFeatures=[getattr(FeatSel,k)  for k in proposed_featset_lst]\n",
    "        ProposedFeatures_flatten=[e for ee in ProposedFeatures for e in ee]\n",
    "        Lists_of_addedFeatures=[getattr(FeatSel,k)  for k in Additional_featureSet]\n",
    "        Lists_of_addedFeatures_flatten=[e for ee in Lists_of_addedFeatures for e in ee]\n",
    "        deltaprob_baselineFeat_baseline=df_shap_values_baseline[BaselineFeatures_flatten].T.sum()\n",
    "        deltaprob_baselineFeat_proposed=df_shap_values_proposed[ProposedFeatures_flatten].T.sum()\n",
    "        deltaprob_additionalFeat_proposed=df_shap_values_proposed[Lists_of_addedFeatures_flatten].T.sum()\n",
    "        \n",
    "        \n",
    "        Prediction_prob_baseline=df_Baseline_changed_decision_info_dict.loc[Inspect_samp,'decisionfunc']\n",
    "        Prediction_prob_proposed=df_Proposed_changed_decision_info_dict.loc[Inspect_samp,'decisionfunc']\n",
    "        # Prediction_prob_baseline=df_Baseline_changed_decision_info_dict.loc[Inspect_samp,'predictproba']\n",
    "        # Prediction_prob_proposed=df_Proposed_changed_decision_info_dict.loc[Inspect_samp,'predictproba']\n",
    "        # =====================================================================\n",
    "        if Reverse_exp == True:\n",
    "            shap_info=shap_info_baseline\n",
    "        else:\n",
    "            shap_info=shap_info_proposed\n",
    "        # expected_value=shap_info['explainer_expected_value'][logit_number]\n",
    "        expected_value=shap_info['explainer_expected_value']\n",
    "        shap_values=shap_info['shap_values'].loc[logit_number].values\n",
    "        df_shap_values=shap_info['shap_values'].loc[[logit_number]]\n",
    "        df_shap_values.index=[Inspect_samp]\n",
    "        \n",
    "        df_shap_values_T=df_shap_values.T\n",
    "        \n",
    "        if Reverse_exp == True:\n",
    "            Selected_feature_set=BaselineFeatures_flatten\n",
    "        else:\n",
    "            Selected_feature_set=ProposedFeatures_flatten\n",
    "        Xtest=shap_info['XTest']        \n",
    "        Xtest_additionaFeat=Xtest[Selected_feature_set]\n",
    "        Xtest_additionaFeat.index=[ Swap2PaperName(name,PprNmeMp) for name in Xtest_additionaFeat.index]\n",
    "        \n",
    "        Xtest_dict[Inspect_samp]=Xtest\n",
    "        df_shap_values_stacked=pd.concat([df_shap_values_stacked,df_shap_values],)\n",
    "        expected_value_lst.append(expected_value)\n",
    "    \n",
    "        # 這個部份跑TASLP的Fig.7 也就是說明有些不顯著的feature卻shap value很高\n",
    "        SHAP_save_path=SHAP_save_path_root.format(quadrant=Analysis_grp_str)\n",
    "        if not os.path.exists(SHAP_save_path):\n",
    "            os.makedirs(SHAP_save_path)\n",
    "        # ============================================================================= \n",
    "        # 觀察哪些additional feature有加分效用\n",
    "        df_shap_values_additionaFeat=df_shap_values_T.loc[Selected_feature_set].T\n",
    "        df_shap_values_toInspect=df_shap_values_T.loc[Selected_feature_set]\n",
    "        deltaprob_additionaFeat_proposed=df_shap_values_additionaFeat.T.sum()\n",
    "        # 把同一個feature set的SHAP values加總\n",
    "        # {Swap2PaperName(FestSet,PprNmeMp): for FestSet in Additional_featureSet:}\n",
    "        if Reverse_exp == True:\n",
    "            used_feature_lst=baseline_featset_lst\n",
    "        else:\n",
    "            used_feature_lst=proposed_featset_lst\n",
    "        FeatContrib_dict={}\n",
    "        for FestSet in used_feature_lst:\n",
    "            FeatContrib_dict[Swap2PaperName(FestSet,PprNmeMp)]=df_shap_values_toInspect.loc[getattr(FeatSel,FestSet)].sum().values[0]\n",
    "            df_shap_values_toInspect.loc[getattr(FeatSel,FestSet)].sum().values[0]\n",
    "        Sum_SHAPval=sum([v for k,v in FeatContrib_dict.items()])\n",
    "        \n",
    "        \n",
    "\n",
    "        # =============================================================================\n",
    "        # XXXDebug area\n",
    "        # =============================================================================\n",
    "        def Unittest_shapEqualsDecision():\n",
    "            tolarance=1e-5\n",
    "            df_Inconsistant=pd.DataFrame([],columns=['ValueFromShap','decision_value'])\n",
    "            for people in Proposed_totalPoeple_info_dict.keys():\n",
    "                Sum_shap_value=Proposed_totalPoeple_info_dict[people]['shap_values'].loc[logit_number].sum()\n",
    "                baseVal=Proposed_totalPoeple_info_dict[people]['explainer_expected_value']\n",
    "                ValueFromShap=baseVal + Sum_shap_value\n",
    "                decision_value=df_Proposed_total_decision_info_dict.loc[people,'decisionfunc']\n",
    "                if (ValueFromShap - decision_value) > tolarance:\n",
    "                    df_Inconsistant.loc[people]=[ValueFromShap,decision_value]\n",
    "        Unittest_shapEqualsDecision()\n",
    "        # =============================================================================\n",
    "        # /Debug area\n",
    "        # =============================================================================\n",
    "        \n",
    "            \n",
    "        # if Reverse_exp == True:\n",
    "        #     FeatContrib_percent_dict = {k:'{}%'.format('%.1f' % -np.round(v*100,3)) for k,v in FeatContrib_dict.items()}\n",
    "        # else:\n",
    "        #     FeatContrib_percent_dict = {k:'{}%'.format('%.1f' % np.round(v*100,3)) for k,v in FeatContrib_dict.items()}\n",
    "        FeatContrib_str_dict = {k:'{}'.format('%.3f' % np.round(v,3)) for k,v in FeatContrib_dict.items()}\n",
    "        SHAP_percent_lst=[float(v.replace(\"%\",\"\")) for k,v in FeatContrib_str_dict.items()]\n",
    "        # SHAP_percent_lst=[float(v.replace(\"%\",\"\")) for k,v in FeatContrib_percent_dict.items()]\n",
    "        \n",
    "        # df_FeatContrib 的整體在這裡\n",
    "        df_FeatContrib=pd.DataFrame([\"\\\\textbf{base val}:\"+ str(np.round(expected_value_proposed,3)) ],columns=['Feat_distrib'])\n",
    "        suffix_str=\" + \".join([str(np.round(v,3)) for v in SHAP_percent_lst])\n",
    "        print(\"base value\", np.round(expected_value_proposed,3), \"%\")\n",
    "        # for key, value in FeatContrib_str_dict.items():\n",
    "        #     print(key, \" : \",value)\n",
    "        #     # print(Swap2PaperName(key,PprNmeMp,method='idx'), \" : \",value)\n",
    "        #     # df_FeatContrib=pd.concat([df_FeatContrib,pd.DataFrame([','+str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \": \"+value])],axis=1)\n",
    "        #     df_FeatContrib['Feat_distrib']=df_FeatContrib['Feat_distrib']+','+str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \":\"+value\n",
    "        \n",
    "        def AppendStr(FeatContrib_str_dict):\n",
    "            Append_str=''\n",
    "            for i,(key, value) in enumerate(FeatContrib_str_dict.items()):\n",
    "                Append_str+=str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \":\"+value\n",
    "                if i % 2 ==1  and i < len(FeatContrib_str_dict.keys())-1:\n",
    "                    Append_str+='\\n'\n",
    "                elif i % 2 ==0 and i < len(FeatContrib_str_dict.keys())-1:\n",
    "                    Append_str+=','\n",
    "                else:\n",
    "                    Append_str+=''\n",
    "            return Append_str\n",
    "        Append_str=AppendStr(FeatContrib_str_dict)\n",
    "        # Append_str=','.join([str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \":\"+value for key, value in FeatContrib_str_dict.items()])\n",
    "        df_FeatContrib=pd.concat([df_FeatContrib,pd.DataFrame([Append_str],columns=df_FeatContrib.columns)],axis=0)\n",
    "        #######################################################################\n",
    "        \n",
    "        if Reverse_exp == True:\n",
    "            FinalProb_fromSHAP=expected_value_baseline + sum([v for k,v in FeatContrib_dict.items()])\n",
    "        else:\n",
    "            FinalProb_fromSHAP=expected_value_proposed + sum([v for k,v in FeatContrib_dict.items()])\n",
    "        \n",
    "        print(np.round(expected_value_proposed,3),\"+\",suffix_str, '=', FinalProb_fromSHAP)\n",
    "        if Reverse_exp == True:\n",
    "            # TDProb_str=\"TD: ({Prediction_prob_proposed}% → {Prediction_prob_baseline}%)\".format(\n",
    "            #                                                                 Prediction_prob_baseline='%.1f' % np.round(Prediction_prob_baseline*100,3),\n",
    "            #                                                                 Prediction_prob_proposed='%.1f' % np.round(Prediction_prob_proposed*100,3)\n",
    "            #                                                                 )\n",
    "            # ASDProb_str=\"ASD: ({Prediction_prob_proposed}% → {Prediction_prob_baseline}%)\".format(\n",
    "            #                                                                 Prediction_prob_baseline='%.1f' % np.round((1-Prediction_prob_baseline)*100,3),\n",
    "            #                                                                 Prediction_prob_proposed='%.1f' % np.round((1-Prediction_prob_proposed)*100,3)\n",
    "            #                                                                 )\n",
    "            # print(TDProb_str)\n",
    "            # print(ASDProb_str)\n",
    "            from_str = \"TD\"  if Prediction_prob_proposed > decision_boundary else \"ASD\"\n",
    "            to_str = \"TD\"  if Prediction_prob_baseline > decision_boundary else \"ASD\"\n",
    "            PredictionProb_str=\"{from_str}: {Prediction_prob_proposed:.2f}→{to_str}: {Prediction_prob_baseline:.2f}\".format(from_str=from_str,\n",
    "                                                                                                        to_str=to_str,\n",
    "                                                                                                        Prediction_prob_proposed=Prediction_prob_proposed,\n",
    "                                                                                                        Prediction_prob_baseline=Prediction_prob_baseline,\n",
    "                                                                                                        )\n",
    "            \n",
    "        else:\n",
    "            # TDProb_str=\"TD: ({Prediction_prob_baseline}% → {Prediction_prob_proposed}%)\".format(\n",
    "            #                                                                 Prediction_prob_baseline='%.1f' % np.round(Prediction_prob_baseline*100,3),\n",
    "            #                                                                 Prediction_prob_proposed='%.1f' % np.round(Prediction_prob_proposed*100,3)\n",
    "            #                                                                 )\n",
    "            # ASDProb_str=\"ASD: ({Prediction_prob_baseline}% → {Prediction_prob_proposed}%)\".format(\n",
    "            #                                                                  Prediction_prob_baseline='%.1f' % np.round((1-Prediction_prob_baseline)*100,3),\n",
    "            #                                                                  Prediction_prob_proposed='%.1f' % np.round((1-Prediction_prob_proposed)*100,3)\n",
    "            #                                                                 )\n",
    "            # print(TDProb_str)\n",
    "            # print(ASDProb_str)\n",
    "            from_str = \"TD\"  if Prediction_prob_baseline > decision_boundary else \"ASD\"\n",
    "            to_str = \"TD\"  if Prediction_prob_proposed > decision_boundary else \"ASD\"\n",
    "            PredictionProb_str=\"{from_str}:{Prediction_prob_baseline:.2f}→{to_str}:{Prediction_prob_proposed:.2f}\".format(from_str=from_str,\n",
    "                                                                                                        to_str=to_str,\n",
    "                                                                                                        Prediction_prob_proposed=Prediction_prob_proposed,\n",
    "                                                                                                        Prediction_prob_baseline=Prediction_prob_baseline,\n",
    "                                                                                                        )\n",
    "        print(PredictionProb_str)\n",
    "        assert from_str != to_str\n",
    "        # 這邊確認SHAP value加起來要從baseline的score到proposed的score\n",
    "        if Reverse_exp == True:\n",
    "            assert np.round(Prediction_prob_baseline,2) == np.round(FinalProb_fromSHAP,2)\n",
    "        else:\n",
    "            assert np.round(Prediction_prob_proposed,2) == np.round(FinalProb_fromSHAP,2)\n",
    "        \n",
    "        \n",
    "        df_instance=pd.DataFrame([str(Inspect_samp)])\n",
    "        df_PredictionProb=pd.DataFrame()\n",
    "        \n",
    "        if Reverse_exp == True:\n",
    "            if Analysis_grp_str == 'quadrant1_indexes' or Analysis_grp_str == 'quadrant3_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": X→O\"])\n",
    "            elif Analysis_grp_str == 'quadrant2_indexes' or Analysis_grp_str == 'quadrant4_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": O→X\"])\n",
    "        else:\n",
    "            if Analysis_grp_str == 'quadrant1_indexes' or Analysis_grp_str == 'quadrant3_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": O→X\"])\n",
    "            elif Analysis_grp_str == 'quadrant2_indexes' or Analysis_grp_str == 'quadrant4_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": X→O\"])\n",
    "        \n",
    "        df_predictionProb=pd.DataFrame([PredictionProb_str])\n",
    "        df_predictionProbnResult=pd.concat([df_classification_result , df_predictionProb],axis=0).reset_index(drop=True)\n",
    "        # df_predictionProb=pd.concat([df_predictionProb,pd.DataFrame([TDProb_str])],axis=0)\n",
    "        # df_predictionProb=pd.concat([df_predictionProb,pd.DataFrame([ASDProb_str])],axis=0)\n",
    "        \n",
    "        # df_predictionProb.loc[0,'Prediction Prob.']=\n",
    "        # df_PredictionProb.loc[1,'Prediction Prob.']=TDProb_str\n",
    "        # df_PredictionProb.loc[2,'Prediction Prob.']=ASDProb_str\n",
    "        df_NICETABLE=pd.DataFrame()\n",
    "        # df_NICETABLE=pd.concat([df_instance,df_predictionProbnResult],axis=1, ignore_index=True).reset_index(drop=True)\n",
    "        df_NICETABLE=pd.concat([df_predictionProbnResult],axis=1, ignore_index=True).reset_index(drop=True)\n",
    "        # df_NICETABLE=pd.merge(df_instance.T,df_predictionProbnResult.T, on=[0], how='inner')\n",
    "        # pd.merge(df_instance,df_predictionProbnResult, how='outer')\n",
    "        # df_instance.merge(df_predictionProbnResult)\n",
    "        \n",
    "        df_NICETABLE.drop_duplicates()\n",
    "        df_NICETABLE.index = range(len(df_NICETABLE))\n",
    "        df_FeatContrib.index=range(len(df_FeatContrib))\n",
    "        \n",
    "        # 最後看這個變數，複製到excel上\n",
    "        df_NICETABLE=pd.concat([df_NICETABLE,df_FeatContrib],axis=1, ignore_index=True)\n",
    "        df_Result_dict[Inspect_samp]=df_NICETABLE\n",
    "        \n",
    "        \n",
    "        fig = shap.force_plot(expected_value, df_shap_values_additionaFeat.values, Xtest_additionaFeat.round(2).T, figsize=(8, 3) ,matplotlib=True,show=False)\n",
    "        # fig = shap.force_plot(expected_value, df_shap_values.values, Xtest.round(2).T, figsize=(8, 3) ,matplotlib=True,show=False)\n",
    "        \n",
    "        \n",
    "        plt.savefig(\"images/SHAP_discussion_{sample}.png\".format(sample=Inspect_samp),dpi=400, bbox_inches='tight')\n",
    "        plt.savefig(SHAP_save_path+\"{sample}.png\".format(sample=Inspect_samp),dpi=150, bbox_inches='tight')\n",
    "        \n",
    "    if UsePaperName_bool==False:\n",
    "        df_shap_values_stacked.columns=[Swap2PaperName(idx, PprNmeMp) for idx in df_shap_values_stacked.columns]\n",
    "    \n",
    "    \n",
    "    \n",
    "# pd.DataFrame.from_dict(df_Result_dict,orient='index')\n",
    "df_Total_ErrAnal_row=pd.DataFrame()\n",
    "df_Total_ErrAnal=pd.DataFrame()\n",
    "\n",
    "\n",
    "group_dict={}\n",
    "count=1\n",
    "for key, values in df_Result_dict.items():\n",
    "    df_Total_ErrAnal_row=pd.concat([df_Total_ErrAnal_row,values],axis=1)\n",
    "    if count % 3 == 0:\n",
    "        df_Total_ErrAnal=pd.concat([df_Total_ErrAnal,df_Total_ErrAnal_row])\n",
    "        df_Total_ErrAnal_row=pd.DataFrame()\n",
    "    count+=1\n",
    "\n",
    "\n",
    "if not os.path.exists(Result_path+\"/Analyzed/\"):\n",
    "    os.makedirs(Result_path+\"/Analyzed/\")\n",
    "df_Total_ErrAnal.to_excel(Result_path+\"/Analyzed/\"+\"Classification_\"+args.Feature_mode+\".xlsx\")\n",
    "#%%\n",
    "# =============================================================================\n",
    "'''\n",
    "以下就是TASLP paper沒有用到備著的部分\n",
    "'''\n",
    "# =============================================================================\n",
    "Reverse_lst=[\n",
    "'Inter-Vowel Dispersion+GC[VSC]\\\\textsubscript{inv}+Syncrony[VSC]+GC[P]\\\\textsubscript{inv}+GC[P]\\\\textsubscript{part}+Proximity[P]',\n",
    "'GC[P]\\\\textsubscript{inv}+GC[P]\\\\textsubscript{part}+Proximity[P]',\n",
    "'GC[P]\\\\textsubscript{inv}+Proximity[P]+Convergence[P]+Syncrony[P]',\n",
    "'Inter-Vowel Dispersion+GC[VSC]\\\\textsubscript{inv}+GC[VSC]\\\\textsubscript{part}+Proximity[P]',\n",
    "'Proximity[P]',\n",
    "'GC[P]\\\\textsubscript{inv}+Proximity[P]',\n",
    "'formant dependency+Proximity[P]',\n",
    "'Proximity[P]+Syncrony[P]',\n",
    "'Proximity[P]'\n",
    "]\n",
    "for nameComb in Reverse_lst:\n",
    "    print(nameComb)\n",
    "    FeatBag=[]\n",
    "    for single_feat in nameComb.split(\"+\"):\n",
    "        InversedFeat=Swap2PaperName(single_feat,PprNmeMp,method='inverse')\n",
    "        FeatBag.append(InversedFeat)\n",
    "    Feature_combination='+'.join(FeatBag)\n",
    "    print(Feature_combination)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "group_dict={}\n",
    "count=1\n",
    "for key, values in df_Result_dict.items():\n",
    "    df_Total_ErrAnal_row=pd.concat([df_Total_ErrAnal_row,values],axis=1)\n",
    "    if count % 3 == 0:\n",
    "        df_Total_ErrAnal=pd.concat([df_Total_ErrAnal,df_Total_ErrAnal_row])\n",
    "        df_Total_ErrAnal_row=pd.DataFrame()\n",
    "    print(df_Total_ErrAnal)\n",
    "    count+=1\n",
    "\n",
    "\n",
    "if not os.path.exists(Result_path+\"/Analyzed/\"):\n",
    "    os.makedirs(Result_path+\"/Analyzed/\")\n",
    "df_Total_ErrAnal_row.to_excel(Result_path+\"/Analyzed/\"+\"Classification_\"+args.Feature_mode+\".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TASLPReview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
