{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jun 30 15:56:45 2021\n",
    "\n",
    "@author: jackchen\n",
    "\n",
    "\n",
    "    This is a branch of MainClassification that run specific experiments\n",
    "    \n",
    "    Include SHAP values in this script\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import spearmanr,pearsonr \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from addict import Dict\n",
    "# import functions\n",
    "import argparse\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn.svm\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from articulation.HYPERPARAM import phonewoprosody, Label\n",
    "from articulation.HYPERPARAM.PeopleSelect import SellectP_define\n",
    "import articulation.HYPERPARAM.FeatureSelect as FeatSel\n",
    "\n",
    "import articulation.articulation\n",
    "from sklearn.metrics import f1_score,recall_score,roc_auc_score,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import combinations\n",
    "import shap\n",
    "import articulation.HYPERPARAM.PaperNameMapping as PprNmeMp\n",
    "import inspect\n",
    "import seaborn as sns\n",
    "\n",
    "from articulation.HYPERPARAM.PlotFigureVars import *\n",
    "\n",
    "def Find_Experiment_actualindex(Total_FeatComb_idxs,search_string):\n",
    "    # usage:\n",
    "    # e.x. :\n",
    "    # search_string='Phonation_Trend_K_cols+Phonation_Syncrony_cols+Phonation_Trend_D_cols'\n",
    "    # Total_FeatComb_idxs=FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation['static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation'].keys()\n",
    "    # Find_Experiment_actualindex(Total_FeatComb_idxs,search_string)\n",
    "    for FC_str in Total_FeatComb_idxs:\n",
    "        if ''.join(sorted(FC_str.split(\"+\"))) == ''.join(sorted(search_string.split(\"+\"))):\n",
    "            print(FC_str)\n",
    "\n",
    "\n",
    "\n",
    "def Assert_labelfeature(feat_name,lab_name):\n",
    "    # =============================================================================\n",
    "    #     To check if the label match with feature\n",
    "    # =============================================================================\n",
    "    for i,n in enumerate(feat_name):\n",
    "        assert feat_name[i] == lab_name[i]\n",
    "\n",
    "def FilterFile_withinManualName(files,Manual_choosen_feature):\n",
    "    files_manualChoosen=[f  for f in files if os.path.basename(f).split(\".\")[0]  in Manual_choosen_feature]\n",
    "    return files_manualChoosen\n",
    "\n",
    "def Merge_dfs(df_1, df_2):\n",
    "    return pd.merge(df_1,df_2,left_index=True, right_index=True)\n",
    "\n",
    "def Add_label(df_formant_statistic,Label,label_choose='ADOS_cate_C'):\n",
    "    for people in df_formant_statistic.index:\n",
    "        bool_ind=Label.label_raw['name']==people\n",
    "        df_formant_statistic.loc[people,label_choose]=Label.label_raw.loc[bool_ind,label_choose].values\n",
    "    return df_formant_statistic\n",
    "def Swap2PaperName(feature_rawname,PprNmeMp,method='origin'):\n",
    "    if method=='origin':\n",
    "        PaperNameMapping=PprNmeMp.Paper_name_map\n",
    "    elif method=='inverse':\n",
    "        PaperNameMapping=PprNmeMp.Inverse_Paper_name_map\n",
    "    elif method=='idx':\n",
    "        PaperNameMapping=PprNmeMp.Feature2idx_map\n",
    "    \n",
    "    if feature_rawname in PaperNameMapping.keys():\n",
    "        featurename_paper=PaperNameMapping[feature_rawname]\n",
    "        feature_keys=featurename_paper\n",
    "    else: \n",
    "        feature_keys=feature_rawname\n",
    "    return feature_keys\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def Return_ALigned_dfXtestndfShapValues(Feature_SHAP_info_dict,logit_number=1):\n",
    "    #Inputs\n",
    "    # logit_numberit_number=1\n",
    "    # Feature_SHAP_info_dict=Proposed_changed_info_dict\n",
    "    ###################################\n",
    "    df_XTest_stacked=pd.DataFrame()\n",
    "    df_ShapValues_stacked=pd.DataFrame()\n",
    "    for people in Feature_SHAP_info_dict.keys():\n",
    "        df_XTest=Feature_SHAP_info_dict[people]['XTest']\n",
    "        df_ShapValues=Feature_SHAP_info_dict[people]['shap_values'].loc[logit_number]\n",
    "        df_ShapValues.name=df_XTest.name\n",
    "        df_XTest_stacked=pd.concat([df_XTest_stacked,df_XTest],axis=1)\n",
    "        df_ShapValues_stacked=pd.concat([df_ShapValues_stacked,df_ShapValues],axis=1)\n",
    "    return df_XTest_stacked,df_ShapValues_stacked\n",
    "\n",
    "def Calculate_XTestShape_correlation(Feature_SHAP_info_dict,logit_number=1):\n",
    "    df_XTest_stacked,df_ShapValues_stacked=Return_ALigned_dfXtestndfShapValues(Feature_SHAP_info_dict,logit_number=logit_number)\n",
    "    Correlation_XtestnShap={}\n",
    "    for features in df_XTest_stacked.index:\n",
    "        r,p=pearsonr(df_XTest_stacked.loc[features],df_ShapValues_stacked.loc[features])\n",
    "        Correlation_XtestnShap[features]=r\n",
    "    df_Correlation_XtestnShap=pd.DataFrame.from_dict(Correlation_XtestnShap,orient='index')\n",
    "    df_Correlation_XtestnShap.columns=['correlation w logit:{}'.format(logit_number)]\n",
    "    return df_Correlation_XtestnShap\n",
    "\n",
    "def Prepare_data_for_summaryPlot(SHAPval_info_dict, feature_columns=None, PprNmeMp=None):\n",
    "    keys_bag=[]\n",
    "    XTest_dict={}\n",
    "    shap_values_0_bag=[]\n",
    "    shap_values_1_bag=[]\n",
    "    for people in sorted(SHAPval_info_dict.keys()):\n",
    "        keys_bag.append(people)\n",
    "        if not feature_columns == None:\n",
    "            df_=SHAPval_info_dict[people]['XTest'][feature_columns]\n",
    "            df_shape_value=SHAPval_info_dict[people]['shap_values'][feature_columns]\n",
    "        else:\n",
    "            df_=SHAPval_info_dict[people]['XTest']\n",
    "            df_shape_value=SHAPval_info_dict[people]['shap_values']\n",
    "        # if not SumCategorical_feats == None:\n",
    "        #     for k,values in SumCategorical_feats.items():\n",
    "        #         df_[k]=df_.loc[values].sum()\n",
    "        XTest_dict[people]=df_        \n",
    "        shap_values_0_bag.append(df_shape_value.loc[0].values)\n",
    "        shap_values_1_bag.append(df_shape_value.loc[1].values)\n",
    "    shap_values_0_array=np.vstack(shap_values_0_bag)\n",
    "    shap_values_1_array=np.vstack(shap_values_1_bag)\n",
    "    shap_values=[shap_values_0_array,shap_values_1_array]\n",
    "    # df_XTest=pd.DataFrame.from_dict(XTest_dict,orient='index')\n",
    "    df_XTest=pd.DataFrame.from_dict(XTest_dict).T\n",
    "    if PprNmeMp!=None:\n",
    "        df_XTest.columns=[Swap2PaperName(k,PprNmeMp) for k in df_XTest.columns]\n",
    "    return shap_values, df_XTest, keys_bag\n",
    "\n",
    "def get_args():\n",
    "    # we add compulsary arguments as named arguments for readability\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Feature_mode', default='Customized_feature',\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--preprocess', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--start_point', default=-1,\n",
    "                        help='In case that the program stop at certain point, we can resume the progress by setting this variable')\n",
    "    parser.add_argument('--experiment', default='gop_exp_ADOShappyDAAIKidallDeceiptformosaCSRC',\n",
    "                        help='If the mode is set to Session_phone_phf, you may need to determine the experiment used to generate the gop feature')\n",
    "    parser.add_argument('--pseudo', default=False,\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--suffix', default=\"\",\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--FS_method_str', default=None,\n",
    "                        help='Feature selection')\n",
    "    parser.add_argument('--Print_Analysis_grp_Manual_select', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--Plot', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--Reverse_exp', default=False, dest=\"Reverse_exp\",\n",
    "                        help='')\n",
    "    parser.add_argument('--selectModelScoring', default='recall_macro',\n",
    "                        help='[recall_macro,accuracy]')\n",
    "    parser.add_argument('--Mergefeatures', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--logit_number', default=0,\n",
    "                        help='現在都改用decision function了，所以指會有一個loigit')\n",
    "    parser.add_argument('--decision_boundary', default=0,\n",
    "                        help='現在都改用decision function了，decision_boundary = 0')\n",
    "    parser.add_argument('--knn_weights', default='uniform',\n",
    "                            help='path of the base directory')\n",
    "    parser.add_argument('--knn_neighbors', default=2,  type=int,\n",
    "                            help='path of the base directory')\n",
    "    parser.add_argument('--Reorder_type', default='DKIndividual',\n",
    "                            help='[DKIndividual, DKcriteria]')\n",
    "    parser.add_argument('--Normalize_way', default='func15',\n",
    "                            help='func1 func2 func3 func4 func7 proposed')\n",
    "    parser.add_argument('--FeatureComb_mode', default='Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation',\n",
    "                            help='[Add_UttLvl_feature, feat_comb3, feat_comb5, feat_comb6,feat_comb7, baselineFeats,Comb_dynPhonation,Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation]')\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "args = get_args()\n",
    "start_point=args.start_point\n",
    "experiment=args.experiment\n",
    "knn_weights=args.knn_weights\n",
    "knn_neighbors=args.knn_neighbors\n",
    "Reorder_type=args.Reorder_type\n",
    "logit_number=args.logit_number\n",
    "Reverse_exp=args.Reverse_exp\n",
    "decision_boundary=args.decision_boundary\n",
    "\n",
    "\n",
    "Session_level_all=Dict()\n",
    "# Discriminative analysis Main\n",
    "columns=[\n",
    "    'Divergence[within_covariance_norm(A:,i:,u:)]',\n",
    "    'Divergence[within_variance_norm(A:,i:,u:)]',    \n",
    "    'Divergence[between_covariance_norm(A:,i:,u:)]',    \n",
    "    'Divergence[between_variance_norm(A:,i:,u:)]',    \n",
    "    'Divergence[total_covariance_norm(A:,i:,u:)]', \n",
    "    'Divergence[total_variance_norm(A:,i:,u:)]',\n",
    "    'Divergence[sam_wilks_lin_norm(A:,i:,u:)]',    \n",
    "    'Divergence[pillai_lin_norm(A:,i:,u:)]',\n",
    "    'Divergence[roys_root_lin_norm(A:,i:,u:)]',\n",
    "    'Divergence[hotelling_lin_norm(A:,i:,u:)]',\n",
    "    'Divergence[Between_Within_Det_ratio_norm(A:,i:,u:)]',\n",
    "    'Divergence[Between_Within_Tr_ratio_norm(A:,i:,u:)]',\n",
    "    'Divergence[within_covariance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[within_variance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[between_covariance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[between_variance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[total_covariance_norm(A:,i:,u:)]_var_p1', \n",
    "    'Divergence[total_variance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[sam_wilks_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[pillai_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[roys_root_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[hotelling_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[Between_Within_Det_ratio_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[Between_Within_Tr_ratio_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[within_covariance_norm(A:,i:,u:)]_var_p2',    \n",
    "    'Divergence[within_variance_norm(A:,i:,u:)]_var_p2',    \n",
    "    'Divergence[between_covariance_norm(A:,i:,u:)]_var_p2',    \n",
    "    'Divergence[between_variance_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[total_covariance_norm(A:,i:,u:)]_var_p2', \n",
    "    'Divergence[total_variance_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[sam_wilks_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[pillai_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[roys_root_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[hotelling_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[Between_Within_Det_ratio_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[Between_Within_Tr_ratio_norm(A:,i:,u:)]_var_p2',\n",
    "    ]\n",
    "\n",
    "# columns=[\n",
    "#     'VSA2',\n",
    "#     'FCR2',\n",
    "#     'within_covariance_norm(A:,i:,u:)',\n",
    "#     'between_covariance_norm(A:,i:,u:)',\n",
    "#     'within_variance_norm(A:,i:,u:)',\n",
    "#     'between_variance_norm(A:,i:,u:)',\n",
    "#     'total_covariance_norm(A:,i:,u:)',\n",
    "#     'total_variance_norm(A:,i:,u:)',\n",
    "#     'sam_wilks_lin_norm(A:,i:,u:)',\n",
    "#     'pillai_lin_norm(A:,i:,u:)',\n",
    "#     'Between_Within_Det_ratio_norm(A:,i:,u:)',\n",
    "#     'Between_Within_Tr_ratio_norm(A:,i:,u:)',\n",
    "# ]\n",
    "# columns=[\n",
    "# 'Norm(WC)_sam_wilks_DKRaito', \n",
    "# 'Norm(WC)_pillai_DKRaito',\n",
    "# 'Norm(WC)_hotelling_DKRaito', 'Norm(WC)_roys_root_DKRaito',\n",
    "# 'Norm(WC)_Det_DKRaito', 'Norm(WC)_Tr_DKRaito',\n",
    "# 'Norm(BC)_sam_wilks_DKRaito', 'Norm(BC)_pillai_DKRaito',\n",
    "# 'Norm(BC)_hotelling_DKRaito', 'Norm(BC)_roys_root_DKRaito',\n",
    "# 'Norm(BC)_Det_DKRaito', 'Norm(BC)_Tr_DKRaito',\n",
    "# 'Norm(TotalVar)_sam_wilks_DKRaito', 'Norm(TotalVar)_pillai_DKRaito',\n",
    "# 'Norm(TotalVar)_hotelling_DKRaito', 'Norm(TotalVar)_roys_root_DKRaito',\n",
    "# 'Norm(TotalVar)_Det_DKRaito', 'Norm(TotalVar)_Tr_DKRaito',\n",
    "# ]\n",
    "# Discriminative analysis: Side exp\n",
    "# columns=[\n",
    "# 'VSA1',\n",
    "# 'FCR',\n",
    "# 'within_covariance(A:,i:,u:)',\n",
    "# 'between_covariance(A:,i:,u:)',\n",
    "# 'within_variance(A:,i:,u:)',\n",
    "# 'between_variance(A:,i:,u:)',\n",
    "# 'sam_wilks_lin(A:,i:,u:)',\n",
    "# 'pillai_lin(A:,i:,u:)',\n",
    "# ]\n",
    "\n",
    "\n",
    "featuresOfInterest_manual=[ [col] for col in columns]\n",
    "# featuresOfInterest_manual=[ [col] + ['u_num+i_num+a_num'] for col in columns]\n",
    "\n",
    "\n",
    "# label_choose=['ADOS_C','Multi1','Multi2','Multi3','Multi4']\n",
    "label_choose=['ADOS_C']\n",
    "# label_choose=['ADOS_cate','ASDTD']\n",
    "\n",
    "\n",
    "def MERGEFEATURES():\n",
    "    # =============================================================================\n",
    "    '''\n",
    "\n",
    "        Feature merging function\n",
    "        \n",
    "        Ths slice of code provide user to manually make functions to combine df_XXX_infos\n",
    "\n",
    "    '''\n",
    "    # =============================================================================\n",
    "    # dataset_role='ASD_DOCKID'\n",
    "    for dataset_role in ['ASD_DOCKID','TD_DOCKID']:\n",
    "        Merg_filepath={}\n",
    "        Merg_filepath['static_feautre_LOC']='Features/artuculation_AUI/Vowels/Formants/{Normalize_way}/Formant_AUI_tVSAFCRFvals_KID_From{dataset_role}.pkl'.format(dataset_role=dataset_role,Normalize_way=args.Normalize_way)\n",
    "        # Merg_filepath['static_feautre_phonation']='Features/artuculation_AUI/Vowels/Phonation/Phonation_meanvars_KID_From{dataset_role}.pkl'.format(dataset_role=dataset_role)\n",
    "        Merg_filepath['dynamic_feature_LOC']='Features/artuculation_AUI/Interaction/Formants/{Normalize_way}/Syncrony_measure_of_variance_DKIndividual_{dataset_role}.pkl'.format(dataset_role=dataset_role,Normalize_way=args.Normalize_way)\n",
    "        Merg_filepath['dynamic_feature_phonation']='Features/artuculation_AUI/Interaction/Phonation/Syncrony_measure_of_variance_phonation_{dataset_role}.pkl'.format(dataset_role=dataset_role)\n",
    "        \n",
    "        merge_out_path='Features/ClassificationMerged_dfs/{Normalize_way}/{dataset_role}/'.format(\n",
    "            knn_weights=knn_weights,\n",
    "            knn_neighbors=knn_neighbors,\n",
    "            Reorder_type=Reorder_type,\n",
    "            dataset_role=dataset_role,\n",
    "            Normalize_way=args.Normalize_way\n",
    "            )\n",
    "        if not os.path.exists(merge_out_path):\n",
    "            os.makedirs(merge_out_path)\n",
    "        \n",
    "        df_infos_dict=Dict()\n",
    "        for keys, paths in Merg_filepath.items():\n",
    "            df_infos_dict[keys]=pickle.load(open(paths,\"rb\")).sort_index()\n",
    "        \n",
    "        Merged_df_dict=Dict()\n",
    "        comb1 = list(combinations(list(Merg_filepath.keys()), 1))\n",
    "        comb2 = list(combinations(list(Merg_filepath.keys()), 2))\n",
    "        for c in comb1:\n",
    "            e1=c[0]\n",
    "            Merged_df_dict[e1]=df_infos_dict[e1]\n",
    "            OutPklpath=merge_out_path+ e1 + \".pkl\"\n",
    "            pickle.dump(Merged_df_dict[e1],open(OutPklpath,\"wb\"))\n",
    "            \n",
    "            \n",
    "        for c in comb2:\n",
    "            e1, e2=c\n",
    "            Merged_df_dict['+'.join(c)]=Merge_dfs(df_infos_dict[e1],df_infos_dict[e2])\n",
    "            \n",
    "            OutPklpath=merge_out_path+'+'.join(c)+\".pkl\"\n",
    "            pickle.dump(Merged_df_dict['+'.join(c)],open(OutPklpath,\"wb\"))\n",
    "        # Condition for : Columns_comb3 = All possible LOC feature combination + phonation_proximity_col\n",
    "        c = ('static_feautre_LOC', 'dynamic_feature_LOC', 'dynamic_feature_phonation')\n",
    "        e1, e2, e3=c\n",
    "        \n",
    "        Merged_df_dict['+'.join(c)]=Merge_dfs(df_infos_dict[e1],df_infos_dict[e2])\n",
    "        Merged_df_dict['+'.join(c)]=Merge_dfs(Merged_df_dict['+'.join(c)],df_infos_dict[e3])\n",
    "        # Merged_df_dict['+'.join(c)]=Merge_dfs(Merged_df_dict['+'.join(c)],Utt_featuresCombinded_dict[role])\n",
    "        OutPklpath=merge_out_path+'+'.join(c)+\".pkl\"\n",
    "        pickle.dump(Merged_df_dict['+'.join(c)],open(OutPklpath,\"wb\"))\n",
    "if args.Mergefeatures:\n",
    "    MERGEFEATURES()\n",
    "\n",
    "df_formant_statistics_CtxPhone_collect_dict=Dict()\n",
    "# =============================================================================\n",
    "\n",
    "class ADOSdataset():\n",
    "    def __init__(self,knn_weights,knn_neighbors,Reorder_type,FeatureComb_mode):\n",
    "        self.featurepath='Features'            \n",
    "        self.N=2\n",
    "        self.LabelType=Dict()\n",
    "        self.LabelType['ADOS_C']='regression'\n",
    "        self.LabelType['ADOS_cate_C']='classification'\n",
    "        self.LabelType['ASDTD']='classification'\n",
    "        self.Fractionfeatures_str='Features/artuculation_AUI/Vowels/Fraction/*.pkl'    \n",
    "        self.FeatureComb_mode=FeatureComb_mode\n",
    "        if self.FeatureComb_mode == 'Add_UttLvl_feature':\n",
    "            self.File_root_path='Features/ClassificationMerged_dfs/{Normalize_way}/ADDed_UttFeat/{knn_weights}_{knn_neighbors}_{Reorder_type}/'.format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type,Normalize_way=args.Normalize_way)\n",
    "            self.Merge_feature_path=self.File_root_path+'{dataset_role}/*.pkl'.format(dataset_role='ASD_DOCKID')\n",
    "        else:\n",
    "            self.File_root_path='Features/ClassificationMerged_dfs/{Normalize_way}/'.format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type,Normalize_way=args.Normalize_way)\n",
    "            self.Merge_feature_path=self.File_root_path+'{dataset_role}/*.pkl'.format(dataset_role='ASD_DOCKID')\n",
    "        \n",
    "        self.Top_ModuleColumn_mapping_dict={}\n",
    "        self.Top_ModuleColumn_mapping_dict['Add_UttLvl_feature']=FeatSel.Columns_comb2.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb']=FeatSel.Columns_comb.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb3']=FeatSel.Columns_comb3.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb5']=FeatSel.Columns_comb5.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb6']=FeatSel.Columns_comb6.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb7']=FeatSel.Columns_comb7.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['Comb_dynPhonation']=FeatSel.Comb_dynPhonation.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']=FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation.copy()\n",
    "        \n",
    "        self.Top_ModuleColumn_mapping_dict['baselineFeats']=FeatSel.Baseline_comb.copy()\n",
    "        \n",
    "        self.FeatureCombs_manual=Dict()\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatLOC_kid']=['df_formant_statistic_TD_normal_kid', 'df_formant_statistic_agesexmatch_ASDSevereGrp_kid']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatLOC_kid']=['df_formant_statistic_TD_normal_kid', 'df_formant_statistic_agesexmatch_ASDMildGrp_kid']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatLOC_doc']=['df_formant_statistic_TD_normal_doc', 'df_formant_statistic_agesexmatch_ASDSevereGrp_doc']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatLOC_doc']=['df_formant_statistic_TD_normal_doc', 'df_formant_statistic_agesexmatch_ASDMildGrp_doc']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatLOC_DKRatio']=['df_formant_statistic_TD_normalGrp_DKRatio', 'df_formant_statistic_agesexmatch_ASDSevereGrp_DKRatio']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatLOC_DKRatio']=['df_formant_statistic_TD_normalGrp_DKRatio', 'df_formant_statistic_agesexmatch_ASDMildGrp_DKRatio']\n",
    "        \n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatCoor']=['df_syncrony_statistic_TD_normalGrp', 'df_syncrony_statistic_agesexmatch_ASDSevereGrp']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatCoor']=['df_syncrony_statistic_TD_normalGrp', 'df_syncrony_statistic_agesexmatch_ASDMildGrp']\n",
    "        # self.FeatureCombs_manual['Notautism vs ASD']=['df_formant_statistic_77_Notautism', 'df_formant_statistic_77_ASD']\n",
    "        # self.FeatureCombs_manual['ASD vs Autism']=['df_formant_statistic_77_ASD', 'df_formant_statistic_77_Autism']\n",
    "        # self.FeatureCombs_manual['Notautism vs Autism']=['df_formant_statistic_77_Notautism', 'df_formant_statistic_77_Autism']\n",
    "    \n",
    "        # self._FeatureBuild_single()\n",
    "        self._FeatureBuild_Module()\n",
    "    def Get_FormantAUI_feat(self,label_choose,pickle_path,featuresOfInterest=['MSB_f1','MSB_f2','MSB_mix'],filterbyNum=True,**kwargs):\n",
    "        arti=articulation.articulation.Articulation()\n",
    "        #如果path有放的話字串的話，就使用path的字串，不然就使用「feat_」等於的東西，在function裡面會以kwargs的形式出現\n",
    "        if not kwargs and len(pickle_path)>0:\n",
    "            df_tmp=pickle.load(open(pickle_path,\"rb\")).sort_index()\n",
    "        elif len(kwargs)>0: # usage Get_FormantAUI_feat(...,key1=values1):\n",
    "            for k, v in kwargs.items(): #there will be only one element\n",
    "                df_tmp=kwargs[k].sort_index()\n",
    "\n",
    "        if filterbyNum:\n",
    "            df_tmp=arti.BasicFilter_byNum(df_tmp,N=self.N)\n",
    "\n",
    "        if label_choose not in df_tmp.columns:\n",
    "            for people in df_tmp.index:\n",
    "                lab=Label.label_raw[label_choose][Label.label_raw['name']==people]\n",
    "                df_tmp.loc[people,'ADOS']=lab.values\n",
    "            df_y=df_tmp['ADOS'] #Still keep the form of dataframe\n",
    "        else:\n",
    "            df_y=df_tmp[label_choose] #Still keep the form of dataframe\n",
    "        \n",
    "        \n",
    "        feature_array=df_tmp[featuresOfInterest]\n",
    "        \n",
    "            \n",
    "        LabType=self.LabelType[label_choose]\n",
    "        return feature_array, df_y, LabType\n",
    "    def _FeatureBuild_single(self):\n",
    "        Features=Dict()\n",
    "        Features_comb=Dict()\n",
    "        files = glob.glob(self.Fractionfeatures_str)\n",
    "        for file in files:\n",
    "            feat_name=os.path.basename(file).replace(\".pkl\",\"\")\n",
    "            df_tmp=pickle.load(open(file,\"rb\")).sort_index()\n",
    "            Features[feat_name]=df_tmp\n",
    "        for keys in self.FeatureCombs_manual.keys():\n",
    "            combF=[Features[k] for k in self.FeatureCombs_manual[keys]]\n",
    "            Features_comb[keys]=pd.concat(combF)\n",
    "        \n",
    "        self.Features_comb_single=Features_comb\n",
    "    def _FeatureBuild_Module(self):\n",
    "        Labels_add=['ASDTD']\n",
    "        ModuledFeatureCombination=self.Top_ModuleColumn_mapping_dict[self.FeatureComb_mode]\n",
    "        \n",
    "        sellect_people_define=SellectP_define()\n",
    "        #Loading features from ASD        \n",
    "        Features_comb=Dict()\n",
    "        IterateFilesFullPaths = glob.glob(self.Merge_feature_path)\n",
    "        \n",
    "\n",
    "        if self.FeatureComb_mode in ['feat_comb3','feat_comb5','feat_comb6','feat_comb7','Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']:\n",
    "            DfCombFilenames=['static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation.pkl']\n",
    "        elif self.FeatureComb_mode == 'Comb_dynPhonation':\n",
    "            DfCombFilenames=['dynamic_feature_phonation.pkl']\n",
    "        elif self.FeatureComb_mode == 'baselineFeats':\n",
    "             DfCombFilenames=['{}.pkl'.format(Dataname) for Dataname in ModuledFeatureCombination.keys()]\n",
    "        else:\n",
    "            DfCombFilenames=[os.path.basename(f) for f in IterateFilesFullPaths]\n",
    "        File_ASD_paths=[self.File_root_path+\"ASD_DOCKID/\"+f for f in DfCombFilenames]\n",
    "        File_TD_paths=[self.File_root_path+\"TD_DOCKID/\"+f for f in DfCombFilenames]\n",
    "        \n",
    "        \n",
    "        df_Top_Check_length=pd.DataFrame()\n",
    "        for file_ASD, file_TD in zip(File_ASD_paths,File_TD_paths):\n",
    "            if not os.path.exists(file_ASD) or not os.path.exists(file_TD):\n",
    "                raise FileExistsError()\n",
    "            \n",
    "            assert os.path.basename(file_ASD) == os.path.basename(file_TD)\n",
    "            filename=os.path.basename(file_ASD)\n",
    "            k_FeatTypeLayer1=filename.replace(\".pkl\",\"\")\n",
    "            df_feature_ASD=pickle.load(open(file_ASD,\"rb\")).sort_index()\n",
    "            df_feature_TD=pickle.load(open(file_TD,\"rb\")).sort_index()\n",
    "            df_feature_ASD['ASDTD']=sellect_people_define.ASDTD_label['ASD']\n",
    "            df_feature_TD['ASDTD']=sellect_people_define.ASDTD_label['TD']\n",
    "            \n",
    "            # ADD label\n",
    "            df_feature_ASD=Add_label(df_feature_ASD,Label,label_choose='ADOS_cate_CSS')\n",
    "            df_feature_ASD=Add_label(df_feature_ASD,Label,label_choose='age_year')\n",
    "            df_feature_ASD=Add_label(df_feature_ASD,Label,label_choose='sex')\n",
    "            # create different ASD cohort\n",
    "            filter_Minimal_TCSS=df_feature_ASD['ADOS_cate_CSS']==0\n",
    "            filter_low_TCSS=df_feature_ASD['ADOS_cate_CSS']==1\n",
    "            filter_moderate_TCSS=df_feature_ASD['ADOS_cate_CSS']==2\n",
    "            filter_high_TCSS=df_feature_ASD['ADOS_cate_CSS']==3\n",
    "            \n",
    "            df_feauture_ASDgrp_dict={}\n",
    "            df_feauture_ASDgrp_dict['df_feature_ASD']=df_feature_ASD\n",
    "            \n",
    "            # df_feauture_ASDgrp_dict['df_feature_Minimal_CSS']=df_feature_ASD[filter_Minimal_TCSS]\n",
    "            # df_feauture_ASDgrp_dict['df_feature_low_CSS']=df_feature_ASD[filter_low_TCSS]\n",
    "            # df_feauture_ASDgrp_dict['df_feature_moderate_CSS']=df_feature_ASD[filter_moderate_TCSS]\n",
    "            # df_feauture_ASDgrp_dict['df_feature_high_CSS']=df_feature_ASD[filter_high_TCSS]\n",
    "            df_feauture_ASDgrp_dict['df_feature_lowMinimal_CSS']=df_feature_ASD[filter_low_TCSS | filter_Minimal_TCSS]\n",
    "            df_feauture_ASDgrp_dict['df_feature_moderate_CSS']=df_feature_ASD[filter_moderate_TCSS]\n",
    "            df_feauture_ASDgrp_dict['df_feature_high_CSS']=df_feature_ASD[filter_high_TCSS]\n",
    "\n",
    "            #Check the length of each paired comparison, should be stored on the top of for loop\n",
    "            Tmp_Numcmp_dict={}\n",
    "            for key in df_feauture_ASDgrp_dict.keys():\n",
    "                Numcmp_str='ASD({0}) vs TD({1})'.format(len(df_feauture_ASDgrp_dict[key]),len(df_feature_TD))\n",
    "                Tmp_Numcmp_dict[key]=Numcmp_str\n",
    "                \n",
    "            \n",
    "            df_Tmp_Numcmp_list=pd.DataFrame.from_dict(Tmp_Numcmp_dict,orient='index')\n",
    "            df_Tmp_Numcmp_list.columns=[k_FeatTypeLayer1]\n",
    "\n",
    "            if len(df_Top_Check_length)==0:\n",
    "                df_Top_Check_length=df_Tmp_Numcmp_list\n",
    "            else:\n",
    "                df_Top_Check_length=Merge_dfs(df_Top_Check_length,df_Tmp_Numcmp_list)\n",
    "            # 手動執行到這邊，從for 上面\n",
    "            for k_FeatTypeLayer2 in ModuledFeatureCombination[k_FeatTypeLayer1].keys():\n",
    "                colums_sel=ModuledFeatureCombination[k_FeatTypeLayer1][k_FeatTypeLayer2]\n",
    "                \n",
    "\n",
    "                # 1. Set ASD vs TD experiment\n",
    "                for k_ASDgrp in df_feauture_ASDgrp_dict.keys():\n",
    "                    df_ASD_subgrp=df_feauture_ASDgrp_dict[k_ASDgrp].copy()[colums_sel+Labels_add]\n",
    "                    df_TD_subgrp=df_feature_TD.copy()[colums_sel+Labels_add]\n",
    "                    \n",
    "                    experiment_str=\"{TD_name} vs {ASD_name} >> {feature_type}\".format(TD_name='TD',ASD_name=k_ASDgrp,feature_type=k_FeatTypeLayer2)\n",
    "                    Features_comb[experiment_str]=pd.concat([df_ASD_subgrp,df_TD_subgrp],axis=0)\n",
    "                # 2. Set ASDsevere vs ASDmild experiment\n",
    "                # experiment_str=\"{ASDsevere_name} vs {ASDmild_name} >> {feature_type}\".format(ASDsevere_name='df_feature_moderatehigh_CSS',ASDmild_name='df_feature_lowMinimal_CSS',feature_type=k_FeatTypeLayer2)\n",
    "                # df_ASDsevere_subgrp=df_feauture_ASDgrp_dict['df_feature_moderatehigh_CSS'].copy()\n",
    "                # df_ASDmild_subgrp=df_feauture_ASDgrp_dict['df_feature_lowMinimal_CSS'].copy()\n",
    "                # df_ASDsevere_subgrp['ASDsevereMild']=sellect_people_define.ASDsevereMild_label['ASDsevere']\n",
    "                # df_ASDmild_subgrp['ASDsevereMild']=sellect_people_define.ASDsevereMild_label['ASDmild']\n",
    "                # Features_comb[experiment_str]=pd.concat([df_ASDsevere_subgrp,df_ASDmild_subgrp],axis=0)\n",
    "        self.Features_comb_multi=Features_comb\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Feature merging function\n",
    "    \n",
    "    Ths slice of code provide user to manually make functions to combine df_XXX_infos\n",
    "\n",
    "'''\n",
    "# =============================================================================\n",
    "\n",
    "ados_ds=ADOSdataset(knn_weights,knn_neighbors,Reorder_type,FeatureComb_mode=args.FeatureComb_mode)\n",
    "ErrorFeat_bookeep=Dict()\n",
    "\n",
    "\n",
    "FeatureLabelMatch_manual=[\n",
    "\n",
    "    ['TD vs df_feature_lowMinimal_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Syncrony_cols+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "    ['TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "    ['TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols', 'ASDTD'],\n",
    "    \n",
    "\n",
    "    ['TD vs df_feature_moderate_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Trend_K_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "    ['TD vs df_feature_moderate_CSS >> Phonation_Proximity_cols', 'ASDTD'],\n",
    "    # ['TD vs df_feature_moderate_CSS >> Phonation_Trend_D_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "\n",
    "    ['TD vs df_feature_high_CSS >> DEP_columns+Phonation_Proximity_cols', 'ASDTD'],\n",
    "    # ['TD vs df_feature_high_CSS >> Phonation_Proximity_cols+Phonation_Syncrony_cols', 'ASDTD'],\n",
    "    ['TD vs df_feature_high_CSS >> Phonation_Proximity_cols', 'ASDTD'],\n",
    " ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FeatSel 掌管該出現的columns\n",
    "# ados_ds.Features_comb_multi 掌管load進來的data\n",
    "\n",
    "\n",
    "Top_ModuleColumn_mapping_dict={}\n",
    "Top_ModuleColumn_mapping_dict['Add_UttLvl_feature']={ e2_str:FeatSel.Columns_comb2[e_str][e2_str] for e_str in FeatSel.Columns_comb2.keys() for e2_str in FeatSel.Columns_comb2[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb3']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb3[e_str][e2_str] for e_str in FeatSel.Columns_comb3.keys() for e2_str in FeatSel.Columns_comb3[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb5']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb5[e_str][e2_str] for e_str in FeatSel.Columns_comb5.keys() for e2_str in FeatSel.Columns_comb5[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb6']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb6[e_str][e2_str] for e_str in FeatSel.Columns_comb6.keys() for e2_str in FeatSel.Columns_comb6[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb7']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb7[e_str][e2_str] for e_str in FeatSel.Columns_comb7.keys() for e2_str in FeatSel.Columns_comb7[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['Comb_dynPhonation']=ModuleColumn_mapping={ e2_str:FeatSel.Comb_dynPhonation[e_str][e2_str] for e_str in FeatSel.Comb_dynPhonation.keys() for e2_str in FeatSel.Comb_dynPhonation[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']=ModuleColumn_mapping={ e2_str:FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation[e_str][e2_str] for e_str in FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation.keys() for e2_str in FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb[e_str][e2_str] for e_str in FeatSel.Columns_comb.keys() for e2_str in FeatSel.Columns_comb[e_str].keys()}\n",
    "\n",
    "\n",
    "ModuleColumn_mapping=Top_ModuleColumn_mapping_dict[args.FeatureComb_mode]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Here starts to load features to Session_level_all dict\n",
    "    \n",
    "'''\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "for exp_str, lab_ in FeatureLabelMatch_manual:\n",
    "    comparison_pair=exp_str.split(\" >> \")[0]\n",
    "    ModuleColumn_str=exp_str.split(\" >> \")[-1]\n",
    "    featuresOfInterest=[ModuleColumn_mapping[ModuleColumn_str]]\n",
    "    # feat_=key\n",
    "    for feat_col in featuresOfInterest:\n",
    "        feat_col_ = list(feat_col) # ex: ['MSB_f1']\n",
    "        if len(feat_col) > 144: # 144 is the limit of the filename\n",
    "            key=feat_col_\n",
    "        else:\n",
    "            key=[feat_col_]\n",
    "        # X,y, featType=ados_ds.Get_FormantAUI_feat(\\\n",
    "        #     label_choose=lab_,pickle_path='',featuresOfInterest=feat_col_,filterbyNum=False,\\\n",
    "        #     feat_=ados_ds.Features_comb_single[feat_])\n",
    "        \n",
    "        X,y, featType=ados_ds.Get_FormantAUI_feat(\\\n",
    "            label_choose=lab_,pickle_path='',featuresOfInterest=feat_col,filterbyNum=False,\\\n",
    "            feat_=ados_ds.Features_comb_multi[exp_str])\n",
    "        \n",
    "        if X.isnull().values.any() or y.isnull().values.any():\n",
    "            print(\"Feat: \",key,'Contains nan')\n",
    "            ErrorFeat_bookeep['{0} {1} {2}'.format(exp_str,lab_,key)].X=X\n",
    "            ErrorFeat_bookeep['{0} {1} {2}'.format(exp_str,lab_,key)].y=y\n",
    "            continue\n",
    "        \n",
    "        Item_name=\"{feat}::{lab}\".format(feat=' >> '.join([comparison_pair,ModuleColumn_str]),lab=lab_)\n",
    "        Session_level_all[Item_name].X, \\\n",
    "            Session_level_all[Item_name].y, \\\n",
    "                Session_level_all[Item_name].feattype = X,y, featType\n",
    "\n",
    "# =============================================================================\n",
    "# Model parameters\n",
    "# =============================================================================\n",
    "# C_variable=np.array([0.0001, 0.01, 0.1,0.5,1.0,10.0, 50.0, 100.0, 1000.0])\n",
    "# C_variable=np.array([0.01, 0.1,0.5,1.0,10.0, 50.0, 100.0])\n",
    "# C_variable=np.array(np.arange(0.1,1.5,0.1))\n",
    "C_variable=np.array([0.001,0.01,0.1,1,5,10.0,25,50,75,100])\n",
    "# C_variable=np.array([0.001,0.01,10.0,50,100] + list(np.arange(0.1,1.5,0.2))  )\n",
    "# C_variable=np.array([0.01, 0.1,0.5,1.0, 5.0])\n",
    "n_estimator=[ 32, 50, 64, 100 ,128, 256]\n",
    "\n",
    "'''\n",
    "\n",
    "    Classifier\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "# This is the closest \n",
    "Classifier={}\n",
    "Classifier['SVC']={'model':sklearn.svm.SVC(),\\\n",
    "                  'parameters':{'model__random_state':[1],\\\n",
    "                      'model__C':C_variable,\\\n",
    "                    'model__kernel': ['rbf'],\\\n",
    "                      # 'model__gamma':['auto'],\\\n",
    "                    'model__probability':[True],\\\n",
    "                                }}\n",
    "\n",
    "   \n",
    "\n",
    "loo=LeaveOneOut()\n",
    "# CV_settings=loo\n",
    "CV_settings=10\n",
    "\n",
    "# =============================================================================\n",
    "# Outputs\n",
    "Best_predict_optimize={}\n",
    "\n",
    "df_best_result_r2=pd.DataFrame([])\n",
    "df_best_result_pear=pd.DataFrame([])\n",
    "df_best_result_spear=pd.DataFrame([])\n",
    "df_best_cross_score=pd.DataFrame([])\n",
    "df_best_result_UAR=pd.DataFrame([])\n",
    "df_best_result_AUC=pd.DataFrame([])\n",
    "df_best_result_f1=pd.DataFrame([])\n",
    "df_best_result_allThreeClassifiers=pd.DataFrame([])\n",
    "# =============================================================================\n",
    "Result_path=\"RESULTS/\"\n",
    "if not os.path.exists(Result_path):\n",
    "    os.makedirs(Result_path)\n",
    "final_result_file=\"_ADOS_{}.xlsx\".format(args.suffix)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "count=0\n",
    "OutFeature_dict=Dict()\n",
    "Best_param_dict=Dict()\n",
    "sellect_people_define=SellectP_define()\n",
    "\n",
    "# ''' 要手動執行一次從Incorrect2Correct_indexes和Correct2Incorrect_indexes決定哪些indexes 需要算shap value 再在這邊指定哪些fold需要停下來算SHAP value '''\n",
    "SHAP_inspect_idxs_manual=None # None means calculate SHAP value of all people\n",
    "# SHAP_inspect_idxs_manual=[4, 6, 15] # None means calculate SHAP value of all people\n",
    "# SHAP_inspect_idxs_manual=[] # empty list means we do not execute shap function\n",
    "# SHAP_inspect_idxs_manual=sorted(list(set([14, 21]+[]+[24, 28, 30, 31, 39, 41, 45]+[22, 23, 27, 47, 58]+[6, 13, 19, 23, 24, 25]+[28, 35, 38, 45])))\n",
    "\n",
    "for clf_keys, clf in Classifier.items(): #Iterate among different classifiers \n",
    "    writer_clf = pd.ExcelWriter(Result_path+\"/\"+clf_keys+\"_\"+args.Feature_mode+\"_\"+final_result_file, engine = 'xlsxwriter')\n",
    "    for feature_lab_str, features in Session_level_all.items():\n",
    "\n",
    "        feature_keys, label_keys= feature_lab_str.split(\"::\")\n",
    "        feature_rawname=feature_keys[feature_keys.find('-')+1:]\n",
    "        # if feature_rawname in paper_name_map.keys():\n",
    "        #     featurename_paper=paper_name_map[feature_rawname]\n",
    "        #     feature_keys=feature_keys.replace(feature_rawname,featurename_paper)\n",
    "        \n",
    "        if SHAP_inspect_idxs_manual != None:\n",
    "            SHAP_inspect_idxs=SHAP_inspect_idxs_manual\n",
    "        else:\n",
    "            SHAP_inspect_idxs=range(len(features.y))\n",
    "        \n",
    "        Labels = Session_level_all.X[feature_keys]\n",
    "        print(\"=====================Cross validation start==================\")\n",
    "        pipe = Pipeline(steps=[('scalar',StandardScaler()),(\"model\", clf['model'])])\n",
    "        p_grid=clf['parameters']\n",
    "        Gclf = GridSearchCV(estimator=pipe, param_grid=p_grid, scoring=args.selectModelScoring, cv=CV_settings, refit=True, n_jobs=-1)\n",
    "        Gclf_manual = GridSearchCV(estimator=pipe, param_grid=p_grid, scoring=args.selectModelScoring, cv=CV_settings, refit=True, n_jobs=-1)\n",
    "        \n",
    "        \n",
    "        # The cv is as the one in cross_val_predict function\n",
    "        cv = sklearn.model_selection.check_cv(CV_settings,features.y,classifier=sklearn.base.is_classifier(Gclf))\n",
    "        splits = list(cv.split(features.X, features.y, groups=None))\n",
    "        test_indices = np.concatenate([test for _, test in splits])\n",
    "\n",
    "        CVpredict_manual=np.zeros(len(features.y))\n",
    "        for i, (train_index, test_index) in enumerate(splits):\n",
    "            X_train, X_test = features.X.iloc[train_index], features.X.iloc[test_index]\n",
    "            y_train, y_test = features.y.iloc[train_index], features.y.iloc[test_index]\n",
    "            Gclf_manual.fit(X_train,y_train)\n",
    "            Gclf_manual_predict=Gclf_manual.predict(X_test)\n",
    "            ##################################################################\n",
    "            #確認predict proba會跟prediction有match到\n",
    "            #原本的SVM的predict proba (https://github.com/scikit-learn/scikit-learn/issues/13211)\n",
    "            # the normal classifiers' predictions are based on decision_function values\n",
    "            ##################################################################\n",
    "            calibrated_bestEst=CalibratedClassifierCV(\n",
    "                estimator=Gclf_manual.best_estimator_,\n",
    "                cv=\"prefit\"\n",
    "                )\n",
    "            calibrated_bestEst.fit(X_train,y_train)\n",
    "            result_bestmodel=calibrated_bestEst.predict(X_test)\n",
    "            ##################################################################\n",
    "\n",
    "            decisFunc=Gclf_manual.decision_function(X_test)\n",
    "            # predict_proba 是把SVM的output另外fit一個ligistic regression 來smooth他的output到0~1之間\n",
    "            # https://splunktool.com/why-is-the-result-of-sklearnsvmsvcpredict-inconsistent-with-sklearnsvmsvcpredictproba\n",
    "            \n",
    "            # ASSERT based on predict_proba\n",
    "            # TDPred_score_logit0=calibrated_bestEst.predict_proba(X_test)[:,0] \n",
    "            # TDPred_score_logit1=calibrated_bestEst.predict_proba(X_test)[:,1]\n",
    "            # Logit01=np.vstack([TDPred_score_logit0,TDPred_score_logit1])\n",
    "            # TDPredtion_Fromscore=np.argmax(Logit01,axis=0)\n",
    "            # assert (result_bestmodel-1 == TDPredtion_Fromscore).all()\n",
    "            \n",
    "            \n",
    "            # 測試calibrated predict有沒有等於原本的predict\n",
    "            # assert (Gclf_manual_predict == result_bestmodel).all()\n",
    "            \n",
    "            # ASSERT based on decision function\n",
    "            # decision function的unit test\n",
    "            decisFunc_pred=np.ones(len(result_bestmodel)).astype(int)\n",
    "            decisFunc_pred[decisFunc >0]=2\n",
    "            assert (Gclf_manual_predict == decisFunc_pred).all()\n",
    "            # from sklearn.calibration import calibration_curve\n",
    "            # calibration_curve(Gclf_manual.decision_function(X_test))\n",
    "            ##################################################################\n",
    "            CVpredict_manual[test_index]=Gclf_manual_predict\n",
    "            # CVpred_fromFunction=CVpredict[test_index]\n",
    "            \n",
    "            # SHAP value generating\n",
    "            # logit_number=0\n",
    "            # inspect_sample=0\n",
    "            # If the indexes we want to examine are in that fold, store the whole fold\n",
    "            # 先把整個fold記錄下來然後在analysis area再拆解\n",
    "            SHAP_exam_lst=[i for i in test_index if i in SHAP_inspect_idxs]\n",
    "            if len(SHAP_exam_lst) != 0:\n",
    "                explainer = shap.KernelExplainer(Gclf_manual.decision_function, X_train)\n",
    "                # explainer = shap.KernelExplainer(Gclf_manual.predict_proba, X_train)\n",
    "                shap_values = explainer.shap_values(X_test)\n",
    "                \n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].explainer_expected_value=explainer.expected_value\n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].shap_values=shap_values # shap_values= [logit, index, feature]\n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].XTest=X_test\n",
    "                Session_level_all[feature_lab_str]['Logit{}_predictproba'.format(logit_number)]['_'.join(test_index.astype(str))].predictproba=Gclf_manual.predict_proba(X_test)[:,logit_number]\n",
    "                Session_level_all[feature_lab_str]['Logit{}_predictproba'.format(logit_number)]['_'.join(test_index.astype(str))].decisionfunc=Gclf_manual.decision_function(X_test)\n",
    "                # Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index)].testIndex=test_index\n",
    "            # shap.force_plot(explainer.expected_value[logit_number], shap_values[logit_number][inspect_sample,:], X_test.iloc[inspect_sample,:], matplotlib=True,show=False)\n",
    "            \n",
    "            \n",
    "            # assert (result_bestmodel==result_bestmodel_fitted_again).all()\n",
    "            # assert (result_bestmodel==CVpred_fromFunction).all()\n",
    "        # XXX 原本的CVpredict有點怪，他們經過\n",
    "        # CVpredict=cross_val_predict(Gclf, features.X, features.y, cv=CV_settings)   \n",
    "        CVpredict = CVpredict_manual\n",
    "        \n",
    "        # assert (CVpredict_manual==CVpredict).all()\n",
    "        Session_level_all[feature_lab_str]['y_pred']=CVpredict_manual\n",
    "        Session_level_all[feature_lab_str]['y_true']=features.y\n",
    "        \n",
    "        Gclf.fit(features.X,features.y)\n",
    "        if clf_keys == \"EN\":\n",
    "            print('The coefficient of best estimator is: ',Gclf.best_estimator_.coef_)\n",
    "        \n",
    "        print(\"The best score with scoring parameter: 'r2' is\", Gclf.best_score_)\n",
    "        print(\"The best parameters are :\", Gclf.best_params_)\n",
    "        best_parameters=Gclf.best_params_\n",
    "        best_score=Gclf.best_score_\n",
    "        best_parameters.update({'best_score':best_score})\n",
    "        Best_param_dict[feature_lab_str]=best_parameters\n",
    "        cv_results_info=Gclf.cv_results_\n",
    "\n",
    "        num_ASD=len(np.where(features.y==sellect_people_define.ASDTD_label['ASD'])[0])\n",
    "        num_TD=len(np.where(features.y==sellect_people_define.ASDTD_label['TD'])[0])\n",
    "        \n",
    "        if features.feattype == 'regression':\n",
    "            r2=r2_score(features.y,CVpredict )\n",
    "            n,p=features.X.shape\n",
    "            r2_adj=1-(1-r2)*(n-1)/(n-p-1)\n",
    "            pearson_result, pearson_p=pearsonr(features.y,CVpredict )\n",
    "            spear_result, spearman_p=spearmanr(features.y,CVpredict )\n",
    "            print('Feature {0}, label {1} ,spear_result {2}'.format(feature_keys, label_keys,spear_result))\n",
    "        elif features.feattype == 'classification':\n",
    "            n,p=features.X.shape\n",
    "            CM=confusion_matrix(features.y, CVpredict)\n",
    "            Session_level_all[feature_lab_str]['Confusion_matrix']=pd.DataFrame(CM,\\\n",
    "                                                                    index=['y_true_{}'.format(ii) for ii in range(CM.shape[0])],\\\n",
    "                                                                    columns=['y_pred_{}'.format(ii) for ii in range(CM.shape[1])])\n",
    "            UAR=recall_score(features.y, CVpredict, average='macro')\n",
    "            AUC=roc_auc_score(features.y, CVpredict)\n",
    "            f1Score=f1_score(features.y, CVpredict, average='macro')\n",
    "            print('Feature {0}, label {1} ,UAR {2}'.format(feature_keys, label_keys,UAR))\n",
    "            \n",
    "        if args.Plot and p <2:\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 10), sharey=True)\n",
    "            kernel_label = [clf_keys]\n",
    "            model_color = ['m']\n",
    "            # axes.plot((features.X - min(features.X) )/ max(features.X), Gclf.best_estimator_.fit(features.X,features.y).predict(features.X), color=model_color[0],\n",
    "            #               label='CV Predict')\n",
    "            axes.scatter((features.X.values - min(features.X.values) )/ max(features.X.values), CVpredict, \n",
    "                         facecolor=\"none\", edgecolor=\"k\", s=150,\n",
    "                         label='{}'.format(feature_lab_str)\n",
    "                         )\n",
    "            axes.scatter((features.X.values - min(features.X.values) )/ max(features.X.values), features.y, \n",
    "                         facecolor=\"none\", edgecolor=\"r\", s=50,\n",
    "                         label='Real Y')\n",
    "            axes.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
    "                    ncol=1, fancybox=True, shadow=True)\n",
    "            \n",
    "            Plot_path='./Plot/LinearRegress/'\n",
    "            if not os.path.exists(Plot_path):\n",
    "                os.makedirs(Plot_path)\n",
    "            plot_file=Plot_path+\"/{0}_{1}.png\".format(clf_keys,feature_lab_str)\n",
    "            plt.savefig(plot_file, dpi=200) \n",
    "        \n",
    "        # =============================================================================\n",
    "        '''\n",
    "            Inspect the best result\n",
    "        '''\n",
    "        # =============================================================================\n",
    "        Best_predict_optimize[label_keys]=pd.DataFrame(np.vstack((CVpredict,features.y)).T,columns=['y_pred','y'])\n",
    "        excel_path='./Statistics/prediction_result'\n",
    "        if not os.path.exists(excel_path):\n",
    "            os.makedirs(excel_path)\n",
    "        excel_file=excel_path+\"/{0}_{1}.xlsx\"\n",
    "        writer = pd.ExcelWriter(excel_file.format(clf_keys,feature_keys.replace(\":\",\"\")), engine = 'xlsxwriter')\n",
    "        for label_name in  Best_predict_optimize.keys():\n",
    "            Best_predict_optimize[label_name].to_excel(writer,sheet_name=label_name.replace(\"/\",\"_\"))\n",
    "        writer.close()\n",
    "                                \n",
    "        # ================================================      =============================\n",
    "        if features.feattype == 'regression':\n",
    "            df_best_result_r2.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(r2_adj,3),np.round(np.nan,6))\n",
    "            df_best_result_pear.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(pearson_result,3),np.round(pearson_p,6))\n",
    "            df_best_result_spear.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(spear_result,3),np.round(spearman_p,6))\n",
    "            df_best_result_spear.loc[feature_keys,'de-zero_num']=len(features.X)\n",
    "            # df_best_cross_score.loc[feature_keys,label_keys]=Score.mean()\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1} (R2adj/pear/spear)'.format(label_keys,clf_keys)]\\\n",
    "                        ='{0}/{1}/{2}'.format(np.round(r2_adj,3),np.round(pearson_result,3),np.round(spear_result,3))\n",
    "\n",
    "        elif features.feattype == 'classification':\n",
    "            df_best_result_UAR.loc[feature_keys,label_keys]='{0}'.format(UAR)\n",
    "            df_best_result_AUC.loc[feature_keys,label_keys]='{0}'.format(AUC)\n",
    "            df_best_result_f1.loc[feature_keys,label_keys]='{0}'.format(f1Score)\n",
    "            # df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1} (UAR/AUC/f1score)'.format(label_keys,clf_keys)]\\\n",
    "            #             ='{0}/{1}/{2}'.format(np.round(UAR,3),np.round(AUC,3),np.round(f1Score,3))\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1}'.format(label_keys,clf_keys)]\\\n",
    "                        ='{0}'.format(np.round(UAR,3))\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'num_ASD']\\\n",
    "                        ='{0}'.format(num_ASD)\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'num_TD']\\\n",
    "                        ='{0}'.format(num_TD)\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'f1']\\\n",
    "                        ='{0}'.format(np.round(f1Score,3))\n",
    "        count+=1\n",
    "    if features.feattype == 'regression':\n",
    "        df_best_result_r2.to_excel(writer_clf,sheet_name=\"R2_adj\")\n",
    "        df_best_result_pear.to_excel(writer_clf,sheet_name=\"pear\")\n",
    "        df_best_result_spear.to_excel(writer_clf,sheet_name=\"spear\")\n",
    "        df_best_result_spear.to_csv(Result_path+\"/\"+clf_keys+\"_\"+args.Feature_mode+\"_spearman.csv\")\n",
    "    elif features.feattype == 'classification':\n",
    "        df_best_result_UAR.to_excel(writer_clf,sheet_name=\"UAR\")\n",
    "        df_best_result_AUC.to_excel(writer_clf,sheet_name=\"AUC\")\n",
    "        df_best_result_f1.to_excel(writer_clf,sheet_name=\"f1\")\n",
    "\n",
    "writer_clf.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Change to paper name\n",
    "df_allThreeClassifiers_paperName=df_best_result_allThreeClassifiers.copy()\n",
    "index_bag=[]\n",
    "for exp_str in df_best_result_allThreeClassifiers.index:\n",
    "    experiment_name, feature_name=exp_str.split(\" >> \")\n",
    "    paper_idx='+'.join([Swap2PaperName(n, PprNmeMp) for n in feature_name.split(\"+\")])\n",
    "    index_bag.append(paper_idx)\n",
    "df_allThreeClassifiers_paperName.index=index_bag\n",
    "\n",
    "df_allThreeClassifiers_paperName.to_excel(Result_path+\"/\"+f\"TASLPTABLE-ClassFusion_Norm[{args.Normalize_way}].xlsx\")\n",
    "print(\"df_allThreeClassifiers_paperName generated at \", Result_path+\"/\"+f\"TASLPTABLE-Class_Norm[{args.Normalize_way}].xlsx\")\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Analysis part\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def Organize_Needed_SHAP_info(Incorrect2Correct_indexes, Session_level_all, proposed_expstr):\n",
    "    Incorrect2Correct_info_dict=Dict()\n",
    "    for tst_idx in Incorrect2Correct_indexes:\n",
    "        for key, values in Session_level_all[proposed_expstr]['SHAP_info'].items():\n",
    "            test_fold_idx=[int(k) for k in key.split(\"_\")]\n",
    "            for i,ii in enumerate(test_fold_idx): #ii is the index of the sample, i is the position of this sample in this test fold\n",
    "                if tst_idx == ii:\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['XTest']=values['XTest'].iloc[i,:]\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['explainer_expected_value']=values['explainer_expected_value']\n",
    "                    \n",
    "                    # 因為之前都用predict_proba 但是自從發現predict_proba有BUG之後就改用decision function了，\n",
    "                    # 但是改成decision function之後資料結構會跟原來的不一樣，所以額外仿造了一個logit的values\n",
    "                    # shap_values_array=[array[i,:] for array in values['shap_values']]\n",
    "                    shap_values_array=[array[i,:] for array in [values['shap_values'],-values['shap_values']]]\n",
    "                    \n",
    "                    df_shap_values=pd.DataFrame(shap_values_array,columns=Incorrect2Correct_info_dict[tst_idx]['XTest'].index)\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['shap_values']=df_shap_values\n",
    "                    print(\"testing sample \", ii, \"is in the \", i, \"position of test fold\", key)\n",
    "                    assert (Incorrect2Correct_info_dict[tst_idx]['XTest'] == Session_level_all[proposed_expstr]['X'].iloc[tst_idx]).all()\n",
    "                    # print(\"It's feature value captured is\", Incorrect2Correct_info_dict[tst_idx]['XTest'])\n",
    "                    # print(\"It's original X value is\", Session_level_all[proposed_expstr]['X'].iloc[tst_idx])\n",
    "                    # print(\"See if they match\")\n",
    "    return Incorrect2Correct_info_dict\n",
    "# =============================================================================\n",
    "# Debug\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# \\Debug\n",
    "# =============================================================================\n",
    "def Get_Model_Type12Errors(model_str='baseline', tureLab_str='y_true'):\n",
    "    # Positive = ASD\n",
    "    Type1Err= ( df_Y_pred[model_str]  == sellect_people_define.ASDTD_label['ASD']) & ( df_Y_pred[tureLab_str] == sellect_people_define.ASDTD_label['TD']  )\n",
    "    Type2Err= ( df_Y_pred[model_str]  == sellect_people_define.ASDTD_label['TD'] ) & ( df_Y_pred[tureLab_str] == sellect_people_define.ASDTD_label['ASD']  )\n",
    "    return Type1Err, Type2Err\n",
    "# =============================================================================\n",
    "\n",
    "'''\n",
    "\n",
    "    Part 1: Check incorrect to correct and correct to incorrect\n",
    "\n",
    "'''\n",
    "\n",
    "if args.Print_Analysis_grp_Manual_select == True:\n",
    "    count=0\n",
    "    for exp_lst in FeatureLabelMatch_manual:\n",
    "        exp_lst_str='::'.join(exp_lst)\n",
    "        if count < len(FeatureLabelMatch_manual)/3:\n",
    "            print(\"proposed_expstr='{}'\".format(exp_lst_str))\n",
    "        else:\n",
    "            print(\"baseline_expstr='{}'\".format(exp_lst_str))\n",
    "        count+=1\n",
    "    \n",
    "\n",
    "############################################################\n",
    "# XXX 選擇要分析的地方\n",
    "# Reverse_exp=False 代表我們檢查從baseline到proposed; \n",
    "# Reverse_exp=True 代表我們檢查從proposed到baseline; \n",
    "# 所以如果是要算I->I+ 就proposed_expstr=I+, baseline_expstr=I; 如果是要算I->I- 就proposed_expstr=I+, baseline_expstr=I 然後Reverse_exp=True\n",
    "\n",
    "\n",
    "# Low Minimal\n",
    "proposed_expstr='TD vs df_feature_lowMinimal_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Syncrony_cols+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
    "baseline_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "Reverse_exp=False\n",
    "\n",
    "[4, 6, 15]\n",
    "[0, 25]\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_lowMinimal_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Syncrony_cols+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Moderate\n",
    "# proposed_expstr='TD vs df_feature_moderate_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_moderate_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=False\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_moderate_CSS >> LOC_columns+LOCDEP_Trend_D_cols+LOCDEP_Trend_K_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_moderate_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=True\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "# high\n",
    "# proposed_expstr='TD vs df_feature_high_CSS >> DEP_columns+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_high_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=False\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_high_CSS >> DEP_columns+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_high_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=True\n",
    "\n",
    "\n",
    "\n",
    "experiment_title=baseline_expstr[re.search(\"df_feature_\",baseline_expstr).end():re.search(\"_CSS >> \",baseline_expstr).start()]\n",
    "\n",
    "proposed_featset_lst=proposed_expstr[re.search(\" >> \",proposed_expstr).end():re.search(\"::\",proposed_expstr).start()].split(\"+\")\n",
    "baseline_featset_lst=baseline_expstr[re.search(\" >> \",baseline_expstr).end():re.search(\"::\",baseline_expstr).start()].split(\"+\")\n",
    "Additional_featureSet=set(proposed_featset_lst) - set(baseline_featset_lst)\n",
    "print(\"For Task\", experiment_title, \" additional feature sets are\", Additional_featureSet)\n",
    "# =============================================================================\n",
    "# Error type analyses\n",
    "# =============================================================================\n",
    "# df_compare_pair=pd.DataFrame(list())\n",
    "Y_pred_lst=[\n",
    "Session_level_all[proposed_expstr]['y_pred'],\n",
    "Session_level_all[baseline_expstr]['y_pred'],\n",
    "Session_level_all[proposed_expstr]['y_true'],\n",
    "Session_level_all[proposed_expstr]['y_true'].index,\n",
    "]\n",
    "assert (Session_level_all[proposed_expstr]['y_true'] == Session_level_all[baseline_expstr]['y_true']).all()\n",
    "\n",
    "df_Y_pred=pd.DataFrame(Y_pred_lst[:-1],index=['proposed','baseline','y_true']).T\n",
    "df_Y_pred_withName=pd.DataFrame(Y_pred_lst,index=['proposed','baseline','y_true','name']).T\n",
    "df_Index2Name_mapping=df_Y_pred_withName['name']\n",
    "\n",
    "\n",
    "Incorrect=df_Y_pred['baseline'] != df_Y_pred['y_true']\n",
    "Correct=df_Y_pred['proposed'] == df_Y_pred['y_true']\n",
    "Incorrect2Correct= Correct & Incorrect\n",
    "\n",
    "\n",
    "Incorrect=df_Y_pred['baseline'] == df_Y_pred['y_true']\n",
    "Correct=df_Y_pred['proposed'] != df_Y_pred['y_true']\n",
    "Correct2Incorrect= Correct & Incorrect\n",
    "\n",
    "Incorrect2Correct_indexes=list(df_Y_pred[Incorrect2Correct].index)\n",
    "Correct2Incorrect_indexes=list(df_Y_pred[Correct2Incorrect].index)\n",
    "print('Incorrect2Correct_indexes: ', Incorrect2Correct_indexes)\n",
    "print('Correct2Incorrect_indexes: ', Correct2Incorrect_indexes)\n",
    "\n",
    "\n",
    "Ones=df_Y_pred['baseline'] ==sellect_people_define.ASDTD_label['ASD']\n",
    "Twos=df_Y_pred['proposed'] ==sellect_people_define.ASDTD_label['TD']\n",
    "Ones2Twos=  Ones & Twos\n",
    "\n",
    "Twos=df_Y_pred['baseline'] ==sellect_people_define.ASDTD_label['TD']\n",
    "Ones=df_Y_pred['proposed'] ==sellect_people_define.ASDTD_label['ASD']\n",
    "Twos2Ones=  Ones & Twos\n",
    "\n",
    "Ones2Twos_indexes=list(df_Y_pred[Ones2Twos].index)\n",
    "Twos2Ones_indexes=list(df_Y_pred[Twos2Ones].index)\n",
    "\n",
    "assert len(Ones2Twos_indexes+Twos2Ones_indexes) == len(Incorrect2Correct_indexes+Correct2Incorrect_indexes)\n",
    "\n",
    "ASDTD2Logit_map={\n",
    "    'TD': sellect_people_define.ASDTD_label['TD']-1,\n",
    "    'ASD': sellect_people_define.ASDTD_label['ASD']-1,\n",
    "    }\n",
    "\n",
    "quadrant1_indexes=intersection(Correct2Incorrect_indexes, Ones2Twos_indexes)\n",
    "quadrant2_indexes=intersection(Incorrect2Correct_indexes, Ones2Twos_indexes)\n",
    "quadrant3_indexes=intersection(Correct2Incorrect_indexes, Twos2Ones_indexes)\n",
    "quadrant4_indexes=intersection(Incorrect2Correct_indexes, Twos2Ones_indexes)\n",
    "\n",
    "\n",
    "Type1Err_dict, Type2Err_dict={}, {}\n",
    "for model_str in ['baseline', 'proposed']:\n",
    "    Type1Err_dict[model_str], Type2Err_dict[model_str] = Get_Model_Type12Errors(model_str=model_str, tureLab_str='y_true')\n",
    "\n",
    "model_str='baseline'\n",
    "Type1Err_baseline_indexes=list(df_Y_pred[Type1Err_dict[model_str]].index)\n",
    "Type2Err_baseline_indexes=list(df_Y_pred[Type2Err_dict[model_str]].index)\n",
    "model_str='proposed'\n",
    "Type1Err_proposed_indexes=list(df_Y_pred[Type1Err_dict[model_str]].index)\n",
    "Type2Err_proposed_indexes=list(df_Y_pred[Type2Err_dict[model_str]].index)\n",
    "\n",
    "\n",
    "\n",
    "All_err_indexes=list(set(Type1Err_baseline_indexes+Type2Err_baseline_indexes+Type1Err_proposed_indexes+Type2Err_proposed_indexes))\n",
    "All_indexes=list(df_Y_pred.index)\n",
    "'''\n",
    "\n",
    "    Part 2: Check the SHAP values based on indexes in part 1\n",
    "    \n",
    "    先紀錄，再執行分析和畫圖\n",
    "\n",
    "'''\n",
    "Manual_inspect_idxs=[2, 3, 29, 36, 44]\n",
    "##############################################\n",
    "\n",
    "# selected_idxs=Ones2Twos_indexes+Twos2Ones_indexes\n",
    "selected_idxs=All_indexes\n",
    "Baseline_changed_info_dict=Organize_Needed_SHAP_info(selected_idxs, Session_level_all, baseline_expstr)\n",
    "Proposed_changed_info_dict=Organize_Needed_SHAP_info(selected_idxs, Session_level_all, proposed_expstr)\n",
    "\n",
    "Baseline_totalPoeple_info_dict=Organize_Needed_SHAP_info(df_Y_pred.index, Session_level_all, baseline_expstr)\n",
    "Proposed_totalPoeple_info_dict=Organize_Needed_SHAP_info(df_Y_pred.index, Session_level_all, proposed_expstr)\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "# 個體分析： 會存到SAHP_figures/{quadrant}/的資料夾，再開Jupyter去看\n",
    "def Organize_Needed_decisionProb(Incorrect2Correct_indexes, Session_level_all, proposed_expstr):\n",
    "    Incorrect2Correct_info_dict=Dict()\n",
    "    for tst_idx in Incorrect2Correct_indexes:\n",
    "        for key, values in Session_level_all[proposed_expstr]['Logit{}_predictproba'.format(logit_number)].items():\n",
    "            test_fold_idx=[int(k) for k in key.split(\"_\")]\n",
    "            for i,ii in enumerate(test_fold_idx): #ii is the index of the sample, i is the position of this sample in this test fold\n",
    "                if tst_idx == ii:\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['predictproba']=values['predictproba'][i]\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['decisionfunc']=values['decisionfunc'][i]\n",
    "\n",
    "    return Incorrect2Correct_info_dict\n",
    "\n",
    "SHAP_save_path_root=\"SHAP_figures/{quadrant}/\"\n",
    "# step 1: prepare data\n",
    "\n",
    "# selected_idxs=Ones2Twos_indexes+Twos2Ones_indexes\n",
    "Baseline_changed_decision_info_dict=Organize_Needed_decisionProb(selected_idxs, Session_level_all, baseline_expstr)\n",
    "Proposed_changed_decision_info_dict=Organize_Needed_decisionProb(selected_idxs, Session_level_all, proposed_expstr)\n",
    "Baseline_total_decision_info_dict=Organize_Needed_decisionProb(df_Y_pred.index, Session_level_all, baseline_expstr)\n",
    "Proposed_total_decision_info_dict=Organize_Needed_decisionProb(df_Y_pred.index, Session_level_all, proposed_expstr)\n",
    "\n",
    "\n",
    "\n",
    "df_Proposed_changed_decision_info_dict=pd.DataFrame.from_dict(Proposed_changed_decision_info_dict,orient='index')\n",
    "df_Baseline_changed_decision_info_dict=pd.DataFrame.from_dict(Baseline_changed_decision_info_dict,orient='index')\n",
    "df_Proposed_total_decision_info_dict=pd.DataFrame.from_dict(Proposed_total_decision_info_dict,orient='index')\n",
    "df_Baseline_total_decision_info_dict=pd.DataFrame.from_dict(Baseline_total_decision_info_dict,orient='index')\n",
    "Sample_idxs_array=df_Baseline_changed_decision_info_dict.index.values\n",
    "\n",
    "df_Y_true=df_Y_pred.loc[df_Baseline_changed_decision_info_dict.index]['y_true']\n",
    "df_Y_true_ASD_bool=df_Y_true==sellect_people_define.ASDTD_label['ASD']\n",
    "df_Y_true_TD_bool=df_Y_true==sellect_people_define.ASDTD_label['TD']\n",
    "Incorrect_baseline=df_Y_pred['baseline'] != df_Y_pred['y_true']\n",
    "Incorrect_proposed=df_Y_pred['proposed'] != df_Y_pred['y_true']\n",
    "Correct_baseline=df_Y_pred['baseline'] == df_Y_pred['y_true']\n",
    "Correct_proposed=df_Y_pred['proposed'] == df_Y_pred['y_true']\n",
    "\n",
    "import shutil\n",
    "from collections import Counter\n",
    "shutil.rmtree(SHAP_save_path_root.format(quadrant=\"\"), ignore_errors = True)\n",
    "\n",
    "\n",
    "#%%\n",
    "Analysis_grp_bool=False\n",
    "N=5\n",
    "Xtest_dict={}\n",
    "expected_value_lst=[]\n",
    "UsePaperName_bool=True\n",
    "Quadrant_FeatureImportance_dict={}\n",
    "Quadrant_feature_AddedTopFive_dict={}\n",
    "Quadrant_feature_AddedFeatureImportance_dict={}\n",
    "Manual_inspect_idxs=[21, 1, 14, 15]\n",
    "df_Result_dict=Dict()\n",
    "# =============================================================================\n",
    "# 有predict probability的變數\n",
    "# 最後結果會放在一個nice table就不用一直手動複製了  \n",
    "# nice table 就放在df_Total_ErrAnal這個變數\n",
    "# 一路上都有放assertion做驗證所以等到assertion有誤的時候再去debug\n",
    "# XXX  放force plot的地方\n",
    "# =============================================================================\n",
    "# for Analysis_grp_str in ['Manual_inspect_idxs','quadrant1_indexes','quadrant2_indexes','quadrant3_indexes','quadrant4_indexes']:\n",
    "for Analysis_grp_str in ['quadrant1_indexes','quadrant2_indexes','quadrant3_indexes','quadrant4_indexes']:    \n",
    "# for Analysis_grp_str in ['quadrant2_indexes','quadrant4_indexes']:\n",
    "# for Analysis_grp_str in ['Manual_inspect_idxs']:\n",
    "    Analysis_grp_indexes=vars()[Analysis_grp_str]\n",
    "    df_shap_values_stacked=pd.DataFrame([])\n",
    "    for Inspect_samp in Analysis_grp_indexes:\n",
    "        shap_info_proposed=Proposed_changed_info_dict[Inspect_samp]\n",
    "        shap_info_baseline=Baseline_changed_info_dict[Inspect_samp]\n",
    "        # expected_value_proposed=shap_info_proposed['explainer_expected_value'][logit_number]\n",
    "        # expected_value_baseline=shap_info_baseline['explainer_expected_value'][logit_number]\n",
    "        expected_value_proposed=shap_info_proposed['explainer_expected_value']\n",
    "        expected_value_baseline=shap_info_baseline['explainer_expected_value']\n",
    "        df_shap_values_proposed=shap_info_proposed['shap_values'].loc[[logit_number]]\n",
    "        df_shap_values_baseline=shap_info_baseline['shap_values'].loc[[logit_number]]\n",
    "        deltaprob_baseline=df_shap_values_baseline.T.sum()\n",
    "        deltaprob_proposed=df_shap_values_proposed.T.sum()\n",
    "        BaselineFeatures=[getattr(FeatSel,k)  for k in baseline_featset_lst]\n",
    "        BaselineFeatures_flatten=[e for ee in BaselineFeatures for e in ee]\n",
    "        ProposedFeatures=[getattr(FeatSel,k)  for k in proposed_featset_lst]\n",
    "        ProposedFeatures_flatten=[e for ee in ProposedFeatures for e in ee]\n",
    "        Lists_of_addedFeatures=[getattr(FeatSel,k)  for k in Additional_featureSet]\n",
    "        Lists_of_addedFeatures_flatten=[e for ee in Lists_of_addedFeatures for e in ee]\n",
    "        deltaprob_baselineFeat_baseline=df_shap_values_baseline[BaselineFeatures_flatten].T.sum()\n",
    "        deltaprob_baselineFeat_proposed=df_shap_values_proposed[ProposedFeatures_flatten].T.sum()\n",
    "        deltaprob_additionalFeat_proposed=df_shap_values_proposed[Lists_of_addedFeatures_flatten].T.sum()\n",
    "        \n",
    "        \n",
    "        Prediction_prob_baseline=df_Baseline_changed_decision_info_dict.loc[Inspect_samp,'decisionfunc']\n",
    "        Prediction_prob_proposed=df_Proposed_changed_decision_info_dict.loc[Inspect_samp,'decisionfunc']\n",
    "        # Prediction_prob_baseline=df_Baseline_changed_decision_info_dict.loc[Inspect_samp,'predictproba']\n",
    "        # Prediction_prob_proposed=df_Proposed_changed_decision_info_dict.loc[Inspect_samp,'predictproba']\n",
    "        # =====================================================================\n",
    "        if Reverse_exp == True:\n",
    "            shap_info=shap_info_baseline\n",
    "        else:\n",
    "            shap_info=shap_info_proposed\n",
    "        # expected_value=shap_info['explainer_expected_value'][logit_number]\n",
    "        expected_value=shap_info['explainer_expected_value']\n",
    "        shap_values=shap_info['shap_values'].loc[logit_number].values\n",
    "        df_shap_values=shap_info['shap_values'].loc[[logit_number]]\n",
    "        df_shap_values.index=[Inspect_samp]\n",
    "        \n",
    "        df_shap_values_T=df_shap_values.T\n",
    "        \n",
    "        if Reverse_exp == True:\n",
    "            Selected_feature_set=BaselineFeatures_flatten\n",
    "        else:\n",
    "            Selected_feature_set=ProposedFeatures_flatten\n",
    "        Xtest=shap_info['XTest']        \n",
    "        Xtest_additionaFeat=Xtest[Selected_feature_set]\n",
    "        Xtest_additionaFeat.index=[ Swap2PaperName(name,PprNmeMp) for name in Xtest_additionaFeat.index]\n",
    "        \n",
    "        Xtest_dict[Inspect_samp]=Xtest\n",
    "        df_shap_values_stacked=pd.concat([df_shap_values_stacked,df_shap_values],)\n",
    "        expected_value_lst.append(expected_value)\n",
    "    \n",
    "        # 這個部份跑TASLP的Fig.7 也就是說明有些不顯著的feature卻shap value很高\n",
    "        SHAP_save_path=SHAP_save_path_root.format(quadrant=Analysis_grp_str)\n",
    "        if not os.path.exists(SHAP_save_path):\n",
    "            os.makedirs(SHAP_save_path)\n",
    "        # ============================================================================= \n",
    "        # 觀察哪些additional feature有加分效用\n",
    "        df_shap_values_additionaFeat=df_shap_values_T.loc[Selected_feature_set].T\n",
    "        df_shap_values_toInspect=df_shap_values_T.loc[Selected_feature_set]\n",
    "        deltaprob_additionaFeat_proposed=df_shap_values_additionaFeat.T.sum()\n",
    "        # 把同一個feature set的SHAP values加總\n",
    "        # {Swap2PaperName(FestSet,PprNmeMp): for FestSet in Additional_featureSet:}\n",
    "        if Reverse_exp == True:\n",
    "            used_feature_lst=baseline_featset_lst\n",
    "        else:\n",
    "            used_feature_lst=proposed_featset_lst\n",
    "        FeatContrib_dict={}\n",
    "        for FestSet in used_feature_lst:\n",
    "            FeatContrib_dict[Swap2PaperName(FestSet,PprNmeMp)]=df_shap_values_toInspect.loc[getattr(FeatSel,FestSet)].sum().values[0]\n",
    "            df_shap_values_toInspect.loc[getattr(FeatSel,FestSet)].sum().values[0]\n",
    "        Sum_SHAPval=sum([v for k,v in FeatContrib_dict.items()])\n",
    "        \n",
    "        \n",
    "\n",
    "        # =============================================================================\n",
    "        # XXXDebug area\n",
    "        # =============================================================================\n",
    "        def Unittest_shapEqualsDecision():\n",
    "            tolarance=1e-5\n",
    "            df_Inconsistant=pd.DataFrame([],columns=['ValueFromShap','decision_value'])\n",
    "            for people in Proposed_totalPoeple_info_dict.keys():\n",
    "                Sum_shap_value=Proposed_totalPoeple_info_dict[people]['shap_values'].loc[logit_number].sum()\n",
    "                baseVal=Proposed_totalPoeple_info_dict[people]['explainer_expected_value']\n",
    "                ValueFromShap=baseVal + Sum_shap_value\n",
    "                decision_value=df_Proposed_total_decision_info_dict.loc[people,'decisionfunc']\n",
    "                if (ValueFromShap - decision_value) > tolarance:\n",
    "                    df_Inconsistant.loc[people]=[ValueFromShap,decision_value]\n",
    "        Unittest_shapEqualsDecision()\n",
    "        # =============================================================================\n",
    "        # /Debug area\n",
    "        # =============================================================================\n",
    "        \n",
    "            \n",
    "        # if Reverse_exp == True:\n",
    "        #     FeatContrib_percent_dict = {k:'{}%'.format('%.1f' % -np.round(v*100,3)) for k,v in FeatContrib_dict.items()}\n",
    "        # else:\n",
    "        #     FeatContrib_percent_dict = {k:'{}%'.format('%.1f' % np.round(v*100,3)) for k,v in FeatContrib_dict.items()}\n",
    "        FeatContrib_str_dict = {k:'{}'.format('%.3f' % np.round(v,3)) for k,v in FeatContrib_dict.items()}\n",
    "        SHAP_percent_lst=[float(v.replace(\"%\",\"\")) for k,v in FeatContrib_str_dict.items()]\n",
    "        # SHAP_percent_lst=[float(v.replace(\"%\",\"\")) for k,v in FeatContrib_percent_dict.items()]\n",
    "        \n",
    "        # df_FeatContrib 的整體在這裡\n",
    "        df_FeatContrib=pd.DataFrame([\"\\\\textbf{base val}:\"+ str(np.round(expected_value_proposed,3)) ],columns=['Feat_distrib'])\n",
    "        suffix_str=\" + \".join([str(np.round(v,3)) for v in SHAP_percent_lst])\n",
    "        print(\"base value\", np.round(expected_value_proposed,3), \"%\")\n",
    "        # for key, value in FeatContrib_str_dict.items():\n",
    "        #     print(key, \" : \",value)\n",
    "        #     # print(Swap2PaperName(key,PprNmeMp,method='idx'), \" : \",value)\n",
    "        #     # df_FeatContrib=pd.concat([df_FeatContrib,pd.DataFrame([','+str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \": \"+value])],axis=1)\n",
    "        #     df_FeatContrib['Feat_distrib']=df_FeatContrib['Feat_distrib']+','+str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \":\"+value\n",
    "        \n",
    "        def AppendStr(FeatContrib_str_dict):\n",
    "            Append_str=''\n",
    "            for i,(key, value) in enumerate(FeatContrib_str_dict.items()):\n",
    "                Append_str+=str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \":\"+value\n",
    "                if i % 2 ==1  and i < len(FeatContrib_str_dict.keys())-1:\n",
    "                    Append_str+='\\n'\n",
    "                elif i % 2 ==0 and i < len(FeatContrib_str_dict.keys())-1:\n",
    "                    Append_str+=','\n",
    "                else:\n",
    "                    Append_str+=''\n",
    "            return Append_str\n",
    "        Append_str=AppendStr(FeatContrib_str_dict)\n",
    "        # Append_str=','.join([str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \":\"+value for key, value in FeatContrib_str_dict.items()])\n",
    "        df_FeatContrib=pd.concat([df_FeatContrib,pd.DataFrame([Append_str],columns=df_FeatContrib.columns)],axis=0)\n",
    "        #######################################################################\n",
    "        \n",
    "        if Reverse_exp == True:\n",
    "            FinalProb_fromSHAP=expected_value_baseline + sum([v for k,v in FeatContrib_dict.items()])\n",
    "        else:\n",
    "            FinalProb_fromSHAP=expected_value_proposed + sum([v for k,v in FeatContrib_dict.items()])\n",
    "        \n",
    "        print(np.round(expected_value_proposed,3),\"+\",suffix_str, '=', FinalProb_fromSHAP)\n",
    "        if Reverse_exp == True:\n",
    "            # TDProb_str=\"TD: ({Prediction_prob_proposed}% → {Prediction_prob_baseline}%)\".format(\n",
    "            #                                                                 Prediction_prob_baseline='%.1f' % np.round(Prediction_prob_baseline*100,3),\n",
    "            #                                                                 Prediction_prob_proposed='%.1f' % np.round(Prediction_prob_proposed*100,3)\n",
    "            #                                                                 )\n",
    "            # ASDProb_str=\"ASD: ({Prediction_prob_proposed}% → {Prediction_prob_baseline}%)\".format(\n",
    "            #                                                                 Prediction_prob_baseline='%.1f' % np.round((1-Prediction_prob_baseline)*100,3),\n",
    "            #                                                                 Prediction_prob_proposed='%.1f' % np.round((1-Prediction_prob_proposed)*100,3)\n",
    "            #                                                                 )\n",
    "            # print(TDProb_str)\n",
    "            # print(ASDProb_str)\n",
    "            from_str = \"TD\"  if Prediction_prob_proposed > decision_boundary else \"ASD\"\n",
    "            to_str = \"TD\"  if Prediction_prob_baseline > decision_boundary else \"ASD\"\n",
    "            PredictionProb_str=\"{from_str}: {Prediction_prob_proposed:.2f}→{to_str}: {Prediction_prob_baseline:.2f}\".format(from_str=from_str,\n",
    "                                                                                                        to_str=to_str,\n",
    "                                                                                                        Prediction_prob_proposed=Prediction_prob_proposed,\n",
    "                                                                                                        Prediction_prob_baseline=Prediction_prob_baseline,\n",
    "                                                                                                        )\n",
    "            \n",
    "        else:\n",
    "            # TDProb_str=\"TD: ({Prediction_prob_baseline}% → {Prediction_prob_proposed}%)\".format(\n",
    "            #                                                                 Prediction_prob_baseline='%.1f' % np.round(Prediction_prob_baseline*100,3),\n",
    "            #                                                                 Prediction_prob_proposed='%.1f' % np.round(Prediction_prob_proposed*100,3)\n",
    "            #                                                                 )\n",
    "            # ASDProb_str=\"ASD: ({Prediction_prob_baseline}% → {Prediction_prob_proposed}%)\".format(\n",
    "            #                                                                  Prediction_prob_baseline='%.1f' % np.round((1-Prediction_prob_baseline)*100,3),\n",
    "            #                                                                  Prediction_prob_proposed='%.1f' % np.round((1-Prediction_prob_proposed)*100,3)\n",
    "            #                                                                 )\n",
    "            # print(TDProb_str)\n",
    "            # print(ASDProb_str)\n",
    "            from_str = \"TD\"  if Prediction_prob_baseline > decision_boundary else \"ASD\"\n",
    "            to_str = \"TD\"  if Prediction_prob_proposed > decision_boundary else \"ASD\"\n",
    "            PredictionProb_str=\"{from_str}:{Prediction_prob_baseline:.2f}→{to_str}:{Prediction_prob_proposed:.2f}\".format(from_str=from_str,\n",
    "                                                                                                        to_str=to_str,\n",
    "                                                                                                        Prediction_prob_proposed=Prediction_prob_proposed,\n",
    "                                                                                                        Prediction_prob_baseline=Prediction_prob_baseline,\n",
    "                                                                                                        )\n",
    "        print(PredictionProb_str)\n",
    "        assert from_str != to_str\n",
    "        # 這邊確認SHAP value加起來要從baseline的score到proposed的score\n",
    "        if Reverse_exp == True:\n",
    "            assert np.round(Prediction_prob_baseline,2) == np.round(FinalProb_fromSHAP,2)\n",
    "        else:\n",
    "            assert np.round(Prediction_prob_proposed,2) == np.round(FinalProb_fromSHAP,2)\n",
    "        \n",
    "        \n",
    "        df_instance=pd.DataFrame([str(Inspect_samp)])\n",
    "        df_PredictionProb=pd.DataFrame()\n",
    "        \n",
    "        if Reverse_exp == True:\n",
    "            if Analysis_grp_str == 'quadrant1_indexes' or Analysis_grp_str == 'quadrant3_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": X→O\"])\n",
    "            elif Analysis_grp_str == 'quadrant2_indexes' or Analysis_grp_str == 'quadrant4_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": O→X\"])\n",
    "        else:\n",
    "            if Analysis_grp_str == 'quadrant1_indexes' or Analysis_grp_str == 'quadrant3_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": O→X\"])\n",
    "            elif Analysis_grp_str == 'quadrant2_indexes' or Analysis_grp_str == 'quadrant4_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": X→O\"])\n",
    "        \n",
    "        df_predictionProb=pd.DataFrame([PredictionProb_str])\n",
    "        df_predictionProbnResult=pd.concat([df_classification_result , df_predictionProb],axis=0).reset_index(drop=True)\n",
    "        # df_predictionProb=pd.concat([df_predictionProb,pd.DataFrame([TDProb_str])],axis=0)\n",
    "        # df_predictionProb=pd.concat([df_predictionProb,pd.DataFrame([ASDProb_str])],axis=0)\n",
    "        \n",
    "        # df_predictionProb.loc[0,'Prediction Prob.']=\n",
    "        # df_PredictionProb.loc[1,'Prediction Prob.']=TDProb_str\n",
    "        # df_PredictionProb.loc[2,'Prediction Prob.']=ASDProb_str\n",
    "        df_NICETABLE=pd.DataFrame()\n",
    "        # df_NICETABLE=pd.concat([df_instance,df_predictionProbnResult],axis=1, ignore_index=True).reset_index(drop=True)\n",
    "        df_NICETABLE=pd.concat([df_predictionProbnResult],axis=1, ignore_index=True).reset_index(drop=True)\n",
    "        # df_NICETABLE=pd.merge(df_instance.T,df_predictionProbnResult.T, on=[0], how='inner')\n",
    "        # pd.merge(df_instance,df_predictionProbnResult, how='outer')\n",
    "        # df_instance.merge(df_predictionProbnResult)\n",
    "        \n",
    "        df_NICETABLE.drop_duplicates()\n",
    "        df_NICETABLE.index = range(len(df_NICETABLE))\n",
    "        df_FeatContrib.index=range(len(df_FeatContrib))\n",
    "        \n",
    "        # 最後看這個變數，複製到excel上\n",
    "        df_NICETABLE=pd.concat([df_NICETABLE,df_FeatContrib],axis=1, ignore_index=True)\n",
    "        df_Result_dict[Inspect_samp]=df_NICETABLE\n",
    "        \n",
    "        \n",
    "        fig = shap.force_plot(expected_value, df_shap_values_additionaFeat.values, Xtest_additionaFeat.round(2).T, figsize=(8, 3) ,matplotlib=True,show=False)\n",
    "        # fig = shap.force_plot(expected_value, df_shap_values.values, Xtest.round(2).T, figsize=(8, 3) ,matplotlib=True,show=False)\n",
    "        \n",
    "        \n",
    "        plt.savefig(\"images/SHAP_discussion_{sample}.png\".format(sample=Inspect_samp),dpi=400, bbox_inches='tight')\n",
    "        plt.savefig(SHAP_save_path+\"{sample}.png\".format(sample=Inspect_samp),dpi=150, bbox_inches='tight')\n",
    "        \n",
    "    if UsePaperName_bool==False:\n",
    "        df_shap_values_stacked.columns=[Swap2PaperName(idx, PprNmeMp) for idx in df_shap_values_stacked.columns]\n",
    "    \n",
    "    \n",
    "    \n",
    "# pd.DataFrame.from_dict(df_Result_dict,orient='index')\n",
    "df_Total_ErrAnal_row=pd.DataFrame()\n",
    "df_Total_ErrAnal=pd.DataFrame()\n",
    "\n",
    "\n",
    "group_dict={}\n",
    "count=1\n",
    "for key, values in df_Result_dict.items():\n",
    "    df_Total_ErrAnal_row=pd.concat([df_Total_ErrAnal_row,values],axis=1)\n",
    "    if count % 3 == 0:\n",
    "        df_Total_ErrAnal=pd.concat([df_Total_ErrAnal,df_Total_ErrAnal_row])\n",
    "        df_Total_ErrAnal_row=pd.DataFrame()\n",
    "    count+=1\n",
    "\n",
    "\n",
    "if not os.path.exists(Result_path+\"/Analyzed/\"):\n",
    "    os.makedirs(Result_path+\"/Analyzed/\")\n",
    "df_Total_ErrAnal.to_excel(Result_path+\"/Analyzed/\"+\"Classification_\"+args.Feature_mode+\".xlsx\")\n",
    "#%%\n",
    "# =============================================================================\n",
    "'''\n",
    "以下就是TASLP paper沒有用到備著的部分\n",
    "'''\n",
    "# =============================================================================\n",
    "Reverse_lst=[\n",
    "'Inter-Vowel Dispersion+GC[VSC]\\\\textsubscript{inv}+Syncrony[VSC]+GC[P]\\\\textsubscript{inv}+GC[P]\\\\textsubscript{part}+Proximity[P]',\n",
    "'GC[P]\\\\textsubscript{inv}+GC[P]\\\\textsubscript{part}+Proximity[P]',\n",
    "'GC[P]\\\\textsubscript{inv}+Proximity[P]+Convergence[P]+Syncrony[P]',\n",
    "'Inter-Vowel Dispersion+GC[VSC]\\\\textsubscript{inv}+GC[VSC]\\\\textsubscript{part}+Proximity[P]',\n",
    "'Proximity[P]',\n",
    "'GC[P]\\\\textsubscript{inv}+Proximity[P]',\n",
    "'formant dependency+Proximity[P]',\n",
    "'Proximity[P]+Syncrony[P]',\n",
    "'Proximity[P]'\n",
    "]\n",
    "for nameComb in Reverse_lst:\n",
    "    print(nameComb)\n",
    "    FeatBag=[]\n",
    "    for single_feat in nameComb.split(\"+\"):\n",
    "        InversedFeat=Swap2PaperName(single_feat,PprNmeMp,method='inverse')\n",
    "        FeatBag.append(InversedFeat)\n",
    "    Feature_combination='+'.join(FeatBag)\n",
    "    print(Feature_combination)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "group_dict={}\n",
    "count=1\n",
    "for key, values in df_Result_dict.items():\n",
    "    df_Total_ErrAnal_row=pd.concat([df_Total_ErrAnal_row,values],axis=1)\n",
    "    if count % 3 == 0:\n",
    "        df_Total_ErrAnal=pd.concat([df_Total_ErrAnal,df_Total_ErrAnal_row])\n",
    "        df_Total_ErrAnal_row=pd.DataFrame()\n",
    "    print(df_Total_ErrAnal)\n",
    "    count+=1\n",
    "\n",
    "\n",
    "if not os.path.exists(Result_path+\"/Analyzed/\"):\n",
    "    os.makedirs(Result_path+\"/Analyzed/\")\n",
    "df_Total_ErrAnal_row.to_excel(Result_path+\"/Analyzed/\"+\"Classification_\"+args.Feature_mode+\".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TASLPReview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
