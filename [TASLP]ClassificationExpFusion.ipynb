{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/jack/workspace/DisVoice/articulation/HYPERPARAM/Label.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  label_raw=label_raw.append(df_labels_TD)\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "/media/jack/workspace/DisVoice/utils_jack.py:443: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  Info_name_sex=Info_name_sex.append(Info_name_sex_TD)\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/home/jack/anaconda3/lib/python3.9/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jun 30 15:56:45 2021\n",
    "\n",
    "@author: jackchen\n",
    "\n",
    "\n",
    "    This is a branch of MainClassification that run specific experiments\n",
    "    \n",
    "    Include SHAP values in this script\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import spearmanr,pearsonr \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from addict import Dict\n",
    "# import functions\n",
    "import argparse\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import sklearn.svm\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from articulation.HYPERPARAM import phonewoprosody, Label\n",
    "from articulation.HYPERPARAM.PeopleSelect import SellectP_define\n",
    "import articulation.HYPERPARAM.FeatureSelect as FeatSel\n",
    "\n",
    "import articulation.articulation\n",
    "from sklearn.metrics import f1_score,recall_score,roc_auc_score,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import combinations\n",
    "import shap\n",
    "import articulation.HYPERPARAM.PaperNameMapping as PprNmeMp\n",
    "import inspect\n",
    "import seaborn as sns\n",
    "\n",
    "from articulation.HYPERPARAM.PlotFigureVars import *\n",
    "from utils import debug_print\n",
    "def Find_Experiment_actualindex(Total_FeatComb_idxs,search_string):\n",
    "    # usage:\n",
    "    # e.x. :\n",
    "    # search_string='Phonation_Trend_K_cols+Phonation_Syncrony_cols+Phonation_Trend_D_cols'\n",
    "    # Total_FeatComb_idxs=FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation['static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation'].keys()\n",
    "    # Find_Experiment_actualindex(Total_FeatComb_idxs,search_string)\n",
    "    for FC_str in Total_FeatComb_idxs:\n",
    "        if ''.join(sorted(FC_str.split(\"+\"))) == ''.join(sorted(search_string.split(\"+\"))):\n",
    "            print(FC_str)\n",
    "\n",
    "\n",
    "\n",
    "def Assert_labelfeature(feat_name,lab_name):\n",
    "    # =============================================================================\n",
    "    #     To check if the label match with feature\n",
    "    # =============================================================================\n",
    "    for i,n in enumerate(feat_name):\n",
    "        assert feat_name[i] == lab_name[i]\n",
    "\n",
    "def FilterFile_withinManualName(files,Manual_choosen_feature):\n",
    "    files_manualChoosen=[f  for f in files if os.path.basename(f).split(\".\")[0]  in Manual_choosen_feature]\n",
    "    return files_manualChoosen\n",
    "\n",
    "def Merge_dfs(df_1, df_2):\n",
    "    return pd.merge(df_1,df_2,left_index=True, right_index=True)\n",
    "\n",
    "def Add_label(df_formant_statistic,Label,label_choose='ADOS_cate_C'):\n",
    "    for people in df_formant_statistic.index:\n",
    "        bool_ind=Label.label_raw['name']==people\n",
    "        df_formant_statistic.loc[people,label_choose]=Label.label_raw.loc[bool_ind,label_choose].values\n",
    "    return df_formant_statistic\n",
    "def Swap2PaperName(feature_rawname,PprNmeMp,method='origin'):\n",
    "    if method=='origin':\n",
    "        PaperNameMapping=PprNmeMp.Paper_name_map\n",
    "    elif method=='inverse':\n",
    "        PaperNameMapping=PprNmeMp.Inverse_Paper_name_map\n",
    "    elif method=='idx':\n",
    "        PaperNameMapping=PprNmeMp.Feature2idx_map\n",
    "    \n",
    "    if feature_rawname in PaperNameMapping.keys():\n",
    "        featurename_paper=PaperNameMapping[feature_rawname]\n",
    "        feature_keys=featurename_paper\n",
    "    else: \n",
    "        feature_keys=feature_rawname\n",
    "    return feature_keys\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def Return_ALigned_dfXtestndfShapValues(Feature_SHAP_info_dict,logit_number=1):\n",
    "    #Inputs\n",
    "    # logit_numberit_number=1\n",
    "    # Feature_SHAP_info_dict=Proposed_changed_info_dict\n",
    "    ###################################\n",
    "    df_XTest_stacked=pd.DataFrame()\n",
    "    df_ShapValues_stacked=pd.DataFrame()\n",
    "    for people in Feature_SHAP_info_dict.keys():\n",
    "        df_XTest=Feature_SHAP_info_dict[people]['XTest']\n",
    "        df_ShapValues=Feature_SHAP_info_dict[people]['shap_values'].loc[logit_number]\n",
    "        df_ShapValues.name=df_XTest.name\n",
    "        df_XTest_stacked=pd.concat([df_XTest_stacked,df_XTest],axis=1)\n",
    "        df_ShapValues_stacked=pd.concat([df_ShapValues_stacked,df_ShapValues],axis=1)\n",
    "    return df_XTest_stacked,df_ShapValues_stacked\n",
    "\n",
    "def Calculate_XTestShape_correlation(Feature_SHAP_info_dict,logit_number=1):\n",
    "    df_XTest_stacked,df_ShapValues_stacked=Return_ALigned_dfXtestndfShapValues(Feature_SHAP_info_dict,logit_number=logit_number)\n",
    "    Correlation_XtestnShap={}\n",
    "    for features in df_XTest_stacked.index:\n",
    "        r,p=pearsonr(df_XTest_stacked.loc[features],df_ShapValues_stacked.loc[features])\n",
    "        Correlation_XtestnShap[features]=r\n",
    "    df_Correlation_XtestnShap=pd.DataFrame.from_dict(Correlation_XtestnShap,orient='index')\n",
    "    df_Correlation_XtestnShap.columns=['correlation w logit:{}'.format(logit_number)]\n",
    "    return df_Correlation_XtestnShap\n",
    "\n",
    "def Prepare_data_for_summaryPlot(SHAPval_info_dict, feature_columns=None, PprNmeMp=None):\n",
    "    keys_bag=[]\n",
    "    XTest_dict={}\n",
    "    shap_values_0_bag=[]\n",
    "    shap_values_1_bag=[]\n",
    "    for people in sorted(SHAPval_info_dict.keys()):\n",
    "        keys_bag.append(people)\n",
    "        if not feature_columns == None:\n",
    "            df_=SHAPval_info_dict[people]['XTest'][feature_columns]\n",
    "            df_shape_value=SHAPval_info_dict[people]['shap_values'][feature_columns]\n",
    "        else:\n",
    "            df_=SHAPval_info_dict[people]['XTest']\n",
    "            df_shape_value=SHAPval_info_dict[people]['shap_values']\n",
    "        # if not SumCategorical_feats == None:\n",
    "        #     for k,values in SumCategorical_feats.items():\n",
    "        #         df_[k]=df_.loc[values].sum()\n",
    "        XTest_dict[people]=df_        \n",
    "        shap_values_0_bag.append(df_shape_value.loc[0].values)\n",
    "        shap_values_1_bag.append(df_shape_value.loc[1].values)\n",
    "    shap_values_0_array=np.vstack(shap_values_0_bag)\n",
    "    shap_values_1_array=np.vstack(shap_values_1_bag)\n",
    "    shap_values=[shap_values_0_array,shap_values_1_array]\n",
    "    df_XTest=pd.DataFrame.from_dict(XTest_dict,orient='index')\n",
    "    if PprNmeMp!=None:\n",
    "        df_XTest.columns=[Swap2PaperName(k,PprNmeMp) for k in df_XTest.columns]\n",
    "    return shap_values, df_XTest, keys_bag\n",
    "\n",
    "def get_args():\n",
    "    # we add compulsary arguments as named arguments for readability\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Feature_mode', default='Customized_feature',\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--preprocess', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--start_point', default=-1,\n",
    "                        help='In case that the program stop at certain point, we can resume the progress by setting this variable')\n",
    "    parser.add_argument('--experiment', default='gop_exp_ADOShappyDAAIKidallDeceiptformosaCSRC',\n",
    "                        help='If the mode is set to Session_phone_phf, you may need to determine the experiment used to generate the gop feature')\n",
    "    parser.add_argument('--pseudo', default=False,\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--suffix', default=\"\",\n",
    "                        help='what kind of data you want to get')\n",
    "    parser.add_argument('--FS_method_str', default=None,\n",
    "                        help='Feature selection')\n",
    "    parser.add_argument('--Print_Analysis_grp_Manual_select', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--Plot', default=True,\n",
    "                        help='')\n",
    "    parser.add_argument('--Reverse_exp', default=False, dest=\"Reverse_exp\",\n",
    "                        help='')\n",
    "    parser.add_argument('--selectModelScoring', default='recall_macro',\n",
    "                        help='[recall_macro,accuracy]')\n",
    "    parser.add_argument('--Mergefeatures', default=False,\n",
    "                        help='')\n",
    "    parser.add_argument('--logit_number', default=0,\n",
    "                        help='現在都改用decision function了，所以指會有一個loigit')\n",
    "    parser.add_argument('--decision_boundary', default=0,\n",
    "                        help='現在都改用decision function了，decision_boundary = 0')\n",
    "    parser.add_argument('--knn_weights', default='uniform',\n",
    "                            help='path of the base directory')\n",
    "    parser.add_argument('--knn_neighbors', default=2,  type=int,\n",
    "                            help='path of the base directory')\n",
    "    parser.add_argument('--Reorder_type', default='DKIndividual',\n",
    "                            help='[DKIndividual, DKcriteria]')\n",
    "    parser.add_argument('--FeatureComb_mode', default='baselineFeats',\n",
    "                            help='[Add_UttLvl_feature, feat_comb3, feat_comb5, feat_comb6,feat_comb7, baselineFeats,Comb_dynPhonation,Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation]')\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "args = get_args()\n",
    "start_point=args.start_point\n",
    "experiment=args.experiment\n",
    "knn_weights=args.knn_weights\n",
    "knn_neighbors=args.knn_neighbors\n",
    "Reorder_type=args.Reorder_type\n",
    "logit_number=args.logit_number\n",
    "Reverse_exp=args.Reverse_exp\n",
    "decision_boundary=args.decision_boundary\n",
    "\n",
    "\n",
    "Session_level_all=Dict()\n",
    "# Discriminative analysis Main\n",
    "columns=[\n",
    "    'Divergence[within_covariance_norm(A:,i:,u:)]',\n",
    "    'Divergence[within_variance_norm(A:,i:,u:)]',    \n",
    "    'Divergence[between_covariance_norm(A:,i:,u:)]',    \n",
    "    'Divergence[between_variance_norm(A:,i:,u:)]',    \n",
    "    'Divergence[total_covariance_norm(A:,i:,u:)]', \n",
    "    'Divergence[total_variance_norm(A:,i:,u:)]',\n",
    "    'Divergence[sam_wilks_lin_norm(A:,i:,u:)]',    \n",
    "    'Divergence[pillai_lin_norm(A:,i:,u:)]',\n",
    "    'Divergence[roys_root_lin_norm(A:,i:,u:)]',\n",
    "    'Divergence[hotelling_lin_norm(A:,i:,u:)]',\n",
    "    'Divergence[Between_Within_Det_ratio_norm(A:,i:,u:)]',\n",
    "    'Divergence[Between_Within_Tr_ratio_norm(A:,i:,u:)]',\n",
    "    'Divergence[within_covariance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[within_variance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[between_covariance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[between_variance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[total_covariance_norm(A:,i:,u:)]_var_p1', \n",
    "    'Divergence[total_variance_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[sam_wilks_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[pillai_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[roys_root_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[hotelling_lin_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[Between_Within_Det_ratio_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[Between_Within_Tr_ratio_norm(A:,i:,u:)]_var_p1',\n",
    "    'Divergence[within_covariance_norm(A:,i:,u:)]_var_p2',    \n",
    "    'Divergence[within_variance_norm(A:,i:,u:)]_var_p2',    \n",
    "    'Divergence[between_covariance_norm(A:,i:,u:)]_var_p2',    \n",
    "    'Divergence[between_variance_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[total_covariance_norm(A:,i:,u:)]_var_p2', \n",
    "    'Divergence[total_variance_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[sam_wilks_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[pillai_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[roys_root_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[hotelling_lin_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[Between_Within_Det_ratio_norm(A:,i:,u:)]_var_p2',\n",
    "    'Divergence[Between_Within_Tr_ratio_norm(A:,i:,u:)]_var_p2',\n",
    "    ]\n",
    "\n",
    "featuresOfInterest_manual=[ [col] for col in columns]\n",
    "# featuresOfInterest_manual=[ [col] + ['u_num+i_num+a_num'] for col in columns]\n",
    "\n",
    "\n",
    "label_choose=['ADOS_C']\n",
    "# label_choose=['ADOS_cate','ASDTD']\n",
    "df_formant_statistics_CtxPhone_collect_dict=Dict()\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "class ADOSdataset():\n",
    "    def __init__(self,knn_weights,knn_neighbors,Reorder_type,FeatureComb_mode):\n",
    "        self.featurepath='Features'            \n",
    "        self.N=2\n",
    "        self.LabelType=Dict()\n",
    "        self.LabelType['ADOS_C']='regression'\n",
    "        self.LabelType['ADOS_cate_C']='classification'\n",
    "        self.LabelType['ASDTD']='classification'\n",
    "        self.Fractionfeatures_str='Features/artuculation_AUI/Vowels/Fraction/*.pkl'    \n",
    "        self.FeatureComb_mode=FeatureComb_mode\n",
    "        if self.FeatureComb_mode == 'Add_UttLvl_feature':\n",
    "            self.File_root_path='Features/ClassificationMerged_dfs/ADDed_UttFeat/{knn_weights}_{knn_neighbors}_{Reorder_type}/'.format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type)\n",
    "            self.Merge_feature_path=self.File_root_path+'{dataset_role}/*.pkl'.format(dataset_role='ASD_DOCKID')\n",
    "        else:\n",
    "            self.File_root_path='Features/ClassificationMerged_dfs/{knn_weights}_{knn_neighbors}_{Reorder_type}/'.format(knn_weights=knn_weights,knn_neighbors=knn_neighbors,Reorder_type=Reorder_type)\n",
    "            self.Merge_feature_path=self.File_root_path+'{dataset_role}/*.pkl'.format(dataset_role='ASD_DOCKID')\n",
    "        self.Merge_feature_path='Features/ClassificationMerged_dfs/distance_2_DKIndividual/{dataset_role}/*.pkl'.format(dataset_role='ASD_DOCKID')\n",
    "        \n",
    "        self.Top_ModuleColumn_mapping_dict={}\n",
    "        self.Top_ModuleColumn_mapping_dict['Add_UttLvl_feature']=FeatSel.Columns_comb2.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb']=FeatSel.Columns_comb.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb3']=FeatSel.Columns_comb3.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb5']=FeatSel.Columns_comb5.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb6']=FeatSel.Columns_comb6.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['feat_comb7']=FeatSel.Columns_comb7.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['Comb_dynPhonation']=FeatSel.Comb_dynPhonation.copy()\n",
    "        self.Top_ModuleColumn_mapping_dict['Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']=FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation.copy()\n",
    "        \n",
    "        self.Top_ModuleColumn_mapping_dict['baselineFeats']=FeatSel.Baseline_comb.copy()\n",
    "        \n",
    "        self.FeatureCombs_manual=Dict()\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatLOC_kid']=['df_formant_statistic_TD_normal_kid', 'df_formant_statistic_agesexmatch_ASDSevereGrp_kid']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatLOC_kid']=['df_formant_statistic_TD_normal_kid', 'df_formant_statistic_agesexmatch_ASDMildGrp_kid']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatLOC_doc']=['df_formant_statistic_TD_normal_doc', 'df_formant_statistic_agesexmatch_ASDSevereGrp_doc']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatLOC_doc']=['df_formant_statistic_TD_normal_doc', 'df_formant_statistic_agesexmatch_ASDMildGrp_doc']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatLOC_DKRatio']=['df_formant_statistic_TD_normalGrp_DKRatio', 'df_formant_statistic_agesexmatch_ASDSevereGrp_DKRatio']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatLOC_DKRatio']=['df_formant_statistic_TD_normalGrp_DKRatio', 'df_formant_statistic_agesexmatch_ASDMildGrp_DKRatio']\n",
    "        \n",
    "        self.FeatureCombs_manual['TD_normal vs ASDSevere_agesexmatch >> FeatCoor']=['df_syncrony_statistic_TD_normalGrp', 'df_syncrony_statistic_agesexmatch_ASDSevereGrp']\n",
    "        self.FeatureCombs_manual['TD_normal vs ASDMild_agesexmatch >> FeatCoor']=['df_syncrony_statistic_TD_normalGrp', 'df_syncrony_statistic_agesexmatch_ASDMildGrp']\n",
    "        # self.FeatureCombs_manual['Notautism vs ASD']=['df_formant_statistic_77_Notautism', 'df_formant_statistic_77_ASD']\n",
    "        # self.FeatureCombs_manual['ASD vs Autism']=['df_formant_statistic_77_ASD', 'df_formant_statistic_77_Autism']\n",
    "        # self.FeatureCombs_manual['Notautism vs Autism']=['df_formant_statistic_77_Notautism', 'df_formant_statistic_77_Autism']\n",
    "    \n",
    "        # self._FeatureBuild_single()\n",
    "        self._FeatureBuild_Module()\n",
    "    def Get_FormantAUI_feat(self,label_choose,pickle_path,featuresOfInterest=['MSB_f1','MSB_f2','MSB_mix'],filterbyNum=True,**kwargs):\n",
    "        arti=articulation.articulation.Articulation()\n",
    "        #如果path有放的話字串的話，就使用path的字串，不然就使用「feat_」等於的東西，在function裡面會以kwargs的形式出現\n",
    "        if not kwargs and len(pickle_path)>0:\n",
    "            df_tmp=pickle.load(open(pickle_path,\"rb\")).sort_index()\n",
    "        elif len(kwargs)>0: # usage Get_FormantAUI_feat(...,key1=values1):\n",
    "            for k, v in kwargs.items(): #there will be only one element\n",
    "                df_tmp=kwargs[k].sort_index()\n",
    "\n",
    "        if filterbyNum:\n",
    "            df_tmp=arti.BasicFilter_byNum(df_tmp,N=self.N)\n",
    "\n",
    "        if label_choose not in df_tmp.columns:\n",
    "            for people in df_tmp.index:\n",
    "                lab=Label.label_raw[label_choose][Label.label_raw['name']==people]\n",
    "                df_tmp.loc[people,'ADOS']=lab.values\n",
    "            df_y=df_tmp['ADOS'] #Still keep the form of dataframe\n",
    "        else:\n",
    "            df_y=df_tmp[label_choose] #Still keep the form of dataframe\n",
    "        \n",
    "        \n",
    "        feature_array=df_tmp[featuresOfInterest]\n",
    "        \n",
    "            \n",
    "        LabType=self.LabelType[label_choose]\n",
    "        return feature_array, df_y, LabType\n",
    "    def _FeatureBuild_single(self):\n",
    "        Features=Dict()\n",
    "        Features_comb=Dict()\n",
    "        files = glob.glob(self.Fractionfeatures_str)\n",
    "        for file in files:\n",
    "            feat_name=os.path.basename(file).replace(\".pkl\",\"\")\n",
    "            df_tmp=pickle.load(open(file,\"rb\")).sort_index()\n",
    "            Features[feat_name]=df_tmp\n",
    "        for keys in self.FeatureCombs_manual.keys():\n",
    "            combF=[Features[k] for k in self.FeatureCombs_manual[keys]]\n",
    "            Features_comb[keys]=pd.concat(combF)\n",
    "        \n",
    "        self.Features_comb_single=Features_comb\n",
    "    def _FeatureBuild_Module(self):\n",
    "        Labels_add=['ASDTD']\n",
    "        ModuledFeatureCombination=self.Top_ModuleColumn_mapping_dict[self.FeatureComb_mode]\n",
    "        \n",
    "        sellect_people_define=SellectP_define()\n",
    "        #Loading features from ASD        \n",
    "        Features_comb=Dict()\n",
    "        IterateFilesFullPaths = glob.glob(self.Merge_feature_path)\n",
    "        \n",
    "\n",
    "        if self.FeatureComb_mode in ['feat_comb3','feat_comb5','feat_comb6','feat_comb7','Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']:\n",
    "            DfCombFilenames=['static_feautre_LOC+dynamic_feature_LOC+dynamic_feature_phonation.pkl']\n",
    "        elif self.FeatureComb_mode == 'Comb_dynPhonation':\n",
    "            DfCombFilenames=['dynamic_feature_phonation.pkl']\n",
    "        elif self.FeatureComb_mode == 'baselineFeats':\n",
    "             DfCombFilenames=['{}.pkl'.format(Dataname) for Dataname in ModuledFeatureCombination.keys()]\n",
    "        else:\n",
    "            DfCombFilenames=[os.path.basename(f) for f in IterateFilesFullPaths]\n",
    "        \n",
    "        File_ASD_paths=[self.File_root_path+\"ASD_DOCKID/\"+f for f in DfCombFilenames]\n",
    "        File_TD_paths=[self.File_root_path+\"TD_DOCKID/\"+f for f in DfCombFilenames]\n",
    "        \n",
    "        \n",
    "        df_Top_Check_length=pd.DataFrame()\n",
    "        for file_ASD, file_TD in zip(File_ASD_paths,File_TD_paths):\n",
    "            if not os.path.exists(file_ASD) or not os.path.exists(file_TD):\n",
    "                raise FileExistsError()\n",
    "            \n",
    "            assert os.path.basename(file_ASD) == os.path.basename(file_TD)\n",
    "            filename=os.path.basename(file_ASD)\n",
    "            k_FeatTypeLayer1=filename.replace(\".pkl\",\"\")\n",
    "            df_feature_ASD=pickle.load(open(file_ASD,\"rb\")).sort_index()\n",
    "            df_feature_TD=pickle.load(open(file_TD,\"rb\")).sort_index()\n",
    "            df_feature_ASD['ASDTD']=sellect_people_define.ASDTD_label['ASD']\n",
    "            df_feature_TD['ASDTD']=sellect_people_define.ASDTD_label['TD']\n",
    "            \n",
    "            # ADD label\n",
    "            df_feature_ASD=Add_label(df_feature_ASD,Label,label_choose='ADOS_cate_CSS')\n",
    "            df_feature_ASD=Add_label(df_feature_ASD,Label,label_choose='age_year')\n",
    "            df_feature_ASD=Add_label(df_feature_ASD,Label,label_choose='sex')\n",
    "            # create different ASD cohort\n",
    "            filter_Minimal_TCSS=df_feature_ASD['ADOS_cate_CSS']==0\n",
    "            filter_low_TCSS=df_feature_ASD['ADOS_cate_CSS']==1\n",
    "            filter_moderate_TCSS=df_feature_ASD['ADOS_cate_CSS']==2\n",
    "            filter_high_TCSS=df_feature_ASD['ADOS_cate_CSS']==3\n",
    "            \n",
    "            df_feauture_ASDgrp_dict={}\n",
    "            df_feauture_ASDgrp_dict['df_feature_ASD']=df_feature_ASD\n",
    "            \n",
    "            # df_feauture_ASDgrp_dict['df_feature_Minimal_CSS']=df_feature_ASD[filter_Minimal_TCSS]\n",
    "            # df_feauture_ASDgrp_dict['df_feature_low_CSS']=df_feature_ASD[filter_low_TCSS]\n",
    "            # df_feauture_ASDgrp_dict['df_feature_moderate_CSS']=df_feature_ASD[filter_moderate_TCSS]\n",
    "            # df_feauture_ASDgrp_dict['df_feature_high_CSS']=df_feature_ASD[filter_high_TCSS]\n",
    "            df_feauture_ASDgrp_dict['df_feature_lowMinimal_CSS']=df_feature_ASD[filter_low_TCSS | filter_Minimal_TCSS]\n",
    "            df_feauture_ASDgrp_dict['df_feature_moderate_CSS']=df_feature_ASD[filter_moderate_TCSS]\n",
    "            df_feauture_ASDgrp_dict['df_feature_high_CSS']=df_feature_ASD[filter_high_TCSS]\n",
    "\n",
    "            #Check the length of each paired comparison, should be stored on the top of for loop\n",
    "            Tmp_Numcmp_dict={}\n",
    "            for key in df_feauture_ASDgrp_dict.keys():\n",
    "                Numcmp_str='ASD({0}) vs TD({1})'.format(len(df_feauture_ASDgrp_dict[key]),len(df_feature_TD))\n",
    "                Tmp_Numcmp_dict[key]=Numcmp_str\n",
    "                \n",
    "            \n",
    "            df_Tmp_Numcmp_list=pd.DataFrame.from_dict(Tmp_Numcmp_dict,orient='index')\n",
    "            df_Tmp_Numcmp_list.columns=[k_FeatTypeLayer1]\n",
    "\n",
    "            if len(df_Top_Check_length)==0:\n",
    "                df_Top_Check_length=df_Tmp_Numcmp_list\n",
    "            else:\n",
    "                df_Top_Check_length=Merge_dfs(df_Top_Check_length,df_Tmp_Numcmp_list)\n",
    "            \n",
    "            # debug_print(File_ASD_paths,File_TD_paths)\n",
    "            # debug_print(file_ASD)\n",
    "            # debug_print(file_TD)\n",
    "            # debug_print(k_FeatTypeLayer1)\n",
    "            # debug_print(ModuledFeatureCombination.keys())\n",
    "            # return df_Top_Check_length ######***************DEBUG用 \n",
    "            # 手動執行到這邊，從for 上面\n",
    "            for k_FeatTypeLayer2 in ModuledFeatureCombination[k_FeatTypeLayer1].keys():\n",
    "                colums_sel=ModuledFeatureCombination[k_FeatTypeLayer1][k_FeatTypeLayer2]\n",
    "                \n",
    "\n",
    "                # 1. Set ASD vs TD experiment\n",
    "                for k_ASDgrp in df_feauture_ASDgrp_dict.keys():\n",
    "                    df_ASD_subgrp=df_feauture_ASDgrp_dict[k_ASDgrp].copy()[colums_sel+Labels_add]\n",
    "                    df_TD_subgrp=df_feature_TD.copy()[colums_sel+Labels_add]\n",
    "                    \n",
    "                    experiment_str=\"{TD_name} vs {ASD_name} >> {feature_type}\".format(TD_name='TD',ASD_name=k_ASDgrp,feature_type=k_FeatTypeLayer2)\n",
    "                    Features_comb[experiment_str]=pd.concat([df_ASD_subgrp,df_TD_subgrp],axis=0)\n",
    "                # 2. Set ASDsevere vs ASDmild experiment\n",
    "                # experiment_str=\"{ASDsevere_name} vs {ASDmild_name} >> {feature_type}\".format(ASDsevere_name='df_feature_moderatehigh_CSS',ASDmild_name='df_feature_lowMinimal_CSS',feature_type=k_FeatTypeLayer2)\n",
    "                # df_ASDsevere_subgrp=df_feauture_ASDgrp_dict['df_feature_moderatehigh_CSS'].copy()\n",
    "                # df_ASDmild_subgrp=df_feauture_ASDgrp_dict['df_feature_lowMinimal_CSS'].copy()\n",
    "                # df_ASDsevere_subgrp['ASDsevereMild']=sellect_people_define.ASDsevereMild_label['ASDsevere']\n",
    "                # df_ASDmild_subgrp['ASDsevereMild']=sellect_people_define.ASDsevereMild_label['ASDmild']\n",
    "                # Features_comb[experiment_str]=pd.concat([df_ASDsevere_subgrp,df_ASDmild_subgrp],axis=0)\n",
    "        self.Features_comb_multi=Features_comb\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Feature merging function\n",
    "    \n",
    "    Ths slice of code provide user to manually make functions to combine df_XXX_infos\n",
    "\n",
    "'''\n",
    "# =============================================================================\n",
    "\n",
    "ados_ds=ADOSdataset(knn_weights,knn_neighbors,Reorder_type,FeatureComb_mode=args.FeatureComb_mode)\n",
    "ErrorFeat_bookeep=Dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LOC_columns+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29959/3711900515.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mcomparison_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" >> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mModuleColumn_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" >> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mfeaturesOfInterest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModuleColumn_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModuleColumn_str\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;31m# feat_=key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeat_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeaturesOfInterest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'LOC_columns+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols'"
     ]
    }
   ],
   "source": [
    "if ados_ds.FeatureComb_mode == \"Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation\":\n",
    "    FeatureLabelMatch_manual=[\n",
    "\n",
    "        ['TD vs df_feature_lowMinimal_CSS >> LOC_columns+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols', 'ASDTD'],\n",
    "        ['TD vs df_feature_lowMinimal_CSS >> LOC_columns+Phonation_Trend_K_cols+Phonation_Syncrony_cols', 'ASDTD'],\n",
    "        ['TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols', 'ASDTD'],\n",
    "        ['TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols', 'ASDTD'],\n",
    "\n",
    "\n",
    "        ['TD vs df_feature_moderate_CSS >> LOCDEP_Trend_D_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "        ['TD vs df_feature_moderate_CSS >> Phonation_Proximity_cols', 'ASDTD'],\n",
    "        \n",
    "\n",
    "        ['TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols', 'ASDTD'],\n",
    "        ['TD vs df_feature_high_CSS >> DEP_columns+Phonation_Trend_D_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "        ['TD vs df_feature_high_CSS >> Phonation_Trend_D_cols+Phonation_Proximity_cols', 'ASDTD'],\n",
    "        ['TD vs df_feature_high_CSS >> DEP_columns+Phonation_Proximity_cols', 'ASDTD'],\n",
    "        ['TD vs df_feature_high_CSS >> Phonation_Proximity_cols', 'ASDTD'],\n",
    "    ]\n",
    "\n",
    "    \n",
    "# FeatSel 掌管該出現的columns\n",
    "# ados_ds.Features_comb_multi 掌管load進來的data\n",
    "\n",
    "\n",
    "Top_ModuleColumn_mapping_dict={}\n",
    "Top_ModuleColumn_mapping_dict['Add_UttLvl_feature']={ e2_str:FeatSel.Columns_comb2[e_str][e2_str] for e_str in FeatSel.Columns_comb2.keys() for e2_str in FeatSel.Columns_comb2[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb3']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb3[e_str][e2_str] for e_str in FeatSel.Columns_comb3.keys() for e2_str in FeatSel.Columns_comb3[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb5']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb5[e_str][e2_str] for e_str in FeatSel.Columns_comb5.keys() for e2_str in FeatSel.Columns_comb5[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb6']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb6[e_str][e2_str] for e_str in FeatSel.Columns_comb6.keys() for e2_str in FeatSel.Columns_comb6[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb7']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb7[e_str][e2_str] for e_str in FeatSel.Columns_comb7.keys() for e2_str in FeatSel.Columns_comb7[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['Comb_dynPhonation']=ModuleColumn_mapping={ e2_str:FeatSel.Comb_dynPhonation[e_str][e2_str] for e_str in FeatSel.Comb_dynPhonation.keys() for e2_str in FeatSel.Comb_dynPhonation[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation']=ModuleColumn_mapping={ e2_str:FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation[e_str][e2_str] for e_str in FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation.keys() for e2_str in FeatSel.Comb_staticLOCDEP_dynamicLOCDEP_dynamicphonation[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['feat_comb']=ModuleColumn_mapping={ e2_str:FeatSel.Columns_comb[e_str][e2_str] for e_str in FeatSel.Columns_comb.keys() for e2_str in FeatSel.Columns_comb[e_str].keys()}\n",
    "Top_ModuleColumn_mapping_dict['baselineFeats']=ModuleColumn_mapping={ e2_str:FeatSel.Baseline_comb[e_str][e2_str] for e_str in FeatSel.Baseline_comb.keys() for e2_str in FeatSel.Baseline_comb[e_str].keys()}\n",
    "\n",
    "\n",
    "ModuleColumn_mapping=Top_ModuleColumn_mapping_dict[args.FeatureComb_mode]\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Here starts to load features to Session_level_all dict\n",
    "\n",
    "'''\n",
    "# =============================================================================\n",
    "for exp_str, lab_ in FeatureLabelMatch_manual:\n",
    "    comparison_pair=exp_str.split(\" >> \")[0]\n",
    "    ModuleColumn_str=exp_str.split(\" >> \")[-1]\n",
    "    featuresOfInterest=[ModuleColumn_mapping[ModuleColumn_str]]\n",
    "    # feat_=key\n",
    "    for feat_col in featuresOfInterest:\n",
    "        feat_col_ = list(feat_col) # ex: ['MSB_f1']\n",
    "        if len(feat_col) > 144: # 144 is the limit of the filename\n",
    "            key=feat_col_\n",
    "        else:\n",
    "            key=[feat_col_]\n",
    "        # X,y, featType=ados_ds.Get_FormantAUI_feat(\\\n",
    "        #     label_choose=lab_,pickle_path='',featuresOfInterest=feat_col_,filterbyNum=False,\\\n",
    "        #     feat_=ados_ds.Features_comb_single[feat_])\n",
    "        \n",
    "        X,y, featType=ados_ds.Get_FormantAUI_feat(\\\n",
    "            label_choose=lab_,pickle_path='',featuresOfInterest=feat_col,filterbyNum=False,\\\n",
    "            feat_=ados_ds.Features_comb_multi[exp_str])\n",
    "        \n",
    "        if X.isnull().values.any() or y.isnull().values.any():\n",
    "            print(\"Feat: \",key,'Contains nan')\n",
    "            ErrorFeat_bookeep['{0} {1} {2}'.format(exp_str,lab_,key)].X=X\n",
    "            ErrorFeat_bookeep['{0} {1} {2}'.format(exp_str,lab_,key)].y=y\n",
    "            continue\n",
    "        \n",
    "        Item_name=\"{feat}::{lab}\".format(feat=' >> '.join([comparison_pair,ModuleColumn_str]),lab=lab_)\n",
    "        Session_level_all[Item_name].X, \\\n",
    "            Session_level_all[Item_name].y, \\\n",
    "                Session_level_all[Item_name].feattype = X,y, featType\n",
    "\n",
    "# =============================================================================\n",
    "# Model parameters\n",
    "# =============================================================================\n",
    "# C_variable=np.array([0.0001, 0.01, 0.1,0.5,1.0,10.0, 50.0, 100.0, 1000.0])\n",
    "# C_variable=np.array([0.01, 0.1,0.5,1.0,10.0, 50.0, 100.0])\n",
    "# C_variable=np.array(np.arange(0.1,1.5,0.1))\n",
    "C_variable=np.array([0.001,0.01,0.1,1,5,10.0,25,50,75,100])\n",
    "# C_variable=np.array([0.001,0.01,10.0,50,100] + list(np.arange(0.1,1.5,0.2))  )\n",
    "# C_variable=np.array([0.01, 0.1,0.5,1.0, 5.0])\n",
    "n_estimator=[ 32, 50, 64, 100 ,128, 256]\n",
    "\n",
    "'''\n",
    "\n",
    "    Classifier\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "# This is the closest \n",
    "Classifier={}\n",
    "Classifier['SVC']={'model':sklearn.svm.SVC(),\\\n",
    "                  'parameters':{'model__random_state':[1],\\\n",
    "                      'model__C':C_variable,\\\n",
    "                    'model__kernel': ['rbf'],\\\n",
    "                      # 'model__gamma':['auto'],\\\n",
    "                    'model__probability':[True],\\\n",
    "                                }}\n",
    "\n",
    "   \n",
    "\n",
    "loo=LeaveOneOut()\n",
    "# CV_settings=loo\n",
    "CV_settings=10\n",
    "\n",
    "# =============================================================================\n",
    "# Outputs\n",
    "Best_predict_optimize={}\n",
    "\n",
    "df_best_result_r2=pd.DataFrame([])\n",
    "df_best_result_pear=pd.DataFrame([])\n",
    "df_best_result_spear=pd.DataFrame([])\n",
    "df_best_cross_score=pd.DataFrame([])\n",
    "df_best_result_UAR=pd.DataFrame([])\n",
    "df_best_result_AUC=pd.DataFrame([])\n",
    "df_best_result_f1=pd.DataFrame([])\n",
    "df_best_result_allThreeClassifiers=pd.DataFrame([])\n",
    "# =============================================================================\n",
    "Result_path=\"RESULTS/\"\n",
    "if not os.path.exists(Result_path):\n",
    "    os.makedirs(Result_path)\n",
    "final_result_file=\"_ADOS_{}.xlsx\".format(args.suffix)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "count=0\n",
    "OutFeature_dict=Dict()\n",
    "Best_param_dict=Dict()\n",
    "sellect_people_define=SellectP_define()\n",
    "\n",
    "# ''' 要手動執行一次從Incorrect2Correct_indexes和Correct2Incorrect_indexes決定哪些indexes 需要算shap value 再在這邊指定哪些fold需要停下來算SHAP value '''\n",
    "# SHAP_inspect_idxs_manual=None # None means calculate SHAP value of all people\n",
    "# SHAP_inspect_idxs_manual=[15] # None means calculate SHAP value of all people\n",
    "SHAP_inspect_idxs_manual=[] # empty list means we do not execute shap function\n",
    "# SHAP_inspect_idxs_manual=sorted(list(set([14, 21]+[]+[24, 28, 30, 31, 39, 41, 45]+[22, 23, 27, 47, 58]+[6, 13, 19, 23, 24, 25]+[28, 35, 38, 45])))\n",
    "\n",
    "for clf_keys, clf in Classifier.items(): #Iterate among different classifiers \n",
    "    writer_clf = pd.ExcelWriter(Result_path+\"/\"+clf_keys+\"_\"+args.Feature_mode+\"_\"+final_result_file, engine = 'xlsxwriter')\n",
    "    for feature_lab_str, features in Session_level_all.items():\n",
    "\n",
    "        feature_keys, label_keys= feature_lab_str.split(\"::\")\n",
    "        feature_rawname=feature_keys[feature_keys.find('-')+1:]\n",
    "        # if feature_rawname in paper_name_map.keys():\n",
    "        #     featurename_paper=paper_name_map[feature_rawname]\n",
    "        #     feature_keys=feature_keys.replace(feature_rawname,featurename_paper)\n",
    "        \n",
    "        if SHAP_inspect_idxs_manual != None:\n",
    "            SHAP_inspect_idxs=SHAP_inspect_idxs_manual\n",
    "        else:\n",
    "            SHAP_inspect_idxs=range(len(features.y))\n",
    "        \n",
    "        Labels = Session_level_all.X[feature_keys]\n",
    "        print(\"=====================Cross validation start==================\")\n",
    "        pipe = Pipeline(steps=[('scalar',StandardScaler()),(\"model\", clf['model'])])\n",
    "        p_grid=clf['parameters']\n",
    "        Gclf = GridSearchCV(estimator=pipe, param_grid=p_grid, scoring=args.selectModelScoring, cv=CV_settings, refit=True, n_jobs=-1)\n",
    "        Gclf_manual = GridSearchCV(estimator=pipe, param_grid=p_grid, scoring=args.selectModelScoring, cv=CV_settings, refit=True, n_jobs=-1)\n",
    "        \n",
    "        \n",
    "        # The cv is as the one in cross_val_predict function\n",
    "        cv = sklearn.model_selection.check_cv(CV_settings,features.y,classifier=sklearn.base.is_classifier(Gclf))\n",
    "        splits = list(cv.split(features.X, features.y, groups=None))\n",
    "        test_indices = np.concatenate([test for _, test in splits])\n",
    "\n",
    "        CVpredict_manual=np.zeros(len(features.y))\n",
    "        for i, (train_index, test_index) in enumerate(splits):\n",
    "            X_train, X_test = features.X.iloc[train_index], features.X.iloc[test_index]\n",
    "            y_train, y_test = features.y.iloc[train_index], features.y.iloc[test_index]\n",
    "            Gclf_manual.fit(X_train,y_train)\n",
    "            Gclf_manual_predict=Gclf_manual.predict(X_test)\n",
    "            ##################################################################\n",
    "            #確認predict proba會跟prediction有match到\n",
    "            #原本的SVM的predict proba (https://github.com/scikit-learn/scikit-learn/issues/13211)\n",
    "            # the normal classifiers' predictions are based on decision_function values\n",
    "            ##################################################################\n",
    "            calibrated_bestEst=CalibratedClassifierCV(\n",
    "                base_estimator=Gclf_manual.best_estimator_,\n",
    "                cv=\"prefit\"\n",
    "                )\n",
    "            calibrated_bestEst.fit(X_train,y_train)\n",
    "            result_bestmodel=calibrated_bestEst.predict(X_test)\n",
    "            ##################################################################\n",
    "\n",
    "            decisFunc=Gclf_manual.decision_function(X_test)\n",
    "            # predict_proba 是把SVM的output另外fit一個ligistic regression 來smooth他的output到0~1之間\n",
    "            # https://splunktool.com/why-is-the-result-of-sklearnsvmsvcpredict-inconsistent-with-sklearnsvmsvcpredictproba\n",
    "\n",
    "            # 測試calibrated predict有沒有等於原本的predict\n",
    "            # assert (Gclf_manual_predict == result_bestmodel).all()\n",
    "            \n",
    "            # ASSERT based on decision function\n",
    "            # decision function的unit test\n",
    "            decisFunc_pred=np.ones(len(result_bestmodel)).astype(int)\n",
    "            decisFunc_pred[decisFunc >0]=2\n",
    "            assert (Gclf_manual_predict == decisFunc_pred).all()\n",
    "            # from sklearn.calibration import calibration_curve\n",
    "            # calibration_curve(Gclf_manual.decision_function(X_test))\n",
    "            ##################################################################\n",
    "            CVpredict_manual[test_index]=Gclf_manual_predict\n",
    "            # CVpred_fromFunction=CVpredict[test_index]\n",
    "            \n",
    "            # SHAP value generating\n",
    "            # logit_number=0\n",
    "            # inspect_sample=0\n",
    "            # If the indexes we want to examine are in that fold, store the whole fold\n",
    "            # 先把整個fold記錄下來然後在analysis area再拆解\n",
    "            SHAP_exam_lst=[i for i in test_index if i in SHAP_inspect_idxs]\n",
    "            if len(SHAP_exam_lst) != 0:\n",
    "                explainer = shap.KernelExplainer(Gclf_manual.decision_function, X_train)\n",
    "                shap_values = explainer.shap_values(X_test)\n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].explainer_expected_value=explainer.expected_value\n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].shap_values=shap_values # shap_values= [logit, index, feature]\n",
    "                Session_level_all[feature_lab_str]['SHAP_info']['_'.join(test_index.astype(str))].XTest=X_test\n",
    "                Session_level_all[feature_lab_str]['Logit{}_predictproba'.format(logit_number)]['_'.join(test_index.astype(str))].predictproba=Gclf_manual.predict_proba(X_test)[:,logit_number]\n",
    "                Session_level_all[feature_lab_str]['Logit{}_predictproba'.format(logit_number)]['_'.join(test_index.astype(str))].decisionfunc=Gclf_manual.decision_function(X_test)\n",
    "            \n",
    "        CVpredict = CVpredict_manual\n",
    "        \n",
    "        Session_level_all[feature_lab_str]['y_pred']=CVpredict_manual\n",
    "        Session_level_all[feature_lab_str]['y_true']=features.y\n",
    "        \n",
    "        Gclf.fit(features.X,features.y)\n",
    "        if clf_keys == \"EN\":\n",
    "            print('The coefficient of best estimator is: ',Gclf.best_estimator_.coef_)\n",
    "        \n",
    "        print(\"The best score with scoring parameter: 'r2' is\", Gclf.best_score_)\n",
    "        print(\"The best parameters are :\", Gclf.best_params_)\n",
    "        best_parameters=Gclf.best_params_\n",
    "        best_score=Gclf.best_score_\n",
    "        best_parameters.update({'best_score':best_score})\n",
    "        Best_param_dict[feature_lab_str]=best_parameters\n",
    "        cv_results_info=Gclf.cv_results_\n",
    "\n",
    "        num_ASD=len(np.where(features.y==sellect_people_define.ASDTD_label['ASD'])[0])\n",
    "        num_TD=len(np.where(features.y==sellect_people_define.ASDTD_label['TD'])[0])\n",
    "        \n",
    "        if features.feattype == 'regression':\n",
    "            r2=r2_score(features.y,CVpredict )\n",
    "            n,p=features.X.shape\n",
    "            r2_adj=1-(1-r2)*(n-1)/(n-p-1)\n",
    "            pearson_result, pearson_p=pearsonr(features.y,CVpredict )\n",
    "            spear_result, spearman_p=spearmanr(features.y,CVpredict )\n",
    "            print('Feature {0}, label {1} ,spear_result {2}'.format(feature_keys, label_keys,spear_result))\n",
    "        elif features.feattype == 'classification':\n",
    "            n,p=features.X.shape\n",
    "            CM=confusion_matrix(features.y, CVpredict)\n",
    "            Session_level_all[feature_lab_str]['Confusion_matrix']=pd.DataFrame(CM,\\\n",
    "                                                                    index=['y_true_{}'.format(ii) for ii in range(CM.shape[0])],\\\n",
    "                                                                    columns=['y_pred_{}'.format(ii) for ii in range(CM.shape[1])])\n",
    "            UAR=recall_score(features.y, CVpredict, average='macro')\n",
    "            AUC=roc_auc_score(features.y, CVpredict)\n",
    "            f1Score=f1_score(features.y, CVpredict, average='macro')\n",
    "            print('Feature {0}, label {1} ,UAR {2}'.format(feature_keys, label_keys,UAR))\n",
    "            \n",
    "        if args.Plot and p <2:\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 10), sharey=True)\n",
    "            kernel_label = [clf_keys]\n",
    "            model_color = ['m']\n",
    "            # axes.plot((features.X - min(features.X) )/ max(features.X), Gclf.best_estimator_.fit(features.X,features.y).predict(features.X), color=model_color[0],\n",
    "            #               label='CV Predict')\n",
    "            axes.scatter((features.X.values - min(features.X.values) )/ max(features.X.values), CVpredict, \n",
    "                         facecolor=\"none\", edgecolor=\"k\", s=150,\n",
    "                         label='{}'.format(feature_lab_str)\n",
    "                         )\n",
    "            axes.scatter((features.X.values - min(features.X.values) )/ max(features.X.values), features.y, \n",
    "                         facecolor=\"none\", edgecolor=\"r\", s=50,\n",
    "                         label='Real Y')\n",
    "            axes.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
    "                    ncol=1, fancybox=True, shadow=True)\n",
    "            \n",
    "            Plot_path='./Plot/LinearRegress/'\n",
    "            if not os.path.exists(Plot_path):\n",
    "                os.makedirs(Plot_path)\n",
    "            plot_file=Plot_path+\"/{0}_{1}.png\".format(clf_keys,feature_lab_str)\n",
    "            plt.savefig(plot_file, dpi=200) \n",
    "        \n",
    "        # =============================================================================\n",
    "        '''\n",
    "            Inspect the best result\n",
    "        '''\n",
    "        # =============================================================================\n",
    "        Best_predict_optimize[label_keys]=pd.DataFrame(np.vstack((CVpredict,features.y)).T,columns=['y_pred','y'])\n",
    "        excel_path='./Statistics/prediction_result'\n",
    "        if not os.path.exists(excel_path):\n",
    "            os.makedirs(excel_path)\n",
    "        excel_file=excel_path+\"/{0}_{1}.xlsx\"\n",
    "        writer = pd.ExcelWriter(excel_file.format(clf_keys,feature_keys.replace(\":\",\"\")), engine = 'xlsxwriter')\n",
    "        for label_name in  Best_predict_optimize.keys():\n",
    "            Best_predict_optimize[label_name].to_excel(writer,sheet_name=label_name.replace(\"/\",\"_\"))\n",
    "        writer.save()\n",
    "                                \n",
    "        # ================================================      =============================\n",
    "        if features.feattype == 'regression':\n",
    "            df_best_result_r2.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(r2_adj,3),np.round(np.nan,6))\n",
    "            df_best_result_pear.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(pearson_result,3),np.round(pearson_p,6))\n",
    "            df_best_result_spear.loc[feature_keys,label_keys]='{0}/{1}'.format(np.round(spear_result,3),np.round(spearman_p,6))\n",
    "            df_best_result_spear.loc[feature_keys,'de-zero_num']=len(features.X)\n",
    "            # df_best_cross_score.loc[feature_keys,label_keys]=Score.mean()\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1} (R2adj/pear/spear)'.format(label_keys,clf_keys)]\\\n",
    "                        ='{0}/{1}/{2}'.format(np.round(r2_adj,3),np.round(pearson_result,3),np.round(spear_result,3))\n",
    "\n",
    "        elif features.feattype == 'classification':\n",
    "            df_best_result_UAR.loc[feature_keys,label_keys]='{0}'.format(UAR)\n",
    "            df_best_result_AUC.loc[feature_keys,label_keys]='{0}'.format(AUC)\n",
    "            df_best_result_f1.loc[feature_keys,label_keys]='{0}'.format(f1Score)\n",
    "            # df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1} (UAR/AUC/f1score)'.format(label_keys,clf_keys)]\\\n",
    "            #             ='{0}/{1}/{2}'.format(np.round(UAR,3),np.round(AUC,3),np.round(f1Score,3))\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'{0}/{1}'.format(label_keys,clf_keys)]\\\n",
    "                        ='{0}'.format(np.round(UAR,3))\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'num_ASD']\\\n",
    "                        ='{0}'.format(num_ASD)\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'num_TD']\\\n",
    "                        ='{0}'.format(num_TD)\n",
    "            df_best_result_allThreeClassifiers.loc[feature_keys,'f1']\\\n",
    "                        ='{0}'.format(np.round(f1Score,3))\n",
    "        count+=1\n",
    "    if features.feattype == 'regression':\n",
    "        df_best_result_r2.to_excel(writer_clf,sheet_name=\"R2_adj\")\n",
    "        df_best_result_pear.to_excel(writer_clf,sheet_name=\"pear\")\n",
    "        df_best_result_spear.to_excel(writer_clf,sheet_name=\"spear\")\n",
    "        df_best_result_spear.to_csv(Result_path+\"/\"+clf_keys+\"_\"+args.Feature_mode+\"_spearman.csv\")\n",
    "    elif features.feattype == 'classification':\n",
    "        df_best_result_UAR.to_excel(writer_clf,sheet_name=\"UAR\")\n",
    "        df_best_result_AUC.to_excel(writer_clf,sheet_name=\"AUC\")\n",
    "        df_best_result_f1.to_excel(writer_clf,sheet_name=\"f1\")\n",
    "\n",
    "# TASLP table3.\n",
    "writer_clf.save()\n",
    "df_best_result_allThreeClassifiers.to_excel(Result_path+\"/\"+\"Classification_\"+args.Feature_mode+\"_3clsferRESULT.xlsx\")\n",
    "print(df_best_result_allThreeClassifiers)\n",
    "\n",
    "\n",
    "# Change to paper name\n",
    "df_allThreeClassifiers_paperName=df_best_result_allThreeClassifiers.copy()\n",
    "index_bag=[]\n",
    "for exp_str in df_best_result_allThreeClassifiers.index:\n",
    "    experiment_name, feature_name=exp_str.split(\" >> \")\n",
    "    paper_idx='+'.join([Swap2PaperName(n, PprNmeMp) for n in feature_name.split(\"+\")])\n",
    "    index_bag.append(paper_idx)\n",
    "df_allThreeClassifiers_paperName.index=index_bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC_columns': ['VSA2',\n",
       "  'FCR2',\n",
       "  'between_covariance_norm(A:,i:,u:)',\n",
       "  'between_variance_norm(A:,i:,u:)',\n",
       "  'total_covariance_norm(A:,i:,u:)',\n",
       "  'total_variance_norm(A:,i:,u:)',\n",
       "  'sam_wilks_lin_norm(A:,i:,u:)',\n",
       "  'pillai_lin_norm(A:,i:,u:)',\n",
       "  'hotelling_lin_norm(A:,i:,u:)',\n",
       "  'roys_root_lin_norm(A:,i:,u:)',\n",
       "  'Between_Within_Det_ratio_norm(A:,i:,u:)',\n",
       "  'Between_Within_Tr_ratio_norm(A:,i:,u:)'],\n",
       " 'DEP_columns': ['pear_12', 'spear_12', 'kendall_12', 'dcorr_12'],\n",
       " 'LOCDEP_columns': ['VSA2',\n",
       "  'FCR2',\n",
       "  'between_covariance_norm(A:,i:,u:)',\n",
       "  'between_variance_norm(A:,i:,u:)',\n",
       "  'total_covariance_norm(A:,i:,u:)',\n",
       "  'total_variance_norm(A:,i:,u:)',\n",
       "  'sam_wilks_lin_norm(A:,i:,u:)',\n",
       "  'pillai_lin_norm(A:,i:,u:)',\n",
       "  'hotelling_lin_norm(A:,i:,u:)',\n",
       "  'roys_root_lin_norm(A:,i:,u:)',\n",
       "  'Between_Within_Det_ratio_norm(A:,i:,u:)',\n",
       "  'Between_Within_Tr_ratio_norm(A:,i:,u:)',\n",
       "  'pear_12',\n",
       "  'spear_12',\n",
       "  'kendall_12',\n",
       "  'dcorr_12'],\n",
       " 'LOC_columns_Intra': ['within_covariance_norm(A:,i:,u:)',\n",
       "  'within_variance_norm(A:,i:,u:)'],\n",
       " 'Phonation_Trend_D_cols': ['Trend[intensity_mean_mean(A:,i:,u:)]_d',\n",
       "  'Trend[meanF0_mean(A:,i:,u:)]_d',\n",
       "  'Trend[stdevF0_mean(A:,i:,u:)]_d',\n",
       "  'Trend[hnr_mean(A:,i:,u:)]_d',\n",
       "  'Trend[localJitter_mean(A:,i:,u:)]_d',\n",
       "  'Trend[localabsoluteJitter_mean(A:,i:,u:)]_d',\n",
       "  'Trend[localShimmer_mean(A:,i:,u:)]_d',\n",
       "  'Trend[localdbShimmer_mean(A:,i:,u:)]_d',\n",
       "  'Trend[intensity_mean_max(A:,i:,u:)]_d',\n",
       "  'Trend[meanF0_max(A:,i:,u:)]_d',\n",
       "  'Trend[stdevF0_max(A:,i:,u:)]_d',\n",
       "  'Trend[hnr_max(A:,i:,u:)]_d',\n",
       "  'Trend[localJitter_max(A:,i:,u:)]_d',\n",
       "  'Trend[localabsoluteJitter_max(A:,i:,u:)]_d',\n",
       "  'Trend[localShimmer_max(A:,i:,u:)]_d',\n",
       "  'Trend[localdbShimmer_max(A:,i:,u:)]_d'],\n",
       " 'Phonation_Trend_K_cols': ['Trend[intensity_mean_mean(A:,i:,u:)]_k',\n",
       "  'Trend[meanF0_mean(A:,i:,u:)]_k',\n",
       "  'Trend[stdevF0_mean(A:,i:,u:)]_k',\n",
       "  'Trend[hnr_mean(A:,i:,u:)]_k',\n",
       "  'Trend[localJitter_mean(A:,i:,u:)]_k',\n",
       "  'Trend[localabsoluteJitter_mean(A:,i:,u:)]_k',\n",
       "  'Trend[localShimmer_mean(A:,i:,u:)]_k',\n",
       "  'Trend[localdbShimmer_mean(A:,i:,u:)]_k',\n",
       "  'Trend[intensity_mean_max(A:,i:,u:)]_k',\n",
       "  'Trend[meanF0_max(A:,i:,u:)]_k',\n",
       "  'Trend[stdevF0_max(A:,i:,u:)]_k',\n",
       "  'Trend[hnr_max(A:,i:,u:)]_k',\n",
       "  'Trend[localJitter_max(A:,i:,u:)]_k',\n",
       "  'Trend[localabsoluteJitter_max(A:,i:,u:)]_k',\n",
       "  'Trend[localShimmer_max(A:,i:,u:)]_k',\n",
       "  'Trend[localdbShimmer_max(A:,i:,u:)]_k'],\n",
       " 'Phonation_Proximity_cols': ['Proximity[intensity_mean_mean(A:,i:,u:)]',\n",
       "  'Proximity[meanF0_mean(A:,i:,u:)]',\n",
       "  'Proximity[stdevF0_mean(A:,i:,u:)]',\n",
       "  'Proximity[hnr_mean(A:,i:,u:)]',\n",
       "  'Proximity[localJitter_mean(A:,i:,u:)]',\n",
       "  'Proximity[localabsoluteJitter_mean(A:,i:,u:)]',\n",
       "  'Proximity[localShimmer_mean(A:,i:,u:)]',\n",
       "  'Proximity[localdbShimmer_mean(A:,i:,u:)]',\n",
       "  'Proximity[intensity_mean_var(A:,i:,u:)]',\n",
       "  'Proximity[intensity_mean_max(A:,i:,u:)]',\n",
       "  'Proximity[meanF0_max(A:,i:,u:)]',\n",
       "  'Proximity[stdevF0_max(A:,i:,u:)]',\n",
       "  'Proximity[hnr_max(A:,i:,u:)]',\n",
       "  'Proximity[localJitter_max(A:,i:,u:)]',\n",
       "  'Proximity[localabsoluteJitter_max(A:,i:,u:)]',\n",
       "  'Proximity[localShimmer_max(A:,i:,u:)]',\n",
       "  'Proximity[localdbShimmer_max(A:,i:,u:)]'],\n",
       " 'Phonation_Convergence_cols': ['Convergence[intensity_mean_mean(A:,i:,u:)]',\n",
       "  'Convergence[meanF0_mean(A:,i:,u:)]',\n",
       "  'Convergence[stdevF0_mean(A:,i:,u:)]',\n",
       "  'Convergence[hnr_mean(A:,i:,u:)]',\n",
       "  'Convergence[localJitter_mean(A:,i:,u:)]',\n",
       "  'Convergence[localabsoluteJitter_mean(A:,i:,u:)]',\n",
       "  'Convergence[localShimmer_mean(A:,i:,u:)]',\n",
       "  'Convergence[localdbShimmer_mean(A:,i:,u:)]',\n",
       "  'Convergence[intensity_mean_max(A:,i:,u:)]',\n",
       "  'Convergence[meanF0_max(A:,i:,u:)]',\n",
       "  'Convergence[stdevF0_max(A:,i:,u:)]',\n",
       "  'Convergence[hnr_max(A:,i:,u:)]',\n",
       "  'Convergence[localJitter_max(A:,i:,u:)]',\n",
       "  'Convergence[localabsoluteJitter_max(A:,i:,u:)]',\n",
       "  'Convergence[localShimmer_max(A:,i:,u:)]',\n",
       "  'Convergence[localdbShimmer_max(A:,i:,u:)]'],\n",
       " 'Phonation_Syncrony_cols': ['Syncrony[intensity_mean_mean(A:,i:,u:)]',\n",
       "  'Syncrony[meanF0_mean(A:,i:,u:)]',\n",
       "  'Syncrony[stdevF0_mean(A:,i:,u:)]',\n",
       "  'Syncrony[hnr_mean(A:,i:,u:)]',\n",
       "  'Syncrony[localJitter_mean(A:,i:,u:)]',\n",
       "  'Syncrony[localabsoluteJitter_mean(A:,i:,u:)]',\n",
       "  'Syncrony[localShimmer_mean(A:,i:,u:)]',\n",
       "  'Syncrony[localdbShimmer_mean(A:,i:,u:)]',\n",
       "  'Syncrony[intensity_mean_max(A:,i:,u:)]',\n",
       "  'Syncrony[meanF0_max(A:,i:,u:)]',\n",
       "  'Syncrony[stdevF0_max(A:,i:,u:)]',\n",
       "  'Syncrony[hnr_max(A:,i:,u:)]',\n",
       "  'Syncrony[localJitter_max(A:,i:,u:)]',\n",
       "  'Syncrony[localabsoluteJitter_max(A:,i:,u:)]',\n",
       "  'Syncrony[localShimmer_max(A:,i:,u:)]',\n",
       "  'Syncrony[localdbShimmer_max(A:,i:,u:)]'],\n",
       " 'LOCDEP_Proximity_cols': ['Proximity[VSA2]',\n",
       "  'Proximity[FCR2]',\n",
       "  'Proximity[between_covariance_norm(A:,i:,u:)]',\n",
       "  'Proximity[between_variance_norm(A:,i:,u:)]',\n",
       "  'Proximity[within_covariance_norm(A:,i:,u:)]',\n",
       "  'Proximity[within_variance_norm(A:,i:,u:)]',\n",
       "  'Proximity[total_covariance_norm(A:,i:,u:)]',\n",
       "  'Proximity[total_variance_norm(A:,i:,u:)]',\n",
       "  'Proximity[sam_wilks_lin_norm(A:,i:,u:)]',\n",
       "  'Proximity[pillai_lin_norm(A:,i:,u:)]',\n",
       "  'Proximity[hotelling_lin_norm(A:,i:,u:)]',\n",
       "  'Proximity[roys_root_lin_norm(A:,i:,u:)]',\n",
       "  'Proximity[Between_Within_Det_ratio_norm(A:,i:,u:)]',\n",
       "  'Proximity[Between_Within_Tr_ratio_norm(A:,i:,u:)]',\n",
       "  'Proximity[pear_12]',\n",
       "  'Proximity[spear_12]',\n",
       "  'Proximity[kendall_12]',\n",
       "  'Proximity[dcorr_12]'],\n",
       " 'LOCDEP_Trend_D_cols': ['Trend[VSA2]_d',\n",
       "  'Trend[FCR2]_d',\n",
       "  'Trend[between_covariance_norm(A:,i:,u:)]_d',\n",
       "  'Trend[between_variance_norm(A:,i:,u:)]_d',\n",
       "  'Trend[within_covariance_norm(A:,i:,u:)]_d',\n",
       "  'Trend[within_variance_norm(A:,i:,u:)]_d',\n",
       "  'Trend[total_covariance_norm(A:,i:,u:)]_d',\n",
       "  'Trend[total_variance_norm(A:,i:,u:)]_d',\n",
       "  'Trend[sam_wilks_lin_norm(A:,i:,u:)]_d',\n",
       "  'Trend[pillai_lin_norm(A:,i:,u:)]_d',\n",
       "  'Trend[hotelling_lin_norm(A:,i:,u:)]_d',\n",
       "  'Trend[roys_root_lin_norm(A:,i:,u:)]_d',\n",
       "  'Trend[Between_Within_Det_ratio_norm(A:,i:,u:)]_d',\n",
       "  'Trend[Between_Within_Tr_ratio_norm(A:,i:,u:)]_d',\n",
       "  'Trend[pear_12]_d',\n",
       "  'Trend[spear_12]_d',\n",
       "  'Trend[kendall_12]_d',\n",
       "  'Trend[dcorr_12]_d'],\n",
       " 'LOCDEP_Trend_K_cols': ['Trend[VSA2]_k',\n",
       "  'Trend[FCR2]_k',\n",
       "  'Trend[between_covariance_norm(A:,i:,u:)]_k',\n",
       "  'Trend[between_variance_norm(A:,i:,u:)]_k',\n",
       "  'Trend[within_covariance_norm(A:,i:,u:)]_k',\n",
       "  'Trend[within_variance_norm(A:,i:,u:)]_k',\n",
       "  'Trend[total_covariance_norm(A:,i:,u:)]_k',\n",
       "  'Trend[total_variance_norm(A:,i:,u:)]_k',\n",
       "  'Trend[sam_wilks_lin_norm(A:,i:,u:)]_k',\n",
       "  'Trend[pillai_lin_norm(A:,i:,u:)]_k',\n",
       "  'Trend[hotelling_lin_norm(A:,i:,u:)]_k',\n",
       "  'Trend[roys_root_lin_norm(A:,i:,u:)]_k',\n",
       "  'Trend[Between_Within_Det_ratio_norm(A:,i:,u:)]_k',\n",
       "  'Trend[Between_Within_Tr_ratio_norm(A:,i:,u:)]_k',\n",
       "  'Trend[pear_12]_k',\n",
       "  'Trend[spear_12]_k',\n",
       "  'Trend[kendall_12]_k',\n",
       "  'Trend[dcorr_12]_k'],\n",
       " 'LOCDEP_Convergence_cols': ['Convergence[VSA2]',\n",
       "  'Convergence[FCR2]',\n",
       "  'Convergence[between_covariance_norm(A:,i:,u:)]',\n",
       "  'Convergence[between_variance_norm(A:,i:,u:)]',\n",
       "  'Convergence[within_covariance_norm(A:,i:,u:)]',\n",
       "  'Convergence[within_variance_norm(A:,i:,u:)]',\n",
       "  'Convergence[total_covariance_norm(A:,i:,u:)]',\n",
       "  'Convergence[total_variance_norm(A:,i:,u:)]',\n",
       "  'Convergence[sam_wilks_lin_norm(A:,i:,u:)]',\n",
       "  'Convergence[pillai_lin_norm(A:,i:,u:)]',\n",
       "  'Convergence[hotelling_lin_norm(A:,i:,u:)]',\n",
       "  'Convergence[roys_root_lin_norm(A:,i:,u:)]',\n",
       "  'Convergence[Between_Within_Det_ratio_norm(A:,i:,u:)]',\n",
       "  'Convergence[Between_Within_Tr_ratio_norm(A:,i:,u:)]',\n",
       "  'Convergence[pear_12]',\n",
       "  'Convergence[spear_12]',\n",
       "  'Convergence[kendall_12]',\n",
       "  'Convergence[dcorr_12]'],\n",
       " 'LOCDEP_Syncrony_cols': ['Syncrony[VSA2]',\n",
       "  'Syncrony[FCR2]',\n",
       "  'Syncrony[between_covariance_norm(A:,i:,u:)]',\n",
       "  'Syncrony[between_variance_norm(A:,i:,u:)]',\n",
       "  'Syncrony[within_covariance_norm(A:,i:,u:)]',\n",
       "  'Syncrony[within_variance_norm(A:,i:,u:)]',\n",
       "  'Syncrony[total_covariance_norm(A:,i:,u:)]',\n",
       "  'Syncrony[total_variance_norm(A:,i:,u:)]',\n",
       "  'Syncrony[sam_wilks_lin_norm(A:,i:,u:)]',\n",
       "  'Syncrony[pillai_lin_norm(A:,i:,u:)]',\n",
       "  'Syncrony[hotelling_lin_norm(A:,i:,u:)]',\n",
       "  'Syncrony[roys_root_lin_norm(A:,i:,u:)]',\n",
       "  'Syncrony[Between_Within_Det_ratio_norm(A:,i:,u:)]',\n",
       "  'Syncrony[Between_Within_Tr_ratio_norm(A:,i:,u:)]',\n",
       "  'Syncrony[pear_12]',\n",
       "  'Syncrony[spear_12]',\n",
       "  'Syncrony[kendall_12]',\n",
       "  'Syncrony[dcorr_12]']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModuleColumn_mapping\n",
    "# ModuleColumn_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Analysis part\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def Organize_Needed_SHAP_info(Incorrect2Correct_indexes, Session_level_all, proposed_expstr):\n",
    "    Incorrect2Correct_info_dict=Dict()\n",
    "    for tst_idx in Incorrect2Correct_indexes:\n",
    "        for key, values in Session_level_all[proposed_expstr]['SHAP_info'].items():\n",
    "            test_fold_idx=[int(k) for k in key.split(\"_\")]\n",
    "            for i,ii in enumerate(test_fold_idx): #ii is the index of the sample, i is the position of this sample in this test fold\n",
    "                if tst_idx == ii:\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['XTest']=values['XTest'].iloc[i,:]\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['explainer_expected_value']=values['explainer_expected_value']\n",
    "                    \n",
    "                    # 因為之前都用predict_proba 但是自從發現predict_proba有BUG之後就改用decision function了，\n",
    "                    # 但是改成decision function之後資料結構會跟原來的不一樣，所以額外仿造了一個logit的values\n",
    "                    # shap_values_array=[array[i,:] for array in values['shap_values']]\n",
    "                    shap_values_array=[array[i,:] for array in [values['shap_values'],-values['shap_values']]]\n",
    "                    \n",
    "                    df_shap_values=pd.DataFrame(shap_values_array,columns=Incorrect2Correct_info_dict[tst_idx]['XTest'].index)\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['shap_values']=df_shap_values\n",
    "                    print(\"testing sample \", ii, \"is in the \", i, \"position of test fold\", key)\n",
    "                    assert (Incorrect2Correct_info_dict[tst_idx]['XTest'] == Session_level_all[proposed_expstr]['X'].iloc[tst_idx]).all()\n",
    "                    # print(\"It's feature value captured is\", Incorrect2Correct_info_dict[tst_idx]['XTest'])\n",
    "                    # print(\"It's original X value is\", Session_level_all[proposed_expstr]['X'].iloc[tst_idx])\n",
    "                    # print(\"See if they match\")\n",
    "    return Incorrect2Correct_info_dict\n",
    "def Get_Model_Type12Errors(model_str='baseline', tureLab_str='y_true'):\n",
    "    # Positive = ASD\n",
    "    Type1Err= ( df_Y_pred[model_str]  == sellect_people_define.ASDTD_label['ASD']) & ( df_Y_pred[tureLab_str] == sellect_people_define.ASDTD_label['TD']  )\n",
    "    Type2Err= ( df_Y_pred[model_str]  == sellect_people_define.ASDTD_label['TD'] ) & ( df_Y_pred[tureLab_str] == sellect_people_define.ASDTD_label['ASD']  )\n",
    "    return Type1Err, Type2Err\n",
    "# =============================================================================\n",
    "\n",
    "'''\n",
    "\n",
    "    Part 1: Check incorrect to correct and correct to incorrect\n",
    "\n",
    "'''\n",
    "\n",
    "if args.Print_Analysis_grp_Manual_select == True:\n",
    "    count=0\n",
    "    for exp_lst in FeatureLabelMatch_manual:\n",
    "        exp_lst_str='::'.join(exp_lst)\n",
    "        if count < len(FeatureLabelMatch_manual)/2:\n",
    "            print(\"proposed_expstr='{}'\".format(exp_lst_str))\n",
    "        else:\n",
    "            print(\"baseline_expstr='{}'\".format(exp_lst_str))\n",
    "        count+=1\n",
    "    \n",
    "\n",
    "############################################################\n",
    "# XXX\n",
    "# Low Minimal\n",
    "# proposed_expstr='TD vs df_feature_lowMinimal_CSS >> LOC_columns+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "# Reverse_exp=False\n",
    "# [14, 21]\n",
    "# []\n",
    "\n",
    "\n",
    "proposed_expstr='TD vs df_feature_lowMinimal_CSS >> LOC_columns+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "baseline_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "Reverse_exp=True\n",
    "[1, 14, 15, 21]\n",
    "[]\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_lowMinimal_CSS >> LOC_columns+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "# Reverse_exp=False\n",
    "# []\n",
    "# [1, 14, 15, 21]\n",
    "\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_lowMinimal_CSS >> LOC_columns+Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "# Reverse_exp=False\n",
    "# [12, 21]\n",
    "# [4 , 11]\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_lowMinimal_CSS >> Phonation_Trend_K_cols+Phonation_Syncrony_cols::ASDTD'\n",
    "# Reverse_exp=False\n",
    "# []\n",
    "# [1, 15]\n",
    "############################################################\n",
    "# Moderate\n",
    "# proposed_expstr='TD vs df_feature_moderate_CSS >> LOCDEP_Trend_D_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_moderate_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=False\n",
    "# [24, 28, 30, 31, 39, 41, 45]\n",
    "# [22, 23, 27, 47, 58]\n",
    "\n",
    "\n",
    "############################################################\n",
    "# high\n",
    "# proposed_expstr='TD vs df_feature_high_CSS >> DEP_columns+Phonation_Trend_D_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_high_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=False\n",
    "# [6, 13, 19, 23, 24, 25]\n",
    "# [28, 35, 38, 45]\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_high_CSS >> DEP_columns+Phonation_Trend_D_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_high_CSS >> Phonation_Trend_D_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# Reverse_exp=True\n",
    "\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_high_CSS >> Phonation_Trend_D_cols+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_high_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "# [13, 19, 23, 24, 25]\n",
    "# [2, 3, 28, 29, 35, 36, 38, 44]\n",
    "\n",
    "# proposed_expstr='TD vs df_feature_high_CSS >> DEP_columns+Phonation_Proximity_cols::ASDTD'\n",
    "# baseline_expstr='TD vs df_feature_high_CSS >> Phonation_Proximity_cols::ASDTD'\n",
    "experiment_title=baseline_expstr[re.search(\"df_feature_\",baseline_expstr).end():re.search(\"_CSS >> \",baseline_expstr).start()]\n",
    "\n",
    "proposed_featset_lst=proposed_expstr[re.search(\" >> \",proposed_expstr).end():re.search(\"::\",proposed_expstr).start()].split(\"+\")\n",
    "baseline_featset_lst=baseline_expstr[re.search(\" >> \",baseline_expstr).end():re.search(\"::\",baseline_expstr).start()].split(\"+\")\n",
    "Additional_featureSet=set(proposed_featset_lst) - set(baseline_featset_lst)\n",
    "print(\"For Task\", experiment_title, \" additional feature sets are\", Additional_featureSet)\n",
    "# =============================================================================\n",
    "# Error type analyses\n",
    "# =============================================================================\n",
    "# df_compare_pair=pd.DataFrame(list())\n",
    "Y_pred_lst=[\n",
    "Session_level_all[proposed_expstr]['y_pred'],\n",
    "Session_level_all[baseline_expstr]['y_pred'],\n",
    "Session_level_all[proposed_expstr]['y_true'],\n",
    "Session_level_all[proposed_expstr]['y_true'].index,\n",
    "]\n",
    "assert (Session_level_all[proposed_expstr]['y_true'] == Session_level_all[baseline_expstr]['y_true']).all()\n",
    "\n",
    "df_Y_pred=pd.DataFrame(Y_pred_lst[:-1],index=['proposed','baseline','y_true']).T\n",
    "df_Y_pred_withName=pd.DataFrame(Y_pred_lst,index=['proposed','baseline','y_true','name']).T\n",
    "df_Index2Name_mapping=df_Y_pred_withName['name']\n",
    "\n",
    "\n",
    "Incorrect=df_Y_pred['baseline'] != df_Y_pred['y_true']\n",
    "Correct=df_Y_pred['proposed'] == df_Y_pred['y_true']\n",
    "Incorrect2Correct= Correct & Incorrect\n",
    "\n",
    "\n",
    "Incorrect=df_Y_pred['baseline'] == df_Y_pred['y_true']\n",
    "Correct=df_Y_pred['proposed'] != df_Y_pred['y_true']\n",
    "Correct2Incorrect= Correct & Incorrect\n",
    "\n",
    "Incorrect2Correct_indexes=list(df_Y_pred[Incorrect2Correct].index)\n",
    "Correct2Incorrect_indexes=list(df_Y_pred[Correct2Incorrect].index)\n",
    "print('Incorrect2Correct_indexes: ', Incorrect2Correct_indexes)\n",
    "print('Correct2Incorrect_indexes: ', Correct2Incorrect_indexes)\n",
    "\n",
    "\n",
    "Ones=df_Y_pred['baseline'] ==sellect_people_define.ASDTD_label['ASD']\n",
    "Twos=df_Y_pred['proposed'] ==sellect_people_define.ASDTD_label['TD']\n",
    "Ones2Twos=  Ones & Twos\n",
    "\n",
    "Twos=df_Y_pred['baseline'] ==sellect_people_define.ASDTD_label['TD']\n",
    "Ones=df_Y_pred['proposed'] ==sellect_people_define.ASDTD_label['ASD']\n",
    "Twos2Ones=  Ones & Twos\n",
    "\n",
    "Ones2Twos_indexes=list(df_Y_pred[Ones2Twos].index)\n",
    "Twos2Ones_indexes=list(df_Y_pred[Twos2Ones].index)\n",
    "\n",
    "assert len(Ones2Twos_indexes+Twos2Ones_indexes) == len(Incorrect2Correct_indexes+Correct2Incorrect_indexes)\n",
    "\n",
    "ASDTD2Logit_map={\n",
    "    'TD': sellect_people_define.ASDTD_label['TD']-1,\n",
    "    'ASD': sellect_people_define.ASDTD_label['ASD']-1,\n",
    "    }\n",
    "\n",
    "quadrant1_indexes=intersection(Correct2Incorrect_indexes, Ones2Twos_indexes)\n",
    "quadrant2_indexes=intersection(Incorrect2Correct_indexes, Ones2Twos_indexes)\n",
    "quadrant3_indexes=intersection(Correct2Incorrect_indexes, Twos2Ones_indexes)\n",
    "quadrant4_indexes=intersection(Incorrect2Correct_indexes, Twos2Ones_indexes)\n",
    "\n",
    "\n",
    "Type1Err_dict, Type2Err_dict={}, {}\n",
    "for model_str in ['baseline', 'proposed']:\n",
    "    Type1Err_dict[model_str], Type2Err_dict[model_str] = Get_Model_Type12Errors(model_str=model_str, tureLab_str='y_true')\n",
    "\n",
    "model_str='baseline'\n",
    "Type1Err_baseline_indexes=list(df_Y_pred[Type1Err_dict[model_str]].index)\n",
    "Type2Err_baseline_indexes=list(df_Y_pred[Type2Err_dict[model_str]].index)\n",
    "model_str='proposed'\n",
    "Type1Err_proposed_indexes=list(df_Y_pred[Type1Err_dict[model_str]].index)\n",
    "Type2Err_proposed_indexes=list(df_Y_pred[Type2Err_dict[model_str]].index)\n",
    "\n",
    "\n",
    "\n",
    "All_err_indexes=list(set(Type1Err_baseline_indexes+Type2Err_baseline_indexes+Type1Err_proposed_indexes+Type2Err_proposed_indexes))\n",
    "All_indexes=list(df_Y_pred.index)\n",
    "'''\n",
    "\n",
    "    Part 2: Check the SHAP values based on indexes in part 1\n",
    "    \n",
    "    先紀錄，再執行分析和畫圖\n",
    "\n",
    "'''\n",
    "Manual_inspect_idxs=[2, 3, 29, 36, 44]\n",
    "##############################################\n",
    "\n",
    "# selected_idxs=Ones2Twos_indexes+Twos2Ones_indexes\n",
    "selected_idxs=All_indexes\n",
    "Baseline_changed_info_dict=Organize_Needed_SHAP_info(selected_idxs, Session_level_all, baseline_expstr)\n",
    "Proposed_changed_info_dict=Organize_Needed_SHAP_info(selected_idxs, Session_level_all, proposed_expstr)\n",
    "\n",
    "Baseline_totalPoeple_info_dict=Organize_Needed_SHAP_info(df_Y_pred.index, Session_level_all, baseline_expstr)\n",
    "Proposed_totalPoeple_info_dict=Organize_Needed_SHAP_info(df_Y_pred.index, Session_level_all, proposed_expstr)\n",
    "\n",
    "#%%\n",
    "# 畫炫炮的錯誤型態分析 (Changed smaples的logit 1 decision function 的移動)\n",
    "\n",
    "def Organize_Needed_decisionProb(Incorrect2Correct_indexes, Session_level_all, proposed_expstr):\n",
    "    Incorrect2Correct_info_dict=Dict()\n",
    "    for tst_idx in Incorrect2Correct_indexes:\n",
    "        for key, values in Session_level_all[proposed_expstr]['Logit{}_predictproba'.format(logit_number)].items():\n",
    "            test_fold_idx=[int(k) for k in key.split(\"_\")]\n",
    "            for i,ii in enumerate(test_fold_idx): #ii is the index of the sample, i is the position of this sample in this test fold\n",
    "                if tst_idx == ii:\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['predictproba']=values['predictproba'][i]\n",
    "                    Incorrect2Correct_info_dict[tst_idx]['decisionfunc']=values['decisionfunc'][i]\n",
    "\n",
    "    return Incorrect2Correct_info_dict\n",
    "# fig, axes = plt.subplots(2, 1, sharex=True, figsize=(10,5))\n",
    "\n",
    "\n",
    "\n",
    "# step 1: prepare data\n",
    "# selected_idxs=Ones2Twos_indexes+Twos2Ones_indexes\n",
    "Baseline_changed_decision_info_dict=Organize_Needed_decisionProb(selected_idxs, Session_level_all, baseline_expstr)\n",
    "Proposed_changed_decision_info_dict=Organize_Needed_decisionProb(selected_idxs, Session_level_all, proposed_expstr)\n",
    "Baseline_total_decision_info_dict=Organize_Needed_decisionProb(df_Y_pred.index, Session_level_all, baseline_expstr)\n",
    "Proposed_total_decision_info_dict=Organize_Needed_decisionProb(df_Y_pred.index, Session_level_all, proposed_expstr)\n",
    "\n",
    "\n",
    "df_Proposed_changed_decision_info_dict=pd.DataFrame.from_dict(Proposed_changed_decision_info_dict,orient='index')\n",
    "df_Baseline_changed_decision_info_dict=pd.DataFrame.from_dict(Baseline_changed_decision_info_dict,orient='index')\n",
    "df_Proposed_total_decision_info_dict=pd.DataFrame.from_dict(Proposed_total_decision_info_dict,orient='index')\n",
    "df_Baseline_total_decision_info_dict=pd.DataFrame.from_dict(Baseline_total_decision_info_dict,orient='index')\n",
    "Sample_idxs_array=df_Baseline_changed_decision_info_dict.index.values\n",
    "\n",
    "df_Y_true=df_Y_pred.loc[df_Baseline_changed_decision_info_dict.index]['y_true']\n",
    "df_Y_true_ASD_bool=df_Y_true==sellect_people_define.ASDTD_label['ASD']\n",
    "df_Y_true_TD_bool=df_Y_true==sellect_people_define.ASDTD_label['TD']\n",
    "Incorrect_baseline=df_Y_pred['baseline'] != df_Y_pred['y_true']\n",
    "Incorrect_proposed=df_Y_pred['proposed'] != df_Y_pred['y_true']\n",
    "Correct_baseline=df_Y_pred['baseline'] == df_Y_pred['y_true']\n",
    "Correct_proposed=df_Y_pred['proposed'] == df_Y_pred['y_true']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# decision function 負的表示predict logit 0, 正的表示logit 1\n",
    "Baseline_x= df_Baseline_changed_decision_info_dict['predictproba'].values\n",
    "Baseline_y_decisionfunc= df_Baseline_changed_decision_info_dict['decisionfunc'].abs().values.copy()\n",
    "Baseline_y= df_Baseline_changed_decision_info_dict['decisionfunc'].abs().values.copy()\n",
    "Baseline_total_x= df_Baseline_total_decision_info_dict['predictproba'].values\n",
    "Baseline_total_y_decisionfunc= df_Baseline_total_decision_info_dict['decisionfunc'].abs().values.copy()\n",
    "Baseline_total_y= df_Baseline_total_decision_info_dict['decisionfunc'].abs().values.copy()\n",
    "# 如果是TD decision function是正的y軸就是正的，decision function是負的y軸就是負的\n",
    "# 如果是ASD decision function是正的y軸就是負的，decision function是負的y軸就是正的\n",
    "Baseline_y[Incorrect2Correct.loc[df_Baseline_changed_decision_info_dict.index]]\\\n",
    "    =-Baseline_y_decisionfunc[Incorrect2Correct.loc[df_Baseline_changed_decision_info_dict.index]]\n",
    "Baseline_y[Correct2Incorrect.loc[df_Baseline_changed_decision_info_dict.index]]\\\n",
    "    =Baseline_y_decisionfunc[Correct2Incorrect.loc[df_Baseline_changed_decision_info_dict.index]]\n",
    "\n",
    "df_Baseline_total_decision_info_dict.loc[Incorrect_baseline]\n",
    "\n",
    "Baseline_total_y[Incorrect_baseline.values]=-Baseline_total_y_decisionfunc[Incorrect_baseline.values]\n",
    "Baseline_total_y[Correct_baseline.values]=Baseline_total_y_decisionfunc[Correct_baseline.values]\n",
    "\n",
    "\n",
    "\n",
    "Proposed_x= df_Proposed_changed_decision_info_dict['predictproba'].values\n",
    "Proposed_y_decisionfunc= df_Proposed_changed_decision_info_dict['decisionfunc'].abs().values.copy()\n",
    "Proposed_y= df_Proposed_changed_decision_info_dict['decisionfunc'].abs().values.copy()\n",
    "Proposed_total_x= df_Proposed_total_decision_info_dict['predictproba'].values\n",
    "Proposed_total_y_decisionfunc= df_Proposed_total_decision_info_dict['decisionfunc'].abs().values.copy()\n",
    "Proposed_total_y= df_Proposed_total_decision_info_dict['decisionfunc'].abs().values.copy()\n",
    "\n",
    "Proposed_y[Incorrect2Correct.loc[df_Baseline_changed_decision_info_dict.index]]=\\\n",
    "    Proposed_y_decisionfunc[Incorrect2Correct.loc[df_Baseline_changed_decision_info_dict.index]]\n",
    "Proposed_y[Correct2Incorrect.loc[df_Baseline_changed_decision_info_dict.index]]=\\\n",
    "    -Proposed_y_decisionfunc[Correct2Incorrect.loc[df_Baseline_changed_decision_info_dict.index]]\n",
    "\n",
    "Proposed_total_y[Incorrect_proposed.values]=-Proposed_total_y_decisionfunc[Incorrect_proposed.values]\n",
    "Proposed_total_y[Correct_proposed.values]=Proposed_total_y_decisionfunc[Correct_proposed.values]\n",
    "\n",
    "# Proposed_total_y[df_Proposed_total_decision_info_dict.loc[Incorrect_proposed]]\\\n",
    "#     =-df_Proposed_total_decision_info_dict.loc[Incorrect_proposed]\n",
    "# Proposed_total_y[df_Proposed_total_decision_info_dict.loc[Incorrect_proposed]]\\\n",
    "#     =df_Proposed_total_decision_info_dict.loc[Correct_proposed]\n",
    "\n",
    "Total_y=list(Baseline_y)+list(Proposed_y)\n",
    "Total_x=list(Baseline_x)+list(Proposed_x)\n",
    "\n",
    "y_max=np.max(Total_y)\n",
    "y_min=np.min(Total_y)\n",
    "x_max=np.max(Total_x)\n",
    "x_min=np.min(Total_x)\n",
    "x_middle=(x_max+x_min)/2\n",
    "y_middle=(y_max+y_min)/2\n",
    "\n",
    "# ax.annotate(\"\", xy=(起點x, 起點y), xytext=(終點x, 終點y),arrowprops=dict(arrowstyle=\"->\"))\n",
    "for B_x, B_y, P_x, P_y,idx in zip(Baseline_x,Baseline_y,Proposed_x,Proposed_y,Sample_idxs_array):\n",
    "    ax.annotate(\"\", xy=(B_x, B_y), xytext=(P_x, P_y),arrowprops=dict(arrowstyle=\"<-\",alpha=.4))\n",
    "    # ax.text((B_x+P_x)/2, (B_y+P_y)/2, str(idx), fontsize=12)\n",
    "    # ax.text(B_x, B_y, str(idx), fontsize=12)\n",
    "    # ax.text(P_x, P_y, str(idx), fontsize=12)\n",
    "\n",
    "plt.scatter(Baseline_x, Baseline_y, c='b', alpha=1)\n",
    "plt.scatter(Proposed_x, Proposed_y, c='r', alpha=1)\n",
    "\n",
    "plt.scatter(df_Baseline_total_decision_info_dict.predictproba, Baseline_total_y, c='b', alpha=.05)\n",
    "plt.scatter(df_Proposed_total_decision_info_dict.predictproba, Proposed_total_y, c='r', alpha=.05)\n",
    "\n",
    "# plt.scatter(df_Baseline_total_decision_info_dict.predictproba, df_Baseline_total_decision_info_dict.abs().decisionfunc, c='b', alpha=.5)\n",
    "# plt.scatter(df_Proposed_total_decision_info_dict.predictproba, df_Proposed_total_decision_info_dict.abs().decisionfunc, c='r', alpha=.5)\n",
    "\n",
    "ax.annotate('',xy=(0, 0), xytext=(1, 0),arrowprops=dict(arrowstyle=\"<->\",alpha=1,))                                                                     \n",
    "ax.annotate('',xy=(0.5, y_min), xytext=(0.5, y_max),arrowprops=dict(arrowstyle=\"<->\",alpha=1,))\n",
    "margin_y=(y_max-y_min)/10\n",
    "margin_x=(1-0)/20\n",
    "\n",
    "ax.text(0, y_middle-margin_y, 'ASD', fontsize=12)\n",
    "ax.text(1-margin_x, y_middle-margin_y, 'TD', fontsize=12)\n",
    "    \n",
    "ax.text(0.5, y_min, 'Incorrect', fontsize=12)\n",
    "ax.text(0.5, y_max-margin_y, 'Correct', fontsize=12)   \n",
    "fig.patch.set_visible(True)\n",
    "ax.axis('off')\n",
    "# plt.ylim(-1.5,1.5)\n",
    "plt.xlim(-0,1.1)\n",
    "plt.title(experiment_title)\n",
    "plt.show()\n",
    "#%%\n",
    "if experiment_title == 'lowMinimal': # Low minimal會有一個重複，所以這邊要多拿幾個人\n",
    "    show_number=11\n",
    "else:\n",
    "    show_number=10\n",
    "# TASLP binary分纇的圖， 記得要改926 927行，因為要跑三個binary classification task\n",
    "shap_values, df_XTest, keys=Prepare_data_for_summaryPlot(Proposed_totalPoeple_info_dict,\\\n",
    "                                                          feature_columns=None,\\\n",
    "                                                          PprNmeMp=PprNmeMp) \n",
    "\n",
    "df_shap_values=pd.DataFrame(shap_values[1],columns=df_XTest.columns)\n",
    "\n",
    "\n",
    "\n",
    "df_shap_values_top5Important=df_shap_values.abs().sum(axis=0).sort_values(ascending=False)\n",
    "df_shap_values_top5Important_normalized=(df_shap_values_top5Important-df_shap_values_top5Important.min())/(df_shap_values_top5Important.max()-df_shap_values_top5Important.min()) * 100\n",
    "df_shap_values_top5Important_normalized_tailed5=df_shap_values_top5Important_normalized.head(show_number).round(1)\n",
    "selected_feature_array = df_shap_values_top5Important_normalized_tailed5.index.values\n",
    "selected_feature_lst = list(df_shap_values_top5Important_normalized_tailed5.index)\n",
    "\n",
    "\n",
    "# 做一個feature to feature set mapping\n",
    "cols2CategoricalName_PprNme={}\n",
    "for k,v in FeatSel.cols2CategoricalName.items():\n",
    "    name_k=Swap2PaperName(k, PprNmeMp, method='inverse')\n",
    "    name_v=Swap2PaperName(v, PprNmeMp, method='inverse')\n",
    "    cols2CategoricalName_PprNme[name_k]=name_v\n",
    "\n",
    "\n",
    "Updated_cols2Categorical_dict={}\n",
    "for featN in selected_feature_lst:\n",
    "    if re.search('GC\\[(Max|Mean|Std).*\\]*inv.*',featN) != None:\n",
    "        Updated_cols2Categorical_dict[featN]='GC[P]$_\\mathrm{inv}$'\n",
    "    elif re.search('GC\\[(Max|Mean|Std).*\\]*part.*',featN) != None:\n",
    "        Updated_cols2Categorical_dict[featN]='GC[P]$_\\mathrm{part}$'\n",
    "    elif re.search('Proximity\\[(Max|Mean|Std).*\\]',featN) != None:\n",
    "        Updated_cols2Categorical_dict[featN]='Proximity[P]'\n",
    "    elif re.search('Syncrony\\[(Max|Mean|Std).*\\]',featN) != None:\n",
    "        Updated_cols2Categorical_dict[featN]='Synchrony[P]'\n",
    "    elif re.search('Convergence\\[(Max|Mean|Std).*\\]',featN) != None:\n",
    "        Updated_cols2Categorical_dict[featN]='Convergence[P]'\n",
    "\n",
    "cols2CategoricalName_PprNme.update(Updated_cols2Categorical_dict)\n",
    "\n",
    "# 計算correlation between df_shap_values[selected_feature_lst],df_XTest[selected_feature_lst] 求他的正負來知道方向\n",
    "df_Table_FirstTotAnaly=pd.DataFrame()  # 直接output 一個好的表格啦\n",
    "for i,feat in enumerate(selected_feature_lst):\n",
    "    pear, pval=pearsonr(df_shap_values[[feat]].values.reshape(-1),df_XTest[[feat]].values.reshape(-1))\n",
    "    direction_str=''\n",
    "    if pear > 0:\n",
    "        direction_str='ASD$<$TD'\n",
    "    elif pear < 0:\n",
    "        direction_str='ASD$>$TD'\n",
    "    \n",
    "        \n",
    "    \n",
    "    # error handling: 不同的jitter measure 像是localdbJitter 或是 absoluteJitter 我這邊都當成Jitter，所以會有一次出現好幾個值的狀況\n",
    "    # if len(df_shap_values_top5Important_normalized_tailed5[feat]) > 1:\n",
    "    if type(df_shap_values_top5Important_normalized_tailed5[feat] ) == pd.Series:\n",
    "        Importance_val=df_shap_values_top5Important_normalized_tailed5[feat][0]\n",
    "    else:\n",
    "        Importance_val=df_shap_values_top5Important_normalized_tailed5[feat]\n",
    "        \n",
    "    feat_str='{} ({})'.format(feat,Importance_val)\n",
    "    df_Table_FirstTotAnaly.loc[feat_str,'Direction']=direction_str\n",
    "    # df_Table_FirstTotAnaly.loc[feat,'Feature Importance']=Importance_val\n",
    "    df_Table_FirstTotAnaly.loc[feat_str,'Feature type']=cols2CategoricalName_PprNme[feat]\n",
    "    \n",
    "\n",
    "# 畫出來用來驗證沒有錯\n",
    "shap.summary_plot(shap_values[1], df_XTest,feature_names=df_XTest.columns,show=False, max_display=show_number)\n",
    "plt.title(experiment_title)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#%%\n",
    "# 個體分析： 會存到SAHP_figures/{quadrant}/的資料夾，再開Jupyter去看\n",
    "SHAP_save_path_root=\"SHAP_figures/{quadrant}/\"\n",
    "\n",
    "\n",
    "import shutil\n",
    "from collections import Counter\n",
    "shutil.rmtree(SHAP_save_path_root.format(quadrant=\"\"), ignore_errors = True)\n",
    "\n",
    "\n",
    "Analysis_grp_bool=False\n",
    "N=5\n",
    "Xtest_dict={}\n",
    "expected_value_lst=[]\n",
    "UsePaperName_bool=True\n",
    "Quadrant_FeatureImportance_dict={}\n",
    "Quadrant_feature_AddedTopFive_dict={}\n",
    "Quadrant_feature_AddedFeatureImportance_dict={}\n",
    "Manual_inspect_idxs=[21, 1, 14, 15]\n",
    "df_Result_dict=Dict()\n",
    "# =============================================================================\n",
    "# 有predict probability的變數\n",
    "# 最後結果會放在一個nice table就不用一直手動複製了  \n",
    "# nice table 就放在df_Total_ErrAnal這個變數\n",
    "# XXX  放force plot的地方\n",
    "# =============================================================================\n",
    "# for Analysis_grp_str in ['Manual_inspect_idxs','quadrant1_indexes','quadrant2_indexes','quadrant3_indexes','quadrant4_indexes']:\n",
    "for Analysis_grp_str in ['quadrant1_indexes','quadrant2_indexes','quadrant3_indexes','quadrant4_indexes']:    \n",
    "# for Analysis_grp_str in ['quadrant2_indexes','quadrant4_indexes']:\n",
    "# for Analysis_grp_str in ['Manual_inspect_idxs']:\n",
    "    Analysis_grp_indexes=vars()[Analysis_grp_str]\n",
    "    df_shap_values_stacked=pd.DataFrame([])\n",
    "    for Inspect_samp in Analysis_grp_indexes:\n",
    "        shap_info_proposed=Proposed_changed_info_dict[Inspect_samp]\n",
    "        shap_info_baseline=Baseline_changed_info_dict[Inspect_samp]\n",
    "        # expected_value_proposed=shap_info_proposed['explainer_expected_value'][logit_number]\n",
    "        # expected_value_baseline=shap_info_baseline['explainer_expected_value'][logit_number]\n",
    "        expected_value_proposed=shap_info_proposed['explainer_expected_value']\n",
    "        expected_value_baseline=shap_info_baseline['explainer_expected_value']\n",
    "        df_shap_values_proposed=shap_info_proposed['shap_values'].loc[[logit_number]]\n",
    "        df_shap_values_baseline=shap_info_baseline['shap_values'].loc[[logit_number]]\n",
    "        deltaprob_baseline=df_shap_values_baseline.T.sum()\n",
    "        deltaprob_proposed=df_shap_values_proposed.T.sum()\n",
    "        BaselineFeatures=[getattr(FeatSel,k)  for k in baseline_featset_lst]\n",
    "        BaselineFeatures_flatten=[e for ee in BaselineFeatures for e in ee]\n",
    "        ProposedFeatures=[getattr(FeatSel,k)  for k in proposed_featset_lst]\n",
    "        ProposedFeatures_flatten=[e for ee in ProposedFeatures for e in ee]\n",
    "        Lists_of_addedFeatures=[getattr(FeatSel,k)  for k in Additional_featureSet]\n",
    "        Lists_of_addedFeatures_flatten=[e for ee in Lists_of_addedFeatures for e in ee]\n",
    "        deltaprob_baselineFeat_baseline=df_shap_values_baseline[BaselineFeatures_flatten].T.sum()\n",
    "        deltaprob_baselineFeat_proposed=df_shap_values_proposed[BaselineFeatures_flatten].T.sum()\n",
    "        deltaprob_additionalFeat_proposed=df_shap_values_proposed[Lists_of_addedFeatures_flatten].T.sum()\n",
    "        \n",
    "        \n",
    "        Prediction_prob_baseline=df_Baseline_changed_decision_info_dict.loc[Inspect_samp,'decisionfunc']\n",
    "        Prediction_prob_proposed=df_Proposed_changed_decision_info_dict.loc[Inspect_samp,'decisionfunc']\n",
    "        # Prediction_prob_baseline=df_Baseline_changed_decision_info_dict.loc[Inspect_samp,'predictproba']\n",
    "        # Prediction_prob_proposed=df_Proposed_changed_decision_info_dict.loc[Inspect_samp,'predictproba']\n",
    "\n",
    "        # =====================================================================\n",
    "        \n",
    "        if Reverse_exp == True:\n",
    "            shap_info=shap_info_baseline\n",
    "        else:\n",
    "            shap_info=shap_info_proposed\n",
    "        # expected_value=shap_info['explainer_expected_value'][logit_number]\n",
    "        expected_value=shap_info['explainer_expected_value']\n",
    "        shap_values=shap_info['shap_values'].loc[logit_number].values\n",
    "        df_shap_values=shap_info['shap_values'].loc[[logit_number]]\n",
    "        df_shap_values.index=[Inspect_samp]\n",
    "        \n",
    "        df_shap_values_T=df_shap_values.T\n",
    "        \n",
    "        if Reverse_exp == True:\n",
    "            Selected_feature_set=BaselineFeatures_flatten\n",
    "        else:\n",
    "            Selected_feature_set=ProposedFeatures_flatten\n",
    "        Xtest=shap_info['XTest']        \n",
    "        Xtest_additionaFeat=Xtest[Selected_feature_set]\n",
    "        Xtest_additionaFeat.index=[ Swap2PaperName(name,PprNmeMp) for name in Xtest_additionaFeat.index]\n",
    "        \n",
    "        Xtest_dict[Inspect_samp]=Xtest\n",
    "        df_shap_values_stacked=pd.concat([df_shap_values_stacked,df_shap_values],)\n",
    "        expected_value_lst.append(expected_value)\n",
    "        \n",
    "\n",
    "        \n",
    "        # 這個部份跑TASLP的Fig.7 也就是說明有些不顯著的feature卻shap value很高\n",
    "        SHAP_save_path=SHAP_save_path_root.format(quadrant=Analysis_grp_str)\n",
    "        if not os.path.exists(SHAP_save_path):\n",
    "            os.makedirs(SHAP_save_path)\n",
    "        # ============================================================================= \n",
    "        # 觀察哪些additional feature有加分效用\n",
    "        df_shap_values_additionaFeat=df_shap_values_T.loc[Selected_feature_set].T\n",
    "        df_shap_values_toInspect=df_shap_values_T.loc[Selected_feature_set]\n",
    "        deltaprob_additionaFeat_proposed=df_shap_values_additionaFeat.T.sum()\n",
    "        # 把同一個feature set的SHAP values加總\n",
    "        # {Swap2PaperName(FestSet,PprNmeMp): for FestSet in Additional_featureSet:}\n",
    "        if Reverse_exp == True:\n",
    "            used_feature_lst=baseline_featset_lst\n",
    "        else:\n",
    "            used_feature_lst=proposed_featset_lst\n",
    "        FeatContrib_dict={}\n",
    "        for FestSet in used_feature_lst:\n",
    "            FeatContrib_dict[Swap2PaperName(FestSet,PprNmeMp)]=df_shap_values_toInspect.loc[getattr(FeatSel,FestSet)].sum().values[0]\n",
    "            df_shap_values_toInspect.loc[getattr(FeatSel,FestSet)].sum().values[0]\n",
    "        Sum_SHAPval=sum([v for k,v in FeatContrib_dict.items()])\n",
    "        \n",
    "            \n",
    "        # if Reverse_exp == True:\n",
    "        #     FeatContrib_percent_dict = {k:'{}%'.format('%.1f' % -np.round(v*100,3)) for k,v in FeatContrib_dict.items()}\n",
    "        # else:\n",
    "        #     FeatContrib_percent_dict = {k:'{}%'.format('%.1f' % np.round(v*100,3)) for k,v in FeatContrib_dict.items()}\n",
    "        FeatContrib_str_dict = {k:'{}'.format('%.3f' % np.round(v,3)) for k,v in FeatContrib_dict.items()}\n",
    "        SHAP_percent_lst=[float(v.replace(\"%\",\"\")) for k,v in FeatContrib_str_dict.items()]\n",
    "        # SHAP_percent_lst=[float(v.replace(\"%\",\"\")) for k,v in FeatContrib_percent_dict.items()]\n",
    "        \n",
    "        # df_FeatContrib 的整體在這裡\n",
    "        df_FeatContrib=pd.DataFrame([\"\\\\textbf{base val}:\"+ str(np.round(expected_value_proposed,3)) ],columns=['Feat_distrib'])\n",
    "        suffix_str=\" + \".join([str(np.round(v,3)) for v in SHAP_percent_lst])\n",
    "        print(\"base value\", np.round(expected_value_proposed,3), \"%\")\n",
    "        # for key, value in FeatContrib_str_dict.items():\n",
    "        #     print(key, \" : \",value)\n",
    "        #     # print(Swap2PaperName(key,PprNmeMp,method='idx'), \" : \",value)\n",
    "        #     # df_FeatContrib=pd.concat([df_FeatContrib,pd.DataFrame([','+str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \": \"+value])],axis=1)\n",
    "        #     df_FeatContrib['Feat_distrib']=df_FeatContrib['Feat_distrib']+','+str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \":\"+value\n",
    "        \n",
    "        def AppendStr(FeatContrib_str_dict):\n",
    "            Append_str=''\n",
    "            for i,(key, value) in enumerate(FeatContrib_str_dict.items()):\n",
    "                Append_str+=str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \":\"+value\n",
    "                if i % 2 ==1  and i < len(FeatContrib_str_dict.keys())-1:\n",
    "                    Append_str+='\\n'\n",
    "                elif i % 2 ==0 and i < len(FeatContrib_str_dict.keys())-1:\n",
    "                    Append_str+=','\n",
    "                else:\n",
    "                    Append_str+=''\n",
    "            return Append_str\n",
    "        Append_str=AppendStr(FeatContrib_str_dict)\n",
    "        # Append_str=','.join([str(Swap2PaperName(key,PprNmeMp,method='idx'))+ \":\"+value for key, value in FeatContrib_str_dict.items()])\n",
    "        df_FeatContrib=pd.concat([df_FeatContrib,pd.DataFrame([Append_str],columns=df_FeatContrib.columns)],axis=0)\n",
    "        \n",
    "        \n",
    "        #######################################################################\n",
    "        \n",
    "        if Reverse_exp == True:\n",
    "            FinalProb_fromSHAP=expected_value_baseline + sum([v for k,v in FeatContrib_dict.items()])\n",
    "        else:\n",
    "            FinalProb_fromSHAP=expected_value_proposed + sum([v for k,v in FeatContrib_dict.items()])\n",
    "        \n",
    "        print(np.round(expected_value_proposed,3),\"+\",suffix_str, '=', FinalProb_fromSHAP)\n",
    "        if Reverse_exp == True:\n",
    "            # TDProb_str=\"TD: ({Prediction_prob_proposed}% → {Prediction_prob_baseline}%)\".format(\n",
    "            #                                                                 Prediction_prob_baseline='%.1f' % np.round(Prediction_prob_baseline*100,3),\n",
    "            #                                                                 Prediction_prob_proposed='%.1f' % np.round(Prediction_prob_proposed*100,3)\n",
    "            #                                                                 )\n",
    "            # ASDProb_str=\"ASD: ({Prediction_prob_proposed}% → {Prediction_prob_baseline}%)\".format(\n",
    "            #                                                                 Prediction_prob_baseline='%.1f' % np.round((1-Prediction_prob_baseline)*100,3),\n",
    "            #                                                                 Prediction_prob_proposed='%.1f' % np.round((1-Prediction_prob_proposed)*100,3)\n",
    "            #                                                                 )\n",
    "            # print(TDProb_str)\n",
    "            # print(ASDProb_str)\n",
    "            from_str = \"TD\"  if Prediction_prob_proposed > decision_boundary else \"ASD\"\n",
    "            to_str = \"TD\"  if Prediction_prob_baseline > decision_boundary else \"ASD\"\n",
    "            PredictionProb_str=\"{from_str}: {Prediction_prob_proposed:.2f}→{to_str}: {Prediction_prob_baseline:.2f}\".format(from_str=from_str,\n",
    "                                                                                                        to_str=to_str,\n",
    "                                                                                                        Prediction_prob_proposed=Prediction_prob_proposed,\n",
    "                                                                                                        Prediction_prob_baseline=Prediction_prob_baseline,\n",
    "                                                                                                        )\n",
    "            \n",
    "        else:\n",
    "            # TDProb_str=\"TD: ({Prediction_prob_baseline}% → {Prediction_prob_proposed}%)\".format(\n",
    "            #                                                                 Prediction_prob_baseline='%.1f' % np.round(Prediction_prob_baseline*100,3),\n",
    "            #                                                                 Prediction_prob_proposed='%.1f' % np.round(Prediction_prob_proposed*100,3)\n",
    "            #                                                                 )\n",
    "            # ASDProb_str=\"ASD: ({Prediction_prob_baseline}% → {Prediction_prob_proposed}%)\".format(\n",
    "            #                                                                  Prediction_prob_baseline='%.1f' % np.round((1-Prediction_prob_baseline)*100,3),\n",
    "            #                                                                  Prediction_prob_proposed='%.1f' % np.round((1-Prediction_prob_proposed)*100,3)\n",
    "            #                                                                 )\n",
    "            # print(TDProb_str)\n",
    "            # print(ASDProb_str)\n",
    "            from_str = \"TD\"  if Prediction_prob_baseline > decision_boundary else \"ASD\"\n",
    "            to_str = \"TD\"  if Prediction_prob_proposed > decision_boundary else \"ASD\"\n",
    "            PredictionProb_str=\"{from_str}:{Prediction_prob_baseline:.2f}→{to_str}:{Prediction_prob_proposed:.2f}\".format(from_str=from_str,\n",
    "                                                                                                        to_str=to_str,\n",
    "                                                                                                        Prediction_prob_proposed=Prediction_prob_proposed,\n",
    "                                                                                                        Prediction_prob_baseline=Prediction_prob_baseline,\n",
    "                                                                                                        )\n",
    "        print(PredictionProb_str)\n",
    "        assert from_str != to_str\n",
    "        # 這邊確認SHAP value加起來要從baseline的score到proposed的score\n",
    "        if Reverse_exp == True:\n",
    "            assert np.round(Prediction_prob_baseline,2) == np.round(FinalProb_fromSHAP,2)\n",
    "        else:\n",
    "            assert np.round(Prediction_prob_proposed,2) == np.round(FinalProb_fromSHAP,2)\n",
    "        \n",
    "        \n",
    "        df_instance=pd.DataFrame([str(Inspect_samp)])\n",
    "        df_PredictionProb=pd.DataFrame()\n",
    "        \n",
    "        if Reverse_exp == True:\n",
    "            if Analysis_grp_str == 'quadrant1_indexes' or Analysis_grp_str == 'quadrant3_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": X→O\"])\n",
    "            elif Analysis_grp_str == 'quadrant2_indexes' or Analysis_grp_str == 'quadrant4_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": O→X\"])\n",
    "        else:\n",
    "            if Analysis_grp_str == 'quadrant1_indexes' or Analysis_grp_str == 'quadrant3_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": O→X\"])\n",
    "            elif Analysis_grp_str == 'quadrant2_indexes' or Analysis_grp_str == 'quadrant4_indexes':\n",
    "                df_classification_result=pd.DataFrame([str(Inspect_samp)+\": X→O\"])\n",
    "        \n",
    "        df_predictionProb=pd.DataFrame([PredictionProb_str])\n",
    "        df_predictionProbnResult=pd.concat([df_classification_result , df_predictionProb],axis=0).reset_index(drop=True)\n",
    "        # df_predictionProb=pd.concat([df_predictionProb,pd.DataFrame([TDProb_str])],axis=0)\n",
    "        # df_predictionProb=pd.concat([df_predictionProb,pd.DataFrame([ASDProb_str])],axis=0)\n",
    "        \n",
    "        # df_predictionProb.loc[0,'Prediction Prob.']=\n",
    "        # df_PredictionProb.loc[1,'Prediction Prob.']=TDProb_str\n",
    "        # df_PredictionProb.loc[2,'Prediction Prob.']=ASDProb_str\n",
    "        df_NICETABLE=pd.DataFrame()\n",
    "        # df_NICETABLE=pd.concat([df_instance,df_predictionProbnResult],axis=1, ignore_index=True).reset_index(drop=True)\n",
    "        df_NICETABLE=pd.concat([df_predictionProbnResult],axis=1, ignore_index=True).reset_index(drop=True)\n",
    "        # df_NICETABLE=pd.merge(df_instance.T,df_predictionProbnResult.T, on=[0], how='inner')\n",
    "        # pd.merge(df_instance,df_predictionProbnResult, how='outer')\n",
    "        # df_instance.merge(df_predictionProbnResult)\n",
    "        \n",
    "        df_NICETABLE.drop_duplicates()\n",
    "        df_NICETABLE.index = range(len(df_NICETABLE))\n",
    "        df_FeatContrib.index=range(len(df_FeatContrib))\n",
    "        \n",
    "        # 最後看這個變數，複製到excel上\n",
    "        df_NICETABLE=pd.concat([df_NICETABLE,df_FeatContrib],axis=1, ignore_index=True)\n",
    "        df_Result_dict[Inspect_samp]=df_NICETABLE\n",
    "        \n",
    "        \n",
    "        fig = shap.force_plot(expected_value, df_shap_values_additionaFeat.values, Xtest_additionaFeat.round(2).T, figsize=(8, 3) ,matplotlib=True,show=False)\n",
    "        # fig = shap.force_plot(expected_value, df_shap_values.values, Xtest.round(2).T, figsize=(8, 3) ,matplotlib=True,show=False)\n",
    "        \n",
    "        \n",
    "        plt.savefig(\"images/SHAP_discussion_{sample}.png\".format(sample=Inspect_samp),dpi=400, bbox_inches='tight')\n",
    "        plt.savefig(SHAP_save_path+\"{sample}.png\".format(sample=Inspect_samp),dpi=150, bbox_inches='tight')\n",
    "        # plt.savefig(SHAP_save_path+\"{sample}.png\".format(sample=Inspect_samp),dpi=150)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # df_FeatureImportance_AddedFeatures=df_shap_values[Lists_of_addedFeatures_flatten].T\n",
    "        # df_FeatureImportance_AddedFeatures_absSorted=df_FeatureImportance_AddedFeatures.abs()[Inspect_samp].sort_values(ascending=False)\n",
    "        # df_addedFeatures_TopN=df_FeatureImportance_AddedFeatures.loc[df_FeatureImportance_AddedFeatures_absSorted.head(N).index]\n",
    "        # Quadrant_feature_AddedTopFive_dict[Inspect_samp]=df_addedFeatures_TopN\n",
    "        # Quadrant_feature_AddedFeatureImportance_dict[Inspect_samp]=df_FeatureImportance_AddedFeatures[Inspect_samp].sort_values(ascending=False)\n",
    "    if UsePaperName_bool==False:\n",
    "            df_shap_values_stacked.columns=[Swap2PaperName(idx, PprNmeMp) for idx in df_shap_values_stacked.columns]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' Analyses of per quadrants \n",
    "        (NOT IN USED ANYMORE)\n",
    "    '''\n",
    "    \n",
    "    # if len(Analysis_grp_indexes)>0 and Analysis_grp_bool == True:\n",
    "    #     # =============================================================================\n",
    "    #     # Feature importance\n",
    "    #     # =============================================================================\n",
    "    #     df_shap_values_stacked_T=df_shap_values_stacked.T\n",
    "    #     df_FeatureImportance=df_shap_values_stacked_T.abs().sum(axis=1).sort_values(ascending=False)\n",
    "\n",
    "    #     df_TopN=df_FeatureImportance.head(N)\n",
    "    #     Quadrant_FeatureImportance_dict[Analysis_grp_str]=df_FeatureImportance\n",
    "    #     df_shap_values_stacked[df_FeatureImportance.index].T\n",
    "        \n",
    "    #     # =============================================================================\n",
    "    #     # Count Top N occurence and plot histogram\n",
    "    #     N=5\n",
    "    #     # =============================================================================\n",
    "    #     df_shap_values_stacked_T=df_shap_values_stacked.T\n",
    "    #     N_totalpeople=len(df_shap_values_stacked_T.columns)\n",
    "    #     TopN_dict={}\n",
    "    #     LowN_dict={}\n",
    "    #     for Inspect_samp in df_shap_values_stacked_T.columns:\n",
    "    #         LowestN=df_shap_values_stacked_T[[Inspect_samp]].sort_values(by=[Inspect_samp])[:N].index\n",
    "    #         TopN=df_shap_values_stacked_T[[Inspect_samp]].sort_values(by=[Inspect_samp])[-N:].index\n",
    "    #         TopN_dict[Inspect_samp]=TopN\n",
    "    #         LowN_dict[Inspect_samp]=LowestN\n",
    "    #     df_TopN_dict=pd.DataFrame.from_dict(TopN_dict)\n",
    "    #     df_LowN_dict=pd.DataFrame.from_dict(LowN_dict)\n",
    "    \n",
    "    #     TopN_total_feat_array=df_TopN_dict.to_numpy().flatten()\n",
    "    #     TopN_total_feat_counts_dict = Counter(TopN_total_feat_array)\n",
    "        \n",
    "    #     df_TopN_total_feat_counts = pd.DataFrame.from_dict(TopN_total_feat_counts_dict, orient='index')\n",
    "    #     df_TopN_total_feat_counts.columns=['feature_counts']\n",
    "    #     df_TopN_total_feat_percentage=df_TopN_total_feat_counts/N_totalpeople*100\n",
    "    #     ax = df_TopN_total_feat_percentage.plot(kind='bar')\n",
    "    #     ax.set_ylabel(\"percentage\")\n",
    "    #     ax.set_xlabel(\"feature\")\n",
    "\n",
    "# pd.DataFrame.from_dict(df_Result_dict,orient='index')\n",
    "df_Total_ErrAnal_row=pd.DataFrame()\n",
    "df_Total_ErrAnal=pd.DataFrame()\n",
    "\n",
    "\n",
    "group_dict={}\n",
    "count=1\n",
    "for key, values in df_Result_dict.items():\n",
    "    df_Total_ErrAnal_row=pd.concat([df_Total_ErrAnal_row,values],axis=1)\n",
    "    if count % 3 == 0:\n",
    "        df_Total_ErrAnal=pd.concat([df_Total_ErrAnal,df_Total_ErrAnal_row])\n",
    "        df_Total_ErrAnal_row=pd.DataFrame()\n",
    "    count+=1\n",
    "\n",
    "\n",
    "#%%\n",
    "def ReorganizeFeatures4SummaryPlot(shap_values_logit, df_XTest,FeatureSet_lst):\n",
    "    # step1 convert shapvalue to df_shapvalues\n",
    "    df_shap_values=pd.DataFrame(shap_values_logit,columns=df_XTest.columns)\n",
    "    # step2 Categorize according to FeatureSet_lst\n",
    "    df_Reorganized_shap_values=pd.DataFrame()\n",
    "    df_Reorganized_XTest=pd.DataFrame()\n",
    "    for FSL in FeatureSet_lst:\n",
    "        FSL_papercolumns=[Swap2PaperName(k,PprNmeMp) for k in FeatSel.CategoricalName2cols[FSL]]\n",
    "    \n",
    "        df_Reorganized_shap_values=pd.concat([df_Reorganized_shap_values,df_shap_values[FSL_papercolumns]],axis=1)\n",
    "        df_Reorganized_XTest=pd.concat([df_Reorganized_XTest,df_XTest[FSL_papercolumns]],axis=1)\n",
    "    assert df_Reorganized_shap_values.shape == df_Reorganized_XTest.shape\n",
    "    return df_Reorganized_shap_values.values, df_Reorganized_XTest\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# \n",
    "#%%\n",
    "# Show feature importance TopN\n",
    "shap_values, df_XTest, keys=Prepare_data_for_summaryPlot(Proposed_totalPoeple_info_dict,\\\n",
    "                                                          feature_columns=None,\\\n",
    "                                                          PprNmeMp=PprNmeMp)\n",
    "\n",
    "\n",
    "df_shapValues=pd.DataFrame(shap_values[logit_number],columns=df_XTest.columns)\n",
    "df_shapValues_sum=df_shapValues.abs().sum(axis=0)\n",
    "df_shapValues_sum_norm=df_shapValues_sum/df_shapValues_sum.sum() * 100\n",
    "# df_shapValues_sum_norm_percentage=\\\n",
    "#     (df_shapValues_sum_norm-df_shapValues_sum_norm.min())/(df_shapValues_sum_norm.max()-df_shapValues_sum_norm.min())\n",
    "df_shapValues_sum_normTopN=df_shapValues_sum_norm.sort_values(ascending=False).iloc[:N]\n",
    "df_shapValues_sum_normSorted=df_shapValues_sum_norm.sort_values(ascending=False)\n",
    "df_shapValues_sum_normSortedForPlotting=pd.DataFrame(df_shapValues_sum_normSorted.T.values,columns=['shap_val'],index=df_shapValues_sum_normSorted.index)\n",
    "switch2paperName_bool=True\n",
    "if switch2paperName_bool==True:\n",
    "    df_shapValues_sum_normSortedForPlotting.index=[Swap2PaperName(n, PprNmeMp) for n in df_shapValues_sum_normSortedForPlotting.index]\n",
    "\n",
    "# sns.histplot(df_shapValues_sum_normSortedForPlotting, x=\"shap_val\", element=\"poly\")\n",
    "plt.figure(figsize=(16,8))\n",
    "x=[]\n",
    "count=1\n",
    "for n in df_shapValues_sum_normSortedForPlotting.index:\n",
    "    if count< 5:\n",
    "        x.append(Swap2PaperName(n, PprNmeMp))\n",
    "    else:\n",
    "        x.append('{}'.format(count))\n",
    "    count+=1\n",
    "\n",
    "y3 = df_shapValues_sum_normSortedForPlotting.values.reshape(-1)\n",
    "ax = sns.barplot(x=x, y=y3, palette=\"deep\")#, ax=ax3)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#%%\n",
    "# =============================================================================\n",
    "\n",
    "# inspect_featuresets='Trend[LOCDEP]_d'  #Trend[LOCDEP]d + Proximity[phonation]\n",
    "# inspect_featuresets='LOCDEP_Trend_D_cols'  #LOC_columns + Syncrony[phonation]\n",
    "inspect_featuresets='Vowel_dispersion'\n",
    "shap_values, df_XTest, keys=Prepare_data_for_summaryPlot(Proposed_changed_info_dict,\\\n",
    "                                                          feature_columns=(FeatSel.CategoricalName2cols)[inspect_featuresets],\\\n",
    "                                                          PprNmeMp=PprNmeMp)\n",
    "\n",
    "\n",
    "# FeatureSet_lst=['Trend[Vowel_dispersion_inter__vowel_centralization]_d','Trend[Vowel_dispersion_inter__vowel_dispersion]_d',\\\n",
    "#                 'Trend[Vowel_dispersion_intra]_d','Trend[formant_dependency]_d']     #Trend[LOCDEP]d + P roximity[phonation]\n",
    "# FeatureSet_lst=['Vowel_dispersion_inter__vowel_centralization','Vowel_dispersion_inter__vowel_dispersion',\\\n",
    "#                 ]     #LOC + P roximity[phonation]\n",
    "\n",
    "FeatureSet_lst=[\n",
    "'Vowel_dispersion_inter__vowel_centralization','Vowel_dispersion_inter__vowel_dispersion',\n",
    "]    # 'LOC_columns', 'Phonation_Trend_D_cols'\n",
    "###########################################\n",
    "# 檢查feature值與model output的方向關係\n",
    "# 你可以看圖（summary plot）\n",
    "shap_values, df_XTest, keys=Prepare_data_for_summaryPlot(Proposed_totalPoeple_info_dict,\\\n",
    "                                                          feature_columns=FeatSel.CategoricalName2cols[inspect_featuresets],\\\n",
    "                                                          PprNmeMp=PprNmeMp)\n",
    "\n",
    "\n",
    "Reorganized_shap_values, df_Reorganized_XTest=ReorganizeFeatures4SummaryPlot(shap_values[1], df_XTest, FeatureSet_lst)\n",
    "shap.summary_plot(Reorganized_shap_values, df_Reorganized_XTest,show=False, max_display=len(df_XTest.columns),sort=False)\n",
    "plt.title(experiment_title)\n",
    "plt.show()\n",
    "\n",
    "# 也可以看Correlation 值\n",
    "df_Pcorrela_FeaturesnSHAPVal=Calculate_XTestShape_correlation(Proposed_totalPoeple_info_dict,logit_number=logit_number)\n",
    "\n",
    "\n",
    "# 看Feature 值 & SHAP值\n",
    "inspect_idxs=quadrant1_indexes + quadrant3_indexes\n",
    "Feature_SHAP_info_dict=Organize_Needed_SHAP_info(inspect_idxs, Session_level_all, proposed_expstr)\n",
    "df_XTest_stacked,df_ShapValues_stacked=Return_ALigned_dfXtestndfShapValues(Feature_SHAP_info_dict,logit_number=logit_number)\n",
    "\n",
    "# Scatter plot \n",
    "\n",
    "df_X_featureRank=pd.DataFrame()\n",
    "for col in df_XTest.columns:\n",
    "    df_rank_features=df_XTest[col].argsort()  #從小排到大\n",
    "    df_rank=pd.Series(df_rank_features.index.values, index=df_rank_features ,name=df_rank_features.name).sort_index()\n",
    "    \n",
    "    df_X_featureRank=pd.concat([df_X_featureRank,df_rank],axis=1)\n",
    "    df_X_featureRank_T=df_X_featureRank.T\n",
    "\n",
    "#%%\n",
    "#////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "# //////\n",
    "# def SummaryPlot_Category(Poeple_info_dict,proposed_expstr,FeatureSet_lst,experiment_title,module):\n",
    "#     ##################################### 統合feautre set values\n",
    "#     # df_Baseline_shap_values=Get_Inspected_SHAP_df(Baseline_totalPoeple_info_dict,logits=[ASDTD2Logit_map['TD']]) [ASDTD2Logit_map['TD']]\n",
    "#     # df_Proposed_shap_values=Get_Inspected_SHAP_df(Poeple_info_dict,logits=[ASDTD2Logit_map['TD']]) [ASDTD2Logit_map['TD']]\n",
    "#     shap_values, df_XTest, keys=Prepare_data_for_summaryPlot(Poeple_info_dict,\\\n",
    "#                                                               feature_columns=None,\\\n",
    "#                                                               PprNmeMp=None)\n",
    "    \n",
    "#     df_shap_values=pd.DataFrame(shap_values[ASDTD2Logit_map['TD']],columns=df_XTest.columns,index=df_XTest.index)\n",
    "    \n",
    "#     df_FeaSet_avg_Comparison_proposed=Calculate_sum_of_SHAP_vals(df_shap_values,FeatSel_module=module,FeatureSet_lst=FeatureSet_lst)\n",
    "#     shap.summary_plot(df_FeaSet_avg_Comparison_proposed.drop(index=['Average','abs_Average']).values,\\\n",
    "#                       feature_names=df_FeaSet_avg_Comparison_proposed.columns,show=False, max_display=10)\n",
    "#     plt.title(experiment_title)\n",
    "#     plt.show()\n",
    "# SummaryPlot_Category(Proposed_changed_info_dict,proposed_expstr,FeatureSet_lst,experiment_title,module=FeatSel.CategoricalName2cols)\n",
    "\n",
    "#%%\n",
    "# shap_values, df_XTest, keys=Prepare_data_for_summaryPlot(Proposed_totalPoeple_info_dict,\\\n",
    "#                                                           feature_columns=FeatSel.CategoricalName2cols['Trend[LOCDEP]_d'],\\\n",
    "#                                                           PprNmeMp=None)\n",
    "# print(Get_Top_10_abs_SHAP_columns(shap_values[1],df_XTest).index)\n",
    "def Calculate_sum_of_SHAP_vals(df_values,FeatSel_module,FeatureSet_lst=['Phonation_Proximity_cols']):\n",
    "    df_FeaSet_avg_Comparison=pd.DataFrame([],columns=FeatureSet_lst)\n",
    "    for feat_set in FeatureSet_lst:\n",
    "        if inspect.ismodule(FeatSel_module): #FeatSel_module is a python module\n",
    "            feature_cols = getattr(FeatSel_module, feat_set)\n",
    "        elif type(FeatSel_module) == dict:\n",
    "            feature_cols = FeatSel_module[feat_set]\n",
    "        else:\n",
    "            raise TypeError()\n",
    "        df_FeaSet_avg_Comparison[feat_set]=df_values.loc[feature_cols,:].sum()\n",
    "    return df_FeaSet_avg_Comparison\n",
    "\n",
    "\n",
    "def Get_Inspected_SHAP_df(Info_dict,logits=[0,1]):\n",
    "    Top_shap_values_collect=Dict()\n",
    "    for logit_number in logits:\n",
    "        Top_shap_values_collect[logit_number]=pd.DataFrame()\n",
    "        \n",
    "        for Inspect_samp in Info_dict.keys():\n",
    "            shap_info=Info_dict[Inspect_samp]\n",
    "            df_shap_values=shap_info['shap_values'].loc[[logit_number]].T\n",
    "            df_shap_values.columns=[Inspect_samp]\n",
    "            Top_shap_values_collect[logit_number]=pd.concat([Top_shap_values_collect[logit_number],df_shap_values],axis=1)\n",
    "            # df_shap_values.columns=[]\n",
    "            # Xtest=shap_info['XTest']\n",
    "            # column2idx_dict={idx:str(i) for i,idx in enumerate(Xtest.index)}\n",
    "            # df_column2idx_dict=pd.DataFrame.from_dict(column2idx_dict,orient='index')\n",
    "            # df_shap_values['feature_idxs']=df_column2idx_dict\n",
    "        Top_shap_values_collect[logit_number]['Average']=Top_shap_values_collect[logit_number].mean(axis=1)\n",
    "        Top_shap_values_collect[logit_number]['abs_Average']=Top_shap_values_collect[logit_number].abs().mean(axis=1)\n",
    "        Top_shap_values_collect[logit_number]=Top_shap_values_collect[logit_number].sort_values(by='Average')\n",
    "    return Top_shap_values_collect\n",
    "\n",
    "Baseline_info_dict=Organize_Needed_SHAP_info(Incorrect2Correct_indexes+Correct2Incorrect_indexes, Session_level_all, baseline_expstr)\n",
    "Proposed_info_dict=Organize_Needed_SHAP_info(Incorrect2Correct_indexes+Correct2Incorrect_indexes, Session_level_all, proposed_expstr)\n",
    "\n",
    "\n",
    "Baseline_shap_values=Get_Inspected_SHAP_df(Baseline_info_dict,logits=[ASDTD2Logit_map['TD']]) [ASDTD2Logit_map['TD']]\n",
    "Proposed_shap_values=Get_Inspected_SHAP_df(Proposed_info_dict,logits=[ASDTD2Logit_map['TD']]) [ASDTD2Logit_map['TD']]\n",
    "# =============================================================================\n",
    "# 分析統計整體SHAP value的平均\n",
    "#Baseline model    \n",
    "baseline_featset_lst=baseline_expstr[re.search(\" >> \",baseline_expstr).end():re.search(\"::\",baseline_expstr).start()].split(\"+\")\n",
    "proposed_featset_lst=proposed_expstr[re.search(\" >> \",proposed_expstr).end():re.search(\"::\",proposed_expstr).start()].split(\"+\")\n",
    "\n",
    "\n",
    "df_FeaSet_avg_Comparison_baseline=Calculate_sum_of_SHAP_vals(Baseline_shap_values,FeatSel_module=FeatSel,FeatureSet_lst=baseline_featset_lst)\n",
    "df_FeaSet_avg_Comparison_proposed=Calculate_sum_of_SHAP_vals(Proposed_shap_values,FeatSel_module=FeatSel,FeatureSet_lst=proposed_featset_lst)\n",
    "\n",
    "assert (df_FeaSet_avg_Comparison_proposed.loc[Ones2Twos_indexes,proposed_featset_lst[0]] >= 0).all()\n",
    "assert (df_FeaSet_avg_Comparison_proposed.loc[Twos2Ones_indexes,proposed_featset_lst[0]] <= 0).all()\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "'''\n",
    "\n",
    "    Check feature importance\n",
    "\n",
    "'''\n",
    "# =============================================================================\n",
    "\n",
    "Proposed_changed_info_dict=Organize_Needed_SHAP_info(Incorrect2Correct_indexes+Correct2Incorrect_indexes, Session_level_all, proposed_expstr)\n",
    "Proposed_All_info_dict=Organize_Needed_SHAP_info(df_Y_pred.index, Session_level_all, proposed_expstr)\n",
    "\n",
    "Proposed_changed_shap_values=Get_Inspected_SHAP_df(Proposed_changed_info_dict,logits=[ASDTD2Logit_map['TD']]) [ASDTD2Logit_map['TD']]\n",
    "Proposed_All_shap_values=Get_Inspected_SHAP_df(Proposed_All_info_dict,logits=[ASDTD2Logit_map['TD']]) [ASDTD2Logit_map['TD']]\n",
    "\n",
    "df_catagorical_featImportance=pd.DataFrame()\n",
    "df_catagorical_featImportance.name='CategoricalFeatureImportance'\n",
    "for shap_valdfs in ['Proposed_changed_shap_values','Proposed_All_shap_values']:\n",
    "    for key in FeatSel.CategoricalName2cols.keys():\n",
    "        #feature importance is defined as absolute average of SHAP values \n",
    "        #refer to https://christophm.github.io/interpretable-ml-book/shap.html#shap-feature-importance\n",
    "        df_catagorical_featImportance.loc[key,shap_valdfs]=vars()[shap_valdfs].loc[FeatSel.CategoricalName2cols[key],'abs_Average'].round(2).sum()\n",
    "print(df_catagorical_featImportance)\n",
    "\n",
    "\n",
    "df_FeaSet_avg_Comparison_baseline=Calculate_sum_of_SHAP_vals(Baseline_shap_values,FeatSel_module=FeatSel,FeatureSet_lst=baseline_featset_lst)\n",
    "df_FeaSet_avg_Comparison_proposed=Calculate_sum_of_SHAP_vals(Proposed_shap_values,FeatSel_module=FeatSel,FeatureSet_lst=proposed_featset_lst)\n",
    "\n",
    "\n",
    "# df_FeaSet_deltaOne2Twos_avg_Comparison=df_FeaSet_avg_Comparison_proposed.loc[Ones2Twos_indexes,baseline_featset_lst] - \\\n",
    "#     df_FeaSet_avg_Comparison_baseline.loc[Ones2Twos_indexes,baseline_featset_lst]\n",
    "# df_FeaSet_deltaTwos2Ones_avg_Comparison=df_FeaSet_avg_Comparison_proposed.loc[Twos2Ones_indexes,baseline_featset_lst] - \\\n",
    "#     df_FeaSet_avg_Comparison_baseline.loc[Twos2Ones_indexes,baseline_featset_lst]\n",
    "# assert (df_FeaSet_deltaOne2Twos_avg_Comparison >=0).values.all()\n",
    "# assert (df_FeaSet_deltaTwos2Ones_avg_Comparison <=0).values.all()\n",
    "# df_values=df_Type1Err_proposed_shap_values\n",
    "# #Proposed model\n",
    "# proposed_featset_lst=proposed_expstr[re.search(\" >> \",proposed_expstr).end():re.search(\"::\",proposed_expstr).start()].split(\"+\")\n",
    "# for feat_set in proposed_featset_lst:\n",
    "#     feature_cols = getattr(FeatSel, feat_set)\n",
    "#     df_FeaSet_avg_Comparison.loc[feat_set,'Average_proposed']=df_values.loc[feature_cols,'Average'].sum()\n",
    "# # =============================================================================\n",
    "\n",
    "\n",
    "# # 對Type2來說**鑑別性資訊**就是幫助推向實際上是陽性（ASD）的那個logit\n",
    "# df_Type2Err_baseline_shap_values=Get_Inspected_SHAP_df(Type2Err_baseline_info_dict,logits=[ASDTD2Logit_map['ASD']]) [ASDTD2Logit_map['ASD']]\n",
    "# df_Type2Err_proposed_shap_values=Get_Inspected_SHAP_df(Type2Err_proposed_info_dict,logits=[ASDTD2Logit_map['ASD']]) [ASDTD2Logit_map['ASD']]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "''' Plotting area\n",
    "\n",
    "\n",
    "'''\n",
    "# =============================================================================\n",
    "# shap_info=Incorrect2Correct_info_dict[Inspect_samp]\n",
    "\n",
    "\n",
    "# expected_value=shap_info['explainer_expected_value'][logit_number]\n",
    "# shap_values=shap_info['shap_values'].loc[logit_number].values\n",
    "# df_shap_values=shap_info['shap_values'].loc[[logit_number]].T\n",
    "# Xtest=shap_info['XTest']\n",
    "\n",
    "# col_idxs=column_index(Xtest, FeatSel.LOCDEP_Trend_D_cols)\n",
    "# excol_idxs=np.array(list(set(range(len(Xtest))) - set(col_idxs)))\n",
    "\n",
    "\n",
    "# column2idx_dict={idx:str(i) for i,idx in enumerate(Xtest.index)}\n",
    "# df_column2idx_dict=pd.DataFrame.from_dict(column2idx_dict,orient='index')\n",
    "# df_shap_values['feature_idxs']=df_column2idx_dict\n",
    "# Xtest_numerized=pd.Series(Xtest.values,index=[ column2idx_dict[idxs] for idxs in  Xtest.index ]).round(2)\n",
    "# Xtest_numerized.name=Xtest.name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# shap.force_plot(expected_value, shap_values, Xtest, matplotlib=True,show=False)\n",
    "# # need adjustment when you are just showing part of the feature and not using all feautres\n",
    "# shap.force_plot(expected_value + sum(shap_values[excol_idxs]), shap_values[col_idxs], Xtest.iloc[col_idxs], matplotlib=True,show=False)\n",
    "def Get_Top_10_abs_SHAP_columns(shap_values,df_XTest):\n",
    "    ''' Get Top 10 abs SHAP columns '''\n",
    "    df_XSumSHAPVals=pd.DataFrame(np.abs(shap_values).sum(axis=0).reshape(1,-1),columns=df_XTest.columns).T\n",
    "    return df_XSumSHAPVals.sort_values(by=0,ascending=False).head(10)\n",
    "def column_index(df, query_cols):\n",
    "    # column_index(df, ['peach', 'banana', 'apple'])\n",
    "    cols = df.index.values\n",
    "    sidx = np.argsort(cols)\n",
    "    return sidx[np.searchsorted(cols,query_cols,sorter=sidx)]\n",
    "\n",
    "\n",
    "\n",
    "OriginFSet2PperNmeFSet_dict={\n",
    "'A':'Phonation_Trend_K_cols+Phonation_Syncrony_cols',\n",
    "'B':'LOC_columns+Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols',\n",
    "'C':'Phonation_Trend_D_cols+Phonation_Trend_K_cols+Phonation_Syncrony_cols',\n",
    "'D':'Phonation_Proximity_cols',\n",
    "'E':'LOCDEP_Trend_D_cols+Phonation_Proximity_cols',\n",
    "'F':'DEP_columns+Phonation_Trend_D_cols+Phonation_Proximity_cols',\n",
    "'G':'Phonation_Trend_D_cols+Phonation_Proximity_cols',\n",
    "}\n",
    "\n",
    "\n",
    "for k,v in OriginFSet2PperNmeFSet_dict.items():\n",
    "    Ppr_str='+'.join([Swap2PaperName(f, PprNmeMp) for f in v.split('+')])\n",
    "    print('Model {M}: {_str}'.format(M=k,_str=Ppr_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
